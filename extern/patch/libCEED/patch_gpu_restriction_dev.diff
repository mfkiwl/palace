diff --git a/.clang-format b/.clang-format
index 6df301e2..03b98475 100644
--- a/.clang-format
+++ b/.clang-format
@@ -9,6 +9,7 @@ AlignOperands:                Align
 AllowShortIfStatementsOnASingleLine: AllIfsAndElse
 ColumnLimit:                  150
 CommentPragmas:               'TESTARGS'
+DerivePointerAlignment:       false
 IncludeBlocks:                Preserve
 IncludeCategories:
   - Regex:                    '^<ceed/.*\.h>'
@@ -25,6 +26,7 @@ IncludeCategories:
     Priority:                 6
   - Regex:                    '.*'
     Priority:                 7
+PointerAlignment:             Right
 TabWidth:                     4
 UseTab:                       Never
 StatementMacros:              [CeedPragmaOptimizeOn, CeedPragmaOptimizeOff]
diff --git a/backends/blocked/ceed-blocked-operator.c b/backends/blocked/ceed-blocked-operator.c
index b33c564b..8a7135a9 100644
--- a/backends/blocked/ceed-blocked-operator.c
+++ b/backends/blocked/ceed-blocked-operator.c
@@ -457,8 +457,8 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Blocked(CeedOperator o
   // Setup
   CeedCallBackend(CeedOperatorSetup_Blocked(op));
 
-  // Check for identity
-  CeedCheck(!impl->is_identity_qf, ceed, CEED_ERROR_BACKEND, "Assembling identity QFunctions not supported");
+  // Check for restriction only operator
+  CeedCheck(!impl->is_identity_rstr_op, ceed, CEED_ERROR_BACKEND, "Assembling restriction only operators is not supported");
 
   // Input Evecs and Restriction
   CeedCallBackend(CeedOperatorSetupInputs_Blocked(num_input_fields, qf_input_fields, op_input_fields, NULL, true, e_data_full, impl, request));
@@ -552,33 +552,46 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Blocked(CeedOperator o
       if (num_active_in > 1) {
         CeedCallBackend(CeedVectorSetValue(active_in[(in + num_active_in - 1) % num_active_in], 0.0));
       }
-      // Set Outputs
-      for (CeedInt out = 0; out < num_output_fields; out++) {
-        CeedVector vec;
-
-        // Get output vector
-        CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
-        // Check if active output
-        if (vec == CEED_VECTOR_ACTIVE) {
-          CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[out], CEED_MEM_HOST, CEED_USE_POINTER, l_vec_array));
-          CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[out], &size));
-          l_vec_array += size * Q * block_size;  // Advance the pointer by the size of the output
+      if (!impl->is_identity_qf) {
+        // Set Outputs
+        for (CeedInt out = 0; out < num_output_fields; out++) {
+          CeedVector vec;
+
+          // Get output vector
+          CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
+          // Check if active output
+          if (vec == CEED_VECTOR_ACTIVE) {
+            CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[out], CEED_MEM_HOST, CEED_USE_POINTER, l_vec_array));
+            CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[out], &size));
+            l_vec_array += size * Q * block_size;  // Advance the pointer by the size of the output
+          }
         }
+        // Apply QFunction
+        CeedCallBackend(CeedQFunctionApply(qf, Q * block_size, impl->q_vecs_in, impl->q_vecs_out));
+      } else {
+        const CeedScalar *q_vec_array;
+
+        // Copy Identity Outputs
+        CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[0], &size));
+        CeedCallBackend(CeedVectorGetArrayRead(impl->q_vecs_out[0], CEED_MEM_HOST, &q_vec_array));
+        for (CeedInt i = 0; i < size * Q * block_size; i++) l_vec_array[i] = q_vec_array[i];
+        CeedCallBackend(CeedVectorRestoreArrayRead(impl->q_vecs_out[0], &q_vec_array));
+        l_vec_array += size * Q * block_size;
       }
-      // Apply QFunction
-      CeedCallBackend(CeedQFunctionApply(qf, Q * block_size, impl->q_vecs_in, impl->q_vecs_out));
     }
   }
 
   // Un-set output Qvecs to prevent accidental overwrite of Assembled
-  for (CeedInt out = 0; out < num_output_fields; out++) {
-    CeedVector vec;
+  if (!impl->is_identity_qf) {
+    for (CeedInt out = 0; out < num_output_fields; out++) {
+      CeedVector vec;
 
-    // Get output vector
-    CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
-    // Check if active output
-    if (vec == CEED_VECTOR_ACTIVE) {
-      CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[out], CEED_MEM_HOST, CEED_COPY_VALUES, NULL));
+      // Get output vector
+      CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
+      // Check if active output
+      if (vec == CEED_VECTOR_ACTIVE) {
+        CeedCallBackend(CeedVectorTakeArray(impl->q_vecs_out[out], CEED_MEM_HOST, NULL));
+      }
     }
   }
 
diff --git a/backends/cuda-ref/ceed-cuda-ref-operator.c b/backends/cuda-ref/ceed-cuda-ref-operator.c
index 7d51d5fb..29d3b083 100644
--- a/backends/cuda-ref/ceed-cuda-ref-operator.c
+++ b/backends/cuda-ref/ceed-cuda-ref-operator.c
@@ -54,16 +54,19 @@ static int CeedOperatorDestroy_Cuda(CeedOperator op) {
 
     CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
     CeedCallCuda(ceed, cuModuleUnload(impl->diag->module));
-    CeedCallBackend(CeedFree(&impl->diag->h_e_mode_in));
-    CeedCallBackend(CeedFree(&impl->diag->h_e_mode_out));
-    CeedCallCuda(ceed, cudaFree(impl->diag->d_e_mode_in));
-    CeedCallCuda(ceed, cudaFree(impl->diag->d_e_mode_out));
+    CeedCallCuda(ceed, cudaFree(impl->diag->d_eval_modes_in));
+    CeedCallCuda(ceed, cudaFree(impl->diag->d_eval_modes_out));
     CeedCallCuda(ceed, cudaFree(impl->diag->d_identity));
     CeedCallCuda(ceed, cudaFree(impl->diag->d_interp_in));
     CeedCallCuda(ceed, cudaFree(impl->diag->d_interp_out));
     CeedCallCuda(ceed, cudaFree(impl->diag->d_grad_in));
     CeedCallCuda(ceed, cudaFree(impl->diag->d_grad_out));
-    CeedCallBackend(CeedElemRestrictionDestroy(&impl->diag->point_block_rstr));
+    CeedCallCuda(ceed, cudaFree(impl->diag->d_div_in));
+    CeedCallCuda(ceed, cudaFree(impl->diag->d_div_out));
+    CeedCallCuda(ceed, cudaFree(impl->diag->d_curl_in));
+    CeedCallCuda(ceed, cudaFree(impl->diag->d_curl_out));
+    CeedCallBackend(CeedElemRestrictionDestroy(&impl->diag->diag_rstr));
+    CeedCallBackend(CeedElemRestrictionDestroy(&impl->diag->point_block_diag_rstr));
     CeedCallBackend(CeedVectorDestroy(&impl->diag->elem_diag));
     CeedCallBackend(CeedVectorDestroy(&impl->diag->point_block_elem_diag));
   }
@@ -86,17 +89,13 @@ static int CeedOperatorDestroy_Cuda(CeedOperator op) {
 //------------------------------------------------------------------------------
 // Setup infields or outfields
 //------------------------------------------------------------------------------
-static int CeedOperatorSetupFields_Cuda(CeedQFunction qf, CeedOperator op, bool is_input, CeedVector *e_vecs, CeedVector *q_vecs, CeedInt e_start,
+static int CeedOperatorSetupFields_Cuda(CeedQFunction qf, CeedOperator op, bool is_input, CeedVector *e_vecs, CeedVector *q_vecs, CeedInt start_e,
                                         CeedInt num_fields, CeedInt Q, CeedInt num_elem) {
   Ceed                ceed;
-  bool                is_strided, skip_restriction;
-  CeedSize            q_size;
-  CeedInt             dim, size;
   CeedQFunctionField *qf_fields;
   CeedOperatorField  *op_fields;
 
   CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
-
   if (is_input) {
     CeedCallBackend(CeedOperatorGetFields(op, NULL, &op_fields, NULL, NULL));
     CeedCallBackend(CeedQFunctionGetFields(qf, NULL, &qf_fields, NULL, NULL));
@@ -107,30 +106,29 @@ static int CeedOperatorSetupFields_Cuda(CeedQFunction qf, CeedOperator op, bool
 
   // Loop over fields
   for (CeedInt i = 0; i < num_fields; i++) {
-    CeedEvalMode e_mode;
+    bool         is_strided = false, skip_restriction = false;
+    CeedSize     q_size;
+    CeedInt      size;
+    CeedEvalMode eval_mode;
     CeedBasis    basis;
 
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &e_mode));
-
-    is_strided       = false;
-    skip_restriction = false;
-    if (e_mode != CEED_EVAL_WEIGHT) {
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
+    if (eval_mode != CEED_EVAL_WEIGHT) {
       CeedElemRestriction elem_rstr;
 
-      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &elem_rstr));
-
       // Check whether this field can skip the element restriction:
-      // must be passive input, with e_mode NONE, and have a strided restriction with CEED_STRIDES_BACKEND.
+      // Must be passive input, with eval_mode NONE, and have a strided restriction with CEED_STRIDES_BACKEND.
+      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &elem_rstr));
 
       // First, check whether the field is input or output:
       if (is_input) {
         CeedVector vec;
 
-        // Check for passive input:
+        // Check for passive input
         CeedCallBackend(CeedOperatorFieldGetVector(op_fields[i], &vec));
         if (vec != CEED_VECTOR_ACTIVE) {
-          // Check e_mode
-          if (e_mode == CEED_EVAL_NONE) {
+          // Check eval_mode
+          if (eval_mode == CEED_EVAL_NONE) {
             // Check for strided restriction
             CeedCallBackend(CeedElemRestrictionIsStrided(elem_rstr, &is_strided));
             if (is_strided) {
@@ -142,27 +140,23 @@ static int CeedOperatorSetupFields_Cuda(CeedQFunction qf, CeedOperator op, bool
       }
       if (skip_restriction) {
         // We do not need an E-Vector, but will use the input field vector's data directly in the operator application.
-        e_vecs[i + e_start] = NULL;
+        e_vecs[i + start_e] = NULL;
       } else {
-        CeedCallBackend(CeedElemRestrictionCreateVector(elem_rstr, NULL, &e_vecs[i + e_start]));
+        CeedCallBackend(CeedElemRestrictionCreateVector(elem_rstr, NULL, &e_vecs[i + start_e]));
       }
     }
 
-    switch (e_mode) {
+    switch (eval_mode) {
       case CEED_EVAL_NONE:
         CeedCallBackend(CeedQFunctionFieldGetSize(qf_fields[i], &size));
         q_size = (CeedSize)num_elem * Q * size;
         CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
         break;
       case CEED_EVAL_INTERP:
-        CeedCallBackend(CeedQFunctionFieldGetSize(qf_fields[i], &size));
-        q_size = (CeedSize)num_elem * Q * size;
-        CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
-        break;
       case CEED_EVAL_GRAD:
-        CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis));
+      case CEED_EVAL_DIV:
+      case CEED_EVAL_CURL:
         CeedCallBackend(CeedQFunctionFieldGetSize(qf_fields[i], &size));
-        CeedCallBackend(CeedBasisGetDimension(basis, &dim));
         q_size = (CeedSize)num_elem * Q * size;
         CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
         break;
@@ -172,10 +166,6 @@ static int CeedOperatorSetupFields_Cuda(CeedQFunction qf, CeedOperator op, bool
         CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
         CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, CEED_EVAL_WEIGHT, CEED_VECTOR_NONE, q_vecs[i]));
         break;
-      case CEED_EVAL_DIV:
-        break;  // TODO: Not implemented
-      case CEED_EVAL_CURL:
-        break;  // TODO: Not implemented
     }
   }
   return CEED_ERROR_SUCCESS;
@@ -206,10 +196,8 @@ static int CeedOperatorSetup_Cuda(CeedOperator op) {
 
   // Allocate
   CeedCallBackend(CeedCalloc(num_input_fields + num_output_fields, &impl->e_vecs));
-
   CeedCallBackend(CeedCalloc(CEED_FIELD_MAX, &impl->q_vecs_in));
   CeedCallBackend(CeedCalloc(CEED_FIELD_MAX, &impl->q_vecs_out));
-
   impl->num_inputs  = num_input_fields;
   impl->num_outputs = num_output_fields;
 
@@ -227,23 +215,25 @@ static int CeedOperatorSetup_Cuda(CeedOperator op) {
 // Setup Operator Inputs
 //------------------------------------------------------------------------------
 static inline int CeedOperatorSetupInputs_Cuda(CeedInt num_input_fields, CeedQFunctionField *qf_input_fields, CeedOperatorField *op_input_fields,
-                                               CeedVector in_vec, const bool skip_active_in, CeedScalar *e_data[2 * CEED_FIELD_MAX],
+                                               CeedVector in_vec, const bool skip_active, CeedScalar *e_data[2 * CEED_FIELD_MAX],
                                                CeedOperator_Cuda *impl, CeedRequest *request) {
   for (CeedInt i = 0; i < num_input_fields; i++) {
-    CeedEvalMode        e_mode;
+    CeedEvalMode        eval_mode;
     CeedVector          vec;
     CeedElemRestriction elem_rstr;
 
     // Get input vector
     CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
-      if (skip_active_in) continue;
+      if (skip_active) continue;
       else vec = in_vec;
     }
 
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_WEIGHT) {  // Skip
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_WEIGHT) {  // Skip
     } else {
+      // Get input vector
+      CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
       // Get input element restriction
       CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_input_fields[i], &elem_rstr));
       if (vec == CEED_VECTOR_ACTIVE) vec = in_vec;
@@ -265,45 +255,40 @@ static inline int CeedOperatorSetupInputs_Cuda(CeedInt num_input_fields, CeedQFu
 // Input Basis Action
 //------------------------------------------------------------------------------
 static inline int CeedOperatorInputBasis_Cuda(CeedInt num_elem, CeedQFunctionField *qf_input_fields, CeedOperatorField *op_input_fields,
-                                              CeedInt num_input_fields, const bool skip_active_in, CeedScalar *e_data[2 * CEED_FIELD_MAX],
+                                              CeedInt num_input_fields, const bool skip_active, CeedScalar *e_data[2 * CEED_FIELD_MAX],
                                               CeedOperator_Cuda *impl) {
   for (CeedInt i = 0; i < num_input_fields; i++) {
     CeedInt             elem_size, size;
-    CeedEvalMode        e_mode;
+    CeedEvalMode        eval_mode;
     CeedElemRestriction elem_rstr;
     CeedBasis           basis;
 
     // Skip active input
-    if (skip_active_in) {
+    if (skip_active) {
       CeedVector vec;
 
       CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
       if (vec == CEED_VECTOR_ACTIVE) continue;
     }
-    // Get elem_size, e_mode, size
+    // Get elem_size, eval_mode, size
     CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_input_fields[i], &elem_rstr));
     CeedCallBackend(CeedElemRestrictionGetElementSize(elem_rstr, &elem_size));
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &e_mode));
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &eval_mode));
     CeedCallBackend(CeedQFunctionFieldGetSize(qf_input_fields[i], &size));
     // Basis action
-    switch (e_mode) {
+    switch (eval_mode) {
       case CEED_EVAL_NONE:
         CeedCallBackend(CeedVectorSetArray(impl->q_vecs_in[i], CEED_MEM_DEVICE, CEED_USE_POINTER, e_data[i]));
         break;
       case CEED_EVAL_INTERP:
-        CeedCallBackend(CeedOperatorFieldGetBasis(op_input_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, CEED_EVAL_INTERP, impl->e_vecs[i], impl->q_vecs_in[i]));
-        break;
       case CEED_EVAL_GRAD:
+      case CEED_EVAL_DIV:
+      case CEED_EVAL_CURL:
         CeedCallBackend(CeedOperatorFieldGetBasis(op_input_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, CEED_EVAL_GRAD, impl->e_vecs[i], impl->q_vecs_in[i]));
+        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, eval_mode, impl->e_vecs[i], impl->q_vecs_in[i]));
         break;
       case CEED_EVAL_WEIGHT:
         break;  // No action
-      case CEED_EVAL_DIV:
-        break;  // TODO: Not implemented
-      case CEED_EVAL_CURL:
-        break;  // TODO: Not implemented
     }
   }
   return CEED_ERROR_SUCCESS;
@@ -313,18 +298,18 @@ static inline int CeedOperatorInputBasis_Cuda(CeedInt num_elem, CeedQFunctionFie
 // Restore Input Vectors
 //------------------------------------------------------------------------------
 static inline int CeedOperatorRestoreInputs_Cuda(CeedInt num_input_fields, CeedQFunctionField *qf_input_fields, CeedOperatorField *op_input_fields,
-                                                 const bool skip_active_in, CeedScalar *e_data[2 * CEED_FIELD_MAX], CeedOperator_Cuda *impl) {
+                                                 const bool skip_active, CeedScalar *e_data[2 * CEED_FIELD_MAX], CeedOperator_Cuda *impl) {
   for (CeedInt i = 0; i < num_input_fields; i++) {
-    CeedEvalMode e_mode;
+    CeedEvalMode eval_mode;
     CeedVector   vec;
 
     // Skip active input
-    if (skip_active_in) {
+    if (skip_active) {
       CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
       if (vec == CEED_VECTOR_ACTIVE) continue;
     }
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_WEIGHT) {  // Skip
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_WEIGHT) {  // Skip
     } else {
       if (!impl->e_vecs[i]) {  // This was a skip_restriction case
         CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
@@ -341,13 +326,12 @@ static inline int CeedOperatorRestoreInputs_Cuda(CeedInt num_input_fields, CeedQ
 // Apply and add to output
 //------------------------------------------------------------------------------
 static int CeedOperatorApplyAdd_Cuda(CeedOperator op, CeedVector in_vec, CeedVector out_vec, CeedRequest *request) {
-  CeedOperator_Cuda  *impl;
   CeedInt             Q, num_elem, elem_size, num_input_fields, num_output_fields, size;
-  CeedEvalMode        e_mode;
   CeedScalar         *e_data[2 * CEED_FIELD_MAX] = {NULL};
-  CeedOperatorField  *op_input_fields, *op_output_fields;
   CeedQFunctionField *qf_input_fields, *qf_output_fields;
   CeedQFunction       qf;
+  CeedOperatorField  *op_input_fields, *op_output_fields;
+  CeedOperator_Cuda  *impl;
 
   CeedCallBackend(CeedOperatorGetData(op, &impl));
   CeedCallBackend(CeedOperatorGetQFunction(op, &qf));
@@ -359,7 +343,7 @@ static int CeedOperatorApplyAdd_Cuda(CeedOperator op, CeedVector in_vec, CeedVec
   // Setup
   CeedCallBackend(CeedOperatorSetup_Cuda(op));
 
-  // Input e_vecs and Restriction
+  // Input Evecs and Restriction
   CeedCallBackend(CeedOperatorSetupInputs_Cuda(num_input_fields, qf_input_fields, op_input_fields, in_vec, false, e_data, impl, request));
 
   // Input basis apply if needed
@@ -367,8 +351,10 @@ static int CeedOperatorApplyAdd_Cuda(CeedOperator op, CeedVector in_vec, CeedVec
 
   // Output pointers, as necessary
   for (CeedInt i = 0; i < num_output_fields; i++) {
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_NONE) {
+    CeedEvalMode eval_mode;
+
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_NONE) {
       // Set the output Q-Vector to use the E-Vector data directly.
       CeedCallBackend(CeedVectorGetArrayWrite(impl->e_vecs[i + impl->num_inputs], CEED_MEM_DEVICE, &e_data[i + num_input_fields]));
       CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[i], CEED_MEM_DEVICE, CEED_USE_POINTER, e_data[i + num_input_fields]));
@@ -380,49 +366,46 @@ static int CeedOperatorApplyAdd_Cuda(CeedOperator op, CeedVector in_vec, CeedVec
 
   // Output basis apply if needed
   for (CeedInt i = 0; i < num_output_fields; i++) {
+    CeedEvalMode        eval_mode;
     CeedElemRestriction elem_rstr;
     CeedBasis           basis;
 
-    // Get elem_size, e_mode, size
+    // Get elem_size, eval_mode, size
     CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_output_fields[i], &elem_rstr));
     CeedCallBackend(CeedElemRestrictionGetElementSize(elem_rstr, &elem_size));
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &e_mode));
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &eval_mode));
     CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[i], &size));
     // Basis action
-    switch (e_mode) {
+    switch (eval_mode) {
       case CEED_EVAL_NONE:
-        break;
+        break;  // No action
       case CEED_EVAL_INTERP:
-        CeedCallBackend(CeedOperatorFieldGetBasis(op_output_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_TRANSPOSE, CEED_EVAL_INTERP, impl->q_vecs_out[i], impl->e_vecs[i + impl->num_inputs]));
-        break;
       case CEED_EVAL_GRAD:
+      case CEED_EVAL_DIV:
+      case CEED_EVAL_CURL:
         CeedCallBackend(CeedOperatorFieldGetBasis(op_output_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_TRANSPOSE, CEED_EVAL_GRAD, impl->q_vecs_out[i], impl->e_vecs[i + impl->num_inputs]));
+        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_TRANSPOSE, eval_mode, impl->q_vecs_out[i], impl->e_vecs[i + impl->num_inputs]));
         break;
       // LCOV_EXCL_START
       case CEED_EVAL_WEIGHT: {
         Ceed ceed;
+
         CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
         return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_WEIGHT cannot be an output evaluation mode");
-        break;  // Should not occur
+        // LCOV_EXCL_STOP
       }
-      case CEED_EVAL_DIV:
-        break;  // TODO: Not implemented
-      case CEED_EVAL_CURL:
-        break;  // TODO: Not implemented
-                // LCOV_EXCL_STOP
     }
   }
 
   // Output restriction
   for (CeedInt i = 0; i < num_output_fields; i++) {
+    CeedEvalMode        eval_mode;
     CeedVector          vec;
     CeedElemRestriction elem_rstr;
 
     // Restore evec
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_NONE) {
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_NONE) {
       CeedCallBackend(CeedVectorRestoreArray(impl->e_vecs[i + impl->num_inputs], &e_data[i + num_input_fields]));
     }
     // Get output vector
@@ -441,14 +424,12 @@ static int CeedOperatorApplyAdd_Cuda(CeedOperator op, CeedVector in_vec, CeedVec
 }
 
 //------------------------------------------------------------------------------
-// Core code for assembling linear QFunction
+// Linear QFunction Assembly Core
 //------------------------------------------------------------------------------
 static inline int CeedOperatorLinearAssembleQFunctionCore_Cuda(CeedOperator op, bool build_objects, CeedVector *assembled, CeedElemRestriction *rstr,
                                                                CeedRequest *request) {
   Ceed                ceed, ceed_parent;
-  bool                is_identity_qf;
   CeedInt             num_active_in, num_active_out, Q, num_elem, num_input_fields, num_output_fields, size;
-  CeedSize            q_size;
   CeedScalar         *assembled_array, *e_data[2 * CEED_FIELD_MAX] = {NULL};
   CeedVector         *active_inputs;
   CeedQFunctionField *qf_input_fields, *qf_output_fields;
@@ -470,11 +451,7 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Cuda(CeedOperator op,
   // Setup
   CeedCallBackend(CeedOperatorSetup_Cuda(op));
 
-  // Check for identity
-  CeedCallBackend(CeedQFunctionIsIdentity(qf, &is_identity_qf));
-  CeedCheck(!is_identity_qf, ceed, CEED_ERROR_BACKEND, "Assembling identity QFunctions not supported");
-
-  // Input e_vecs and Restriction
+  // Input Evecs and Restriction
   CeedCallBackend(CeedOperatorSetupInputs_Cuda(num_input_fields, qf_input_fields, op_input_fields, NULL, true, e_data, impl, request));
 
   // Count number of active input fields
@@ -492,7 +469,8 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Cuda(CeedOperator op,
         CeedCallBackend(CeedVectorGetArray(impl->q_vecs_in[i], CEED_MEM_DEVICE, &q_vec_array));
         CeedCallBackend(CeedRealloc(num_active_in + size, &active_inputs));
         for (CeedInt field = 0; field < size; field++) {
-          q_size = (CeedSize)Q * num_elem;
+          CeedSize q_size = (CeedSize)Q * num_elem;
+
           CeedCallBackend(CeedVectorCreate(ceed, q_size, &active_inputs[num_active_in + field]));
           CeedCallBackend(
               CeedVectorSetArray(active_inputs[num_active_in + field], CEED_MEM_DEVICE, CEED_USE_POINTER, &q_vec_array[field * Q * num_elem]));
@@ -526,12 +504,13 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Cuda(CeedOperator op,
 
   // Build objects if needed
   if (build_objects) {
+    CeedSize l_size     = (CeedSize)num_elem * Q * num_active_in * num_active_out;
+    CeedInt  strides[3] = {1, num_elem * Q, Q}; /* *NOPAD* */
+
     // Create output restriction
-    CeedInt strides[3] = {1, num_elem * Q, Q}; /* *NOPAD* */
     CeedCallBackend(CeedElemRestrictionCreateStrided(ceed_parent, num_elem, Q, num_active_in * num_active_out,
                                                      num_active_in * num_active_out * num_elem * Q, strides, rstr));
     // Create assembled vector
-    CeedSize l_size = (CeedSize)num_elem * Q * num_active_in * num_active_out;
     CeedCallBackend(CeedVectorCreate(ceed_parent, l_size, assembled));
   }
   CeedCallBackend(CeedVectorSetValue(*assembled, 0.0));
@@ -599,49 +578,14 @@ static int CeedOperatorLinearAssembleQFunctionUpdate_Cuda(CeedOperator op, CeedV
 }
 
 //------------------------------------------------------------------------------
-// Create point block restriction
+// Assemble Diagonal Setup
 //------------------------------------------------------------------------------
-static int CreatePointBlockRestriction(CeedElemRestriction rstr, CeedElemRestriction *point_block_rstr) {
-  Ceed           ceed;
-  CeedSize       l_size;
-  CeedInt        num_elem, num_comp, elem_size, comp_stride, *point_block_offsets;
-  const CeedInt *offsets;
-
-  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetOffsets(rstr, CEED_MEM_HOST, &offsets));
-
-  // Expand offsets
-  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
-  CeedCallBackend(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
-  CeedCallBackend(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
-  CeedInt shift = num_comp;
-
-  if (comp_stride != 1) shift *= num_comp;
-  CeedCallBackend(CeedCalloc(num_elem * elem_size, &point_block_offsets));
-  for (CeedInt i = 0; i < num_elem * elem_size; i++) {
-    point_block_offsets[i] = offsets[i] * shift;
-  }
-
-  // Create new restriction
-  CeedCallBackend(CeedElemRestrictionCreate(ceed, num_elem, elem_size, num_comp * num_comp, 1, l_size * num_comp, CEED_MEM_HOST, CEED_OWN_POINTER,
-                                            point_block_offsets, point_block_rstr));
-
-  // Cleanup
-  CeedCallBackend(CeedElemRestrictionRestoreOffsets(rstr, &offsets));
-  return CEED_ERROR_SUCCESS;
-}
-
-//------------------------------------------------------------------------------
-// Assemble diagonal setup
-//------------------------------------------------------------------------------
-static inline int CeedOperatorAssembleDiagonalSetup_Cuda(CeedOperator op, const bool is_point_block, CeedInt use_ceedsize_idx) {
+static inline int CeedOperatorAssembleDiagonalSetup_Cuda(CeedOperator op, CeedInt use_ceedsize_idx) {
   Ceed                ceed;
   char               *diagonal_kernel_path, *diagonal_kernel_source;
-  CeedInt             num_input_fields, num_output_fields, num_e_mode_in = 0, num_comp = 0, dim = 1, num_e_mode_out = 0, num_nodes, num_qpts;
-  CeedEvalMode       *e_mode_in = NULL, *e_mode_out = NULL;
-  CeedElemRestriction rstr_in = NULL, rstr_out = NULL;
+  CeedInt             num_input_fields, num_output_fields, num_eval_modes_in = 0, num_eval_modes_out = 0;
+  CeedInt             num_comp, q_comp, num_nodes, num_qpts;
+  CeedEvalMode       *eval_modes_in = NULL, *eval_modes_out = NULL;
   CeedBasis           basis_in = NULL, basis_out = NULL;
   CeedQFunctionField *qf_fields;
   CeedQFunction       qf;
@@ -660,33 +604,20 @@ static inline int CeedOperatorAssembleDiagonalSetup_Cuda(CeedOperator op, const
 
     CeedCallBackend(CeedOperatorFieldGetVector(op_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
-      CeedEvalMode        e_mode;
-      CeedElemRestriction rstr;
-
-      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis_in));
-      CeedCallBackend(CeedBasisGetNumComponents(basis_in, &num_comp));
-      CeedCallBackend(CeedBasisGetDimension(basis_in, &dim));
-      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &rstr));
-      CeedCheck(!rstr_in || rstr_in == rstr, ceed, CEED_ERROR_BACKEND,
-                "Backend does not implement multi-field non-composite operator diagonal assembly");
-      rstr_in = rstr;
-      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &e_mode));
-      switch (e_mode) {
-        case CEED_EVAL_NONE:
-        case CEED_EVAL_INTERP:
-          CeedCallBackend(CeedRealloc(num_e_mode_in + 1, &e_mode_in));
-          e_mode_in[num_e_mode_in] = e_mode;
-          num_e_mode_in += 1;
-          break;
-        case CEED_EVAL_GRAD:
-          CeedCallBackend(CeedRealloc(num_e_mode_in + dim, &e_mode_in));
-          for (CeedInt d = 0; d < dim; d++) e_mode_in[num_e_mode_in + d] = e_mode;
-          num_e_mode_in += dim;
-          break;
-        case CEED_EVAL_WEIGHT:
-        case CEED_EVAL_DIV:
-        case CEED_EVAL_CURL:
-          break;  // Caught by QF Assembly
+      CeedBasis    basis;
+      CeedEvalMode eval_mode;
+
+      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis));
+      CeedCheck(!basis_in || basis_in == basis, ceed, CEED_ERROR_BACKEND,
+                "Backend does not implement operator diagonal assembly with multiple active bases");
+      basis_in = basis;
+      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_in, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_in + q_comp, &eval_modes_in));
+        for (CeedInt d = 0; d < q_comp; d++) eval_modes_in[num_eval_modes_in + d] = eval_mode;
+        num_eval_modes_in += q_comp;
       }
     }
   }
@@ -699,31 +630,20 @@ static inline int CeedOperatorAssembleDiagonalSetup_Cuda(CeedOperator op, const
 
     CeedCallBackend(CeedOperatorFieldGetVector(op_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
-      CeedEvalMode        e_mode;
-      CeedElemRestriction rstr;
-
-      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis_out));
-      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &rstr));
-      CeedCheck(!rstr_out || rstr_out == rstr, ceed, CEED_ERROR_BACKEND,
-                "Backend does not implement multi-field non-composite operator diagonal assembly");
-      rstr_out = rstr;
-      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &e_mode));
-      switch (e_mode) {
-        case CEED_EVAL_NONE:
-        case CEED_EVAL_INTERP:
-          CeedCallBackend(CeedRealloc(num_e_mode_out + 1, &e_mode_out));
-          e_mode_out[num_e_mode_out] = e_mode;
-          num_e_mode_out += 1;
-          break;
-        case CEED_EVAL_GRAD:
-          CeedCallBackend(CeedRealloc(num_e_mode_out + dim, &e_mode_out));
-          for (CeedInt d = 0; d < dim; d++) e_mode_out[num_e_mode_out + d] = e_mode;
-          num_e_mode_out += dim;
-          break;
-        case CEED_EVAL_WEIGHT:
-        case CEED_EVAL_DIV:
-        case CEED_EVAL_CURL:
-          break;  // Caught by QF Assembly
+      CeedBasis    basis;
+      CeedEvalMode eval_mode;
+
+      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis));
+      CeedCheck(!basis_out || basis_out == basis, ceed, CEED_ERROR_BACKEND,
+                "Backend does not implement operator diagonal assembly with multiple active bases");
+      basis_out = basis;
+      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_out, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_out + q_comp, &eval_modes_out));
+        for (CeedInt d = 0; d < q_comp; d++) eval_modes_out[num_eval_modes_out + d] = eval_mode;
+        num_eval_modes_out += q_comp;
       }
     }
   }
@@ -733,139 +653,189 @@ static inline int CeedOperatorAssembleDiagonalSetup_Cuda(CeedOperator op, const
   CeedCallBackend(CeedCalloc(1, &impl->diag));
   CeedOperatorDiag_Cuda *diag = impl->diag;
 
-  diag->basis_in       = basis_in;
-  diag->basis_out      = basis_out;
-  diag->h_e_mode_in    = e_mode_in;
-  diag->h_e_mode_out   = e_mode_out;
-  diag->num_e_mode_in  = num_e_mode_in;
-  diag->num_e_mode_out = num_e_mode_out;
-
   // Assemble kernel
+  CeedCallBackend(CeedBasisGetNumNodes(basis_in, &num_nodes));
+  CeedCallBackend(CeedBasisGetNumComponents(basis_in, &num_comp));
+  if (basis_in == CEED_BASIS_NONE) num_qpts = num_nodes;
+  else CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts));
   CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/cuda/cuda-ref-operator-assemble-diagonal.h", &diagonal_kernel_path));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Diagonal Assembly Kernel Source -----\n");
   CeedCallBackend(CeedLoadSourceToBuffer(ceed, diagonal_kernel_path, &diagonal_kernel_source));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Diagonal Assembly Source Complete! -----\n");
-  CeedCallBackend(CeedBasisGetNumNodes(basis_in, &num_nodes));
-  CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts));
-  diag->num_nodes = num_nodes;
-  CeedCallCuda(ceed,
-               CeedCompile_Cuda(ceed, diagonal_kernel_source, &diag->module, 6, "NUM_E_MODE_IN", num_e_mode_in, "NUM_E_MODE_OUT", num_e_mode_out,
-                                "NUM_NODES", num_nodes, "NUM_QPTS", num_qpts, "NUM_COMP", num_comp, "USE_CEEDSIZE", use_ceedsize_idx));
-  CeedCallCuda(ceed, CeedGetKernel_Cuda(ceed, diag->module, "linearDiagonal", &diag->linearDiagonal));
-  CeedCallCuda(ceed, CeedGetKernel_Cuda(ceed, diag->module, "linearPointBlockDiagonal", &diag->linearPointBlock));
+  CeedCallCuda(
+      ceed, CeedCompile_Cuda(ceed, diagonal_kernel_source, &diag->module, 6, "NUM_EVAL_MODES_IN", num_eval_modes_in, "NUM_EVAL_MODES_OUT",
+                             num_eval_modes_out, "NUM_COMP", num_comp, "NUM_NODES", num_nodes, "NUM_QPTS", num_qpts, "CEED_SIZE", use_ceedsize_idx));
+  CeedCallCuda(ceed, CeedGetKernel_Cuda(ceed, diag->module, "LinearDiagonal", &diag->LinearDiagonal));
+  CeedCallCuda(ceed, CeedGetKernel_Cuda(ceed, diag->module, "LinearPointBlockDiagonal", &diag->LinearPointBlock));
   CeedCallBackend(CeedFree(&diagonal_kernel_path));
   CeedCallBackend(CeedFree(&diagonal_kernel_source));
 
   // Basis matrices
-  const CeedInt     q_bytes      = num_qpts * sizeof(CeedScalar);
-  const CeedInt     interp_bytes = q_bytes * num_nodes;
-  const CeedInt     grad_bytes   = q_bytes * num_nodes * dim;
-  const CeedInt     e_mode_bytes = sizeof(CeedEvalMode);
-  const CeedScalar *interp_in, *interp_out, *grad_in, *grad_out;
+  const CeedInt interp_bytes     = num_nodes * num_qpts * sizeof(CeedScalar);
+  const CeedInt eval_modes_bytes = sizeof(CeedEvalMode);
+  bool          has_eval_none    = false;
 
   // CEED_EVAL_NONE
-  CeedScalar *identity     = NULL;
-  bool        is_eval_none = false;
+  for (CeedInt i = 0; i < num_eval_modes_in; i++) has_eval_none = has_eval_none || (eval_modes_in[i] == CEED_EVAL_NONE);
+  for (CeedInt i = 0; i < num_eval_modes_out; i++) has_eval_none = has_eval_none || (eval_modes_out[i] == CEED_EVAL_NONE);
+  if (has_eval_none) {
+    CeedScalar *identity = NULL;
 
-  for (CeedInt i = 0; i < num_e_mode_in; i++) is_eval_none = is_eval_none || (e_mode_in[i] == CEED_EVAL_NONE);
-  for (CeedInt i = 0; i < num_e_mode_out; i++) is_eval_none = is_eval_none || (e_mode_out[i] == CEED_EVAL_NONE);
-  if (is_eval_none) {
-    CeedCallBackend(CeedCalloc(num_qpts * num_nodes, &identity));
+    CeedCallBackend(CeedCalloc(num_nodes * num_qpts, &identity));
     for (CeedInt i = 0; i < (num_nodes < num_qpts ? num_nodes : num_qpts); i++) identity[i * num_nodes + i] = 1.0;
     CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_identity, interp_bytes));
     CeedCallCuda(ceed, cudaMemcpy(diag->d_identity, identity, interp_bytes, cudaMemcpyHostToDevice));
+    CeedCallBackend(CeedFree(&identity));
+  }
+
+  // CEED_EVAL_INTERP, CEED_EVAL_GRAD, CEED_EVAL_DIV, and CEED_EVAL_CURL
+  for (CeedInt in = 0; in < 2; in++) {
+    CeedFESpace fespace;
+    CeedBasis   basis = in ? basis_in : basis_out;
+
+    CeedCallBackend(CeedBasisGetFESpace(basis, &fespace));
+    switch (fespace) {
+      case CEED_FE_SPACE_H1: {
+        CeedInt           q_comp_interp, q_comp_grad;
+        const CeedScalar *interp, *grad;
+        CeedScalar       *d_interp, *d_grad;
+
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_GRAD, &q_comp_grad));
+
+        CeedCallBackend(CeedBasisGetInterp(basis, &interp));
+        CeedCallCuda(ceed, cudaMalloc((void **)&d_interp, interp_bytes * q_comp_interp));
+        CeedCallCuda(ceed, cudaMemcpy(d_interp, interp, interp_bytes * q_comp_interp, cudaMemcpyHostToDevice));
+        CeedCallBackend(CeedBasisGetGrad(basis, &grad));
+        CeedCallCuda(ceed, cudaMalloc((void **)&d_grad, interp_bytes * q_comp_grad));
+        CeedCallCuda(ceed, cudaMemcpy(d_grad, grad, interp_bytes * q_comp_grad, cudaMemcpyHostToDevice));
+        if (in) {
+          diag->d_interp_in = d_interp;
+          diag->d_grad_in   = d_grad;
+        } else {
+          diag->d_interp_out = d_interp;
+          diag->d_grad_out   = d_grad;
+        }
+      } break;
+      case CEED_FE_SPACE_HDIV: {
+        CeedInt           q_comp_interp, q_comp_div;
+        const CeedScalar *interp, *div;
+        CeedScalar       *d_interp, *d_div;
+
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_DIV, &q_comp_div));
+
+        CeedCallBackend(CeedBasisGetInterp(basis, &interp));
+        CeedCallCuda(ceed, cudaMalloc((void **)&d_interp, interp_bytes * q_comp_interp));
+        CeedCallCuda(ceed, cudaMemcpy(d_interp, interp, interp_bytes * q_comp_interp, cudaMemcpyHostToDevice));
+        CeedCallBackend(CeedBasisGetDiv(basis, &div));
+        CeedCallCuda(ceed, cudaMalloc((void **)&d_div, interp_bytes * q_comp_div));
+        CeedCallCuda(ceed, cudaMemcpy(d_div, div, interp_bytes * q_comp_div, cudaMemcpyHostToDevice));
+        if (in) {
+          diag->d_interp_in = d_interp;
+          diag->d_div_in    = d_div;
+        } else {
+          diag->d_interp_out = d_interp;
+          diag->d_div_out    = d_div;
+        }
+      } break;
+      case CEED_FE_SPACE_HCURL: {
+        CeedInt           q_comp_interp, q_comp_curl;
+        const CeedScalar *interp, *curl;
+        CeedScalar       *d_interp, *d_curl;
+
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_CURL, &q_comp_curl));
+
+        CeedCallBackend(CeedBasisGetInterp(basis, &interp));
+        CeedCallCuda(ceed, cudaMalloc((void **)&d_interp, interp_bytes * q_comp_interp));
+        CeedCallCuda(ceed, cudaMemcpy(d_interp, interp, interp_bytes * q_comp_interp, cudaMemcpyHostToDevice));
+        CeedCallBackend(CeedBasisGetCurl(basis, &curl));
+        CeedCallCuda(ceed, cudaMalloc((void **)&d_curl, interp_bytes * q_comp_curl));
+        CeedCallCuda(ceed, cudaMemcpy(d_curl, curl, interp_bytes * q_comp_curl, cudaMemcpyHostToDevice));
+        if (in) {
+          diag->d_interp_in = d_interp;
+          diag->d_curl_in   = d_curl;
+        } else {
+          diag->d_interp_out = d_interp;
+          diag->d_curl_out   = d_curl;
+        }
+      } break;
+    }
   }
 
-  // CEED_EVAL_INTERP
-  CeedCallBackend(CeedBasisGetInterp(basis_in, &interp_in));
-  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_interp_in, interp_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(diag->d_interp_in, interp_in, interp_bytes, cudaMemcpyHostToDevice));
-  CeedCallBackend(CeedBasisGetInterp(basis_out, &interp_out));
-  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_interp_out, interp_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(diag->d_interp_out, interp_out, interp_bytes, cudaMemcpyHostToDevice));
-
-  // CEED_EVAL_GRAD
-  CeedCallBackend(CeedBasisGetGrad(basis_in, &grad_in));
-  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_grad_in, grad_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(diag->d_grad_in, grad_in, grad_bytes, cudaMemcpyHostToDevice));
-  CeedCallBackend(CeedBasisGetGrad(basis_out, &grad_out));
-  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_grad_out, grad_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(diag->d_grad_out, grad_out, grad_bytes, cudaMemcpyHostToDevice));
-
-  // Arrays of e_modes
-  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_e_mode_in, num_e_mode_in * e_mode_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(diag->d_e_mode_in, e_mode_in, num_e_mode_in * e_mode_bytes, cudaMemcpyHostToDevice));
-  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_e_mode_out, num_e_mode_out * e_mode_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(diag->d_e_mode_out, e_mode_out, num_e_mode_out * e_mode_bytes, cudaMemcpyHostToDevice));
-
-  // Restriction
-  diag->diag_rstr = rstr_out;
+  // Arrays of eval_modes
+  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_eval_modes_in, num_eval_modes_in * eval_modes_bytes));
+  CeedCallCuda(ceed, cudaMemcpy(diag->d_eval_modes_in, eval_modes_in, num_eval_modes_in * eval_modes_bytes, cudaMemcpyHostToDevice));
+  CeedCallCuda(ceed, cudaMalloc((void **)&diag->d_eval_modes_out, num_eval_modes_out * eval_modes_bytes));
+  CeedCallCuda(ceed, cudaMemcpy(diag->d_eval_modes_out, eval_modes_out, num_eval_modes_out * eval_modes_bytes, cudaMemcpyHostToDevice));
+  CeedCallBackend(CeedFree(&eval_modes_in));
+  CeedCallBackend(CeedFree(&eval_modes_out));
   return CEED_ERROR_SUCCESS;
 }
 
 //------------------------------------------------------------------------------
-// Assemble diagonal common code
+// Assemble Diagonal Core
 //------------------------------------------------------------------------------
 static inline int CeedOperatorAssembleDiagonalCore_Cuda(CeedOperator op, CeedVector assembled, CeedRequest *request, const bool is_point_block) {
   Ceed                ceed;
-  CeedSize            assembled_length = 0, assembled_qf_length = 0;
-  CeedInt             use_ceedsize_idx = 0, num_elem;
+  CeedSize            assembled_length, assembled_qf_length;
+  CeedInt             use_ceedsize_idx = 0, num_elem, num_nodes;
   CeedScalar         *elem_diag_array;
   const CeedScalar   *assembled_qf_array;
-  CeedVector          assembled_qf = NULL;
-  CeedElemRestriction rstr         = NULL;
+  CeedVector          assembled_qf   = NULL, elem_diag;
+  CeedElemRestriction assembled_rstr = NULL, rstr_in, rstr_out, diag_rstr;
   CeedOperator_Cuda  *impl;
 
   CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
   CeedCallBackend(CeedOperatorGetData(op, &impl));
 
   // Assemble QFunction
-  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &rstr, request));
-  CeedCallBackend(CeedElemRestrictionDestroy(&rstr));
+  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &assembled_rstr, request));
+  CeedCallBackend(CeedElemRestrictionDestroy(&assembled_rstr));
+  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &assembled_qf_array));
 
   CeedCallBackend(CeedVectorGetLength(assembled, &assembled_length));
   CeedCallBackend(CeedVectorGetLength(assembled_qf, &assembled_qf_length));
   if ((assembled_length > INT_MAX) || (assembled_qf_length > INT_MAX)) use_ceedsize_idx = 1;
 
   // Setup
-  if (!impl->diag) CeedCallBackend(CeedOperatorAssembleDiagonalSetup_Cuda(op, is_point_block, use_ceedsize_idx));
+  if (!impl->diag) CeedCallBackend(CeedOperatorAssembleDiagonalSetup_Cuda(op, use_ceedsize_idx));
   CeedOperatorDiag_Cuda *diag = impl->diag;
 
   assert(diag != NULL);
 
-  // Restriction
-  if (is_point_block && !diag->point_block_rstr) {
-    CeedElemRestriction point_block_rstr;
-
-    CeedCallBackend(CreatePointBlockRestriction(diag->diag_rstr, &point_block_rstr));
-    diag->point_block_rstr = point_block_rstr;
-  }
-  CeedElemRestriction diag_rstr = is_point_block ? diag->point_block_rstr : diag->diag_rstr;
-
-  // Create diagonal vector
-  CeedVector elem_diag = is_point_block ? diag->point_block_elem_diag : diag->elem_diag;
-
-  if (!elem_diag) {
-    CeedCallBackend(CeedElemRestrictionCreateVector(diag_rstr, NULL, &elem_diag));
-    if (is_point_block) diag->point_block_elem_diag = elem_diag;
-    else diag->elem_diag = elem_diag;
+  // Restriction and diagonal vector
+  CeedCallBackend(CeedOperatorGetActiveElemRestrictions(op, &rstr_in, &rstr_out));
+  CeedCheck(rstr_in == rstr_out, ceed, CEED_ERROR_BACKEND,
+            "Cannot assemble operator diagonal with different input and output active element restrictions");
+  if (!is_point_block && !diag->diag_rstr) {
+    CeedCallBackend(CeedElemRestrictionCreateUnsignedCopy(rstr_out, &diag->diag_rstr));
+    CeedCallBackend(CeedElemRestrictionCreateVector(diag->diag_rstr, NULL, &diag->elem_diag));
+  } else if (is_point_block && !diag->point_block_diag_rstr) {
+    CeedCallBackend(CeedOperatorCreateActivePointBlockRestriction(rstr_out, &diag->point_block_diag_rstr));
+    CeedCallBackend(CeedElemRestrictionCreateVector(diag->point_block_diag_rstr, NULL, &diag->point_block_elem_diag));
   }
+  diag_rstr = is_point_block ? diag->point_block_diag_rstr : diag->diag_rstr;
+  elem_diag = is_point_block ? diag->point_block_elem_diag : diag->elem_diag;
   CeedCallBackend(CeedVectorSetValue(elem_diag, 0.0));
 
   // Assemble element operator diagonals
   CeedCallBackend(CeedVectorGetArray(elem_diag, CEED_MEM_DEVICE, &elem_diag_array));
-  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &assembled_qf_array));
   CeedCallBackend(CeedElemRestrictionGetNumElements(diag_rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(diag_rstr, &num_nodes));
 
   // Compute the diagonal of B^T D B
-  int   elem_per_block = 1;
-  int   grid           = num_elem / elem_per_block + ((num_elem / elem_per_block * elem_per_block < num_elem) ? 1 : 0);
-  void *args[]         = {(void *)&num_elem, &diag->d_identity,  &diag->d_interp_in,  &diag->d_grad_in,    &diag->d_interp_out,
-                          &diag->d_grad_out, &diag->d_e_mode_in, &diag->d_e_mode_out, &assembled_qf_array, &elem_diag_array};
+  CeedInt elems_per_block = 1;
+  CeedInt grid            = CeedDivUpInt(num_elem, elems_per_block);
+  void   *args[]          = {(void *)&num_elem,      &diag->d_identity,       &diag->d_interp_in,  &diag->d_grad_in, &diag->d_div_in,
+                             &diag->d_curl_in,       &diag->d_interp_out,     &diag->d_grad_out,   &diag->d_div_out, &diag->d_curl_out,
+                             &diag->d_eval_modes_in, &diag->d_eval_modes_out, &assembled_qf_array, &elem_diag_array};
+
   if (is_point_block) {
-    CeedCallBackend(CeedRunKernelDim_Cuda(ceed, diag->linearPointBlock, grid, diag->num_nodes, 1, elem_per_block, args));
+    CeedCallBackend(CeedRunKernelDim_Cuda(ceed, diag->LinearPointBlock, grid, num_nodes, 1, elems_per_block, args));
   } else {
-    CeedCallBackend(CeedRunKernelDim_Cuda(ceed, diag->linearDiagonal, grid, diag->num_nodes, 1, elem_per_block, args));
+    CeedCallBackend(CeedRunKernelDim_Cuda(ceed, diag->LinearDiagonal, grid, num_nodes, 1, elems_per_block, args));
   }
 
   // Restore arrays
@@ -897,14 +867,15 @@ static int CeedOperatorLinearAssembleAddPointBlockDiagonal_Cuda(CeedOperator op,
 }
 
 //------------------------------------------------------------------------------
-// Single operator assembly setup
+// Single Operator Assembly Setup
 //------------------------------------------------------------------------------
 static int CeedSingleOperatorAssembleSetup_Cuda(CeedOperator op, CeedInt use_ceedsize_idx) {
-  Ceed    ceed;
-  char   *assembly_kernel_path, *assembly_kernel_source;
-  CeedInt num_input_fields, num_output_fields, num_e_mode_in = 0, dim = 1, num_B_in_mats_to_load = 0, size_B_in = 0, num_qpts = 0, elem_size = 0,
-                                               num_e_mode_out = 0, num_B_out_mats_to_load = 0, size_B_out = 0, num_elem, num_comp;
-  CeedEvalMode       *eval_mode_in = NULL, *eval_mode_out = NULL;
+  Ceed                ceed;
+  Ceed_Cuda          *cuda_data;
+  char               *assembly_kernel_path, *assembly_kernel_source;
+  CeedInt             num_input_fields, num_output_fields, num_eval_modes_in = 0, num_eval_modes_out = 0;
+  CeedInt             elem_size_in, num_qpts_in, num_comp_in, elem_size_out, num_qpts_out, num_comp_out, q_comp;
+  CeedEvalMode       *eval_modes_in = NULL, *eval_modes_out = NULL;
   CeedElemRestriction rstr_in = NULL, rstr_out = NULL;
   CeedBasis           basis_in = NULL, basis_out = NULL;
   CeedQFunctionField *qf_fields;
@@ -921,34 +892,30 @@ static int CeedSingleOperatorAssembleSetup_Cuda(CeedOperator op, CeedInt use_cee
   // Determine active input basis eval mode
   CeedCallBackend(CeedOperatorGetQFunction(op, &qf));
   CeedCallBackend(CeedQFunctionGetFields(qf, NULL, &qf_fields, NULL, NULL));
-  // Note that the kernel will treat each dimension of a gradient action separately;
-  // i.e., when an active input has a CEED_EVAL_GRAD mode, num_e_mode_in will increment by dim.
-  // However, for the purposes of loading the B matrices, it will be treated as one mode, and we will load/copy the entire gradient matrix at once, so
-  // num_B_in_mats_to_load will be incremented by 1.
   for (CeedInt i = 0; i < num_input_fields; i++) {
     CeedVector vec;
 
     CeedCallBackend(CeedOperatorFieldGetVector(input_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
+      CeedBasis    basis;
       CeedEvalMode eval_mode;
 
-      CeedCallBackend(CeedOperatorFieldGetBasis(input_fields[i], &basis_in));
-      CeedCallBackend(CeedBasisGetDimension(basis_in, &dim));
-      CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts));
+      CeedCallBackend(CeedOperatorFieldGetBasis(input_fields[i], &basis));
+      CeedCheck(!basis_in || basis_in == basis, ceed, CEED_ERROR_BACKEND, "Backend does not implement operator assembly with multiple active bases");
+      basis_in = basis;
       CeedCallBackend(CeedOperatorFieldGetElemRestriction(input_fields[i], &rstr_in));
-      CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_in, &elem_size));
+      CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_in, &elem_size_in));
+      if (basis_in == CEED_BASIS_NONE) num_qpts_in = elem_size_in;
+      else CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts_in));
       CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
-      if (eval_mode != CEED_EVAL_NONE) {
-        CeedCallBackend(CeedRealloc(num_B_in_mats_to_load + 1, &eval_mode_in));
-        eval_mode_in[num_B_in_mats_to_load] = eval_mode;
-        num_B_in_mats_to_load += 1;
-        if (eval_mode == CEED_EVAL_GRAD) {
-          num_e_mode_in += dim;
-          size_B_in += dim * elem_size * num_qpts;
-        } else {
-          num_e_mode_in += 1;
-          size_B_in += elem_size * num_qpts;
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_in, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF Assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_in + q_comp, &eval_modes_in));
+        for (CeedInt d = 0; d < q_comp; d++) {
+          eval_modes_in[num_eval_modes_in + d] = eval_mode;
         }
+        num_eval_modes_in += q_comp;
       }
     }
   }
@@ -960,112 +927,134 @@ static int CeedSingleOperatorAssembleSetup_Cuda(CeedOperator op, CeedInt use_cee
 
     CeedCallBackend(CeedOperatorFieldGetVector(output_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
+      CeedBasis    basis;
       CeedEvalMode eval_mode;
 
-      CeedCallBackend(CeedOperatorFieldGetBasis(output_fields[i], &basis_out));
+      CeedCallBackend(CeedOperatorFieldGetBasis(output_fields[i], &basis));
+      CeedCheck(!basis_out || basis_out == basis, ceed, CEED_ERROR_BACKEND,
+                "Backend does not implement operator assembly with multiple active bases");
+      basis_out = basis;
       CeedCallBackend(CeedOperatorFieldGetElemRestriction(output_fields[i], &rstr_out));
-      CeedCheck(!rstr_out || rstr_out == rstr_in, ceed, CEED_ERROR_BACKEND, "Backend does not implement multi-field non-composite operator assembly");
+      CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_out, &elem_size_out));
+      if (basis_out == CEED_BASIS_NONE) num_qpts_out = elem_size_out;
+      else CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_out, &num_qpts_out));
+      CeedCheck(num_qpts_in == num_qpts_out, ceed, CEED_ERROR_UNSUPPORTED,
+                "Active input and output bases must have the same number of quadrature points");
       CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
-      if (eval_mode != CEED_EVAL_NONE) {
-        CeedCallBackend(CeedRealloc(num_B_out_mats_to_load + 1, &eval_mode_out));
-        eval_mode_out[num_B_out_mats_to_load] = eval_mode;
-        num_B_out_mats_to_load += 1;
-        if (eval_mode == CEED_EVAL_GRAD) {
-          num_e_mode_out += dim;
-          size_B_out += dim * elem_size * num_qpts;
-        } else {
-          num_e_mode_out += 1;
-          size_B_out += elem_size * num_qpts;
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_out, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF Assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_out + q_comp, &eval_modes_out));
+        for (CeedInt d = 0; d < q_comp; d++) {
+          eval_modes_out[num_eval_modes_out + d] = eval_mode;
         }
+        num_eval_modes_out += q_comp;
       }
     }
   }
-  CeedCheck(num_e_mode_in > 0 && num_e_mode_out > 0, ceed, CEED_ERROR_UNSUPPORTED, "Cannot assemble operator without inputs/outputs");
-
-  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr_in, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr_in, &num_comp));
+  CeedCheck(num_eval_modes_in > 0 && num_eval_modes_out > 0, ceed, CEED_ERROR_UNSUPPORTED, "Cannot assemble operator without inputs/outputs");
 
   CeedCallBackend(CeedCalloc(1, &impl->asmb));
   CeedOperatorAssemble_Cuda *asmb = impl->asmb;
-  asmb->num_elem                  = num_elem;
-
-  // Compile kernels
-  int elem_per_block    = 1;
-  asmb->elem_per_block  = elem_per_block;
-  CeedInt    block_size = elem_size * elem_size * elem_per_block;
-  Ceed_Cuda *cuda_data;
+  asmb->elems_per_block           = 1;
+  asmb->block_size_x              = elem_size_in;
+  asmb->block_size_y              = elem_size_out;
 
   CeedCallBackend(CeedGetData(ceed, &cuda_data));
-  CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/cuda/cuda-ref-operator-assemble.h", &assembly_kernel_path));
-  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Assembly Kernel Source -----\n");
-  CeedCallBackend(CeedLoadSourceToBuffer(ceed, assembly_kernel_path, &assembly_kernel_source));
-  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Assembly Source Complete! -----\n");
-  bool fallback = block_size > cuda_data->device_prop.maxThreadsPerBlock;
+  bool fallback = asmb->block_size_x * asmb->block_size_y * asmb->elems_per_block > cuda_data->device_prop.maxThreadsPerBlock;
 
   if (fallback) {
     // Use fallback kernel with 1D threadblock
-    block_size         = elem_size * elem_per_block;
-    asmb->block_size_x = elem_size;
     asmb->block_size_y = 1;
-  } else {  // Use kernel with 2D threadblock
-    asmb->block_size_x = elem_size;
-    asmb->block_size_y = elem_size;
   }
-  CeedCallBackend(CeedCompile_Cuda(ceed, assembly_kernel_source, &asmb->module, 8, "NUM_ELEM", num_elem, "NUM_E_MODE_IN", num_e_mode_in,
-                                   "NUM_E_MODE_OUT", num_e_mode_out, "NUM_QPTS", num_qpts, "NUM_NODES", elem_size, "BLOCK_SIZE", block_size,
-                                   "NUM_COMP", num_comp, "USE_CEEDSIZE", use_ceedsize_idx));
-  CeedCallBackend(CeedGetKernel_Cuda(ceed, asmb->module, fallback ? "linearAssembleFallback" : "linearAssemble", &asmb->linearAssemble));
+
+  // Compile kernels
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr_in, &num_comp_in));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr_out, &num_comp_out));
+  CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/cuda/cuda-ref-operator-assemble.h", &assembly_kernel_path));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Assembly Kernel Source -----\n");
+  CeedCallBackend(CeedLoadSourceToBuffer(ceed, assembly_kernel_path, &assembly_kernel_source));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Assembly Source Complete! -----\n");
+  CeedCallBackend(CeedCompile_Cuda(ceed, assembly_kernel_source, &asmb->module, 10, "NUM_EVAL_MODES_IN", num_eval_modes_in, "NUM_EVAL_MODES_OUT",
+                                   num_eval_modes_out, "NUM_COMP_IN", num_comp_in, "NUM_COMP_OUT", num_comp_out, "NUM_NODES_IN", elem_size_in,
+                                   "NUM_NODES_OUT", elem_size_out, "NUM_QPTS", num_qpts_in, "BLOCK_SIZE",
+                                   asmb->block_size_x * asmb->block_size_y * asmb->elems_per_block, "BLOCK_SIZE_Y", asmb->block_size_y, "CEED_SIZE",
+                                   use_ceedsize_idx));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, asmb->module, "LinearAssemble", &asmb->LinearAssemble));
   CeedCallBackend(CeedFree(&assembly_kernel_path));
   CeedCallBackend(CeedFree(&assembly_kernel_source));
 
-  // Build 'full' B matrices (not 1D arrays used for tensor-product matrices)
-  const CeedScalar *interp_in, *grad_in;
+  // Load into B_in, in order that they will be used in eval_modes_in
+  {
+    const CeedInt in_bytes           = elem_size_in * num_qpts_in * num_eval_modes_in * sizeof(CeedScalar);
+    CeedInt       d_in               = 0;
+    CeedEvalMode  eval_modes_in_prev = CEED_EVAL_NONE;
+    bool          has_eval_none      = false;
+    CeedScalar   *identity           = NULL;
+
+    for (CeedInt i = 0; i < num_eval_modes_in; i++) {
+      has_eval_none = has_eval_none || (eval_modes_in[i] == CEED_EVAL_NONE);
+    }
+    if (has_eval_none) {
+      CeedCallBackend(CeedCalloc(elem_size_in * num_qpts_in, &identity));
+      for (CeedInt i = 0; i < (elem_size_in < num_qpts_in ? elem_size_in : num_qpts_in); i++) identity[i * elem_size_in + i] = 1.0;
+    }
 
-  CeedCallBackend(CeedBasisGetInterp(basis_in, &interp_in));
-  CeedCallBackend(CeedBasisGetGrad(basis_in, &grad_in));
+    CeedCallCuda(ceed, cudaMalloc((void **)&asmb->d_B_in, in_bytes));
+    for (CeedInt i = 0; i < num_eval_modes_in; i++) {
+      const CeedScalar *h_B_in;
 
-  // Load into B_in, in order that they will be used in eval_mode
-  const CeedInt inBytes   = size_B_in * sizeof(CeedScalar);
-  CeedInt       mat_start = 0;
+      CeedCallBackend(CeedOperatorGetBasisPointer(basis_in, eval_modes_in[i], identity, &h_B_in));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_in, eval_modes_in[i], &q_comp));
+      if (q_comp > 1) {
+        if (i == 0 || eval_modes_in[i] != eval_modes_in_prev) d_in = 0;
+        else h_B_in = &h_B_in[(++d_in) * elem_size_in * num_qpts_in];
+      }
+      eval_modes_in_prev = eval_modes_in[i];
 
-  CeedCallCuda(ceed, cudaMalloc((void **)&asmb->d_B_in, inBytes));
-  for (int i = 0; i < num_B_in_mats_to_load; i++) {
-    CeedEvalMode eval_mode = eval_mode_in[i];
+      CeedCallCuda(ceed, cudaMemcpy(&asmb->d_B_in[i * elem_size_in * num_qpts_in], h_B_in, elem_size_in * num_qpts_in * sizeof(CeedScalar),
+                                    cudaMemcpyHostToDevice));
+    }
 
-    if (eval_mode == CEED_EVAL_INTERP) {
-      CeedCallCuda(ceed, cudaMemcpy(&asmb->d_B_in[mat_start], interp_in, elem_size * num_qpts * sizeof(CeedScalar), cudaMemcpyHostToDevice));
-      mat_start += elem_size * num_qpts;
-    } else if (eval_mode == CEED_EVAL_GRAD) {
-      CeedCallCuda(ceed, cudaMemcpy(&asmb->d_B_in[mat_start], grad_in, dim * elem_size * num_qpts * sizeof(CeedScalar), cudaMemcpyHostToDevice));
-      mat_start += dim * elem_size * num_qpts;
+    if (identity) {
+      CeedCallBackend(CeedFree(&identity));
     }
   }
 
-  const CeedScalar *interp_out, *grad_out;
+  // Load into B_out, in order that they will be used in eval_modes_out
+  {
+    const CeedInt out_bytes           = elem_size_out * num_qpts_out * num_eval_modes_out * sizeof(CeedScalar);
+    CeedInt       d_out               = 0;
+    CeedEvalMode  eval_modes_out_prev = CEED_EVAL_NONE;
+    bool          has_eval_none       = false;
+    CeedScalar   *identity            = NULL;
 
-  // Note that this function currently assumes 1 basis, so this should always be true for now
-  if (basis_out == basis_in) {
-    interp_out = interp_in;
-    grad_out   = grad_in;
-  } else {
-    CeedCallBackend(CeedBasisGetInterp(basis_out, &interp_out));
-    CeedCallBackend(CeedBasisGetGrad(basis_out, &grad_out));
-  }
+    for (CeedInt i = 0; i < num_eval_modes_out; i++) {
+      has_eval_none = has_eval_none || (eval_modes_out[i] == CEED_EVAL_NONE);
+    }
+    if (has_eval_none) {
+      CeedCallBackend(CeedCalloc(elem_size_out * num_qpts_out, &identity));
+      for (CeedInt i = 0; i < (elem_size_out < num_qpts_out ? elem_size_out : num_qpts_out); i++) identity[i * elem_size_out + i] = 1.0;
+    }
 
-  // Load into B_out, in order that they will be used in eval_mode
-  const CeedInt outBytes = size_B_out * sizeof(CeedScalar);
-  mat_start              = 0;
+    CeedCallCuda(ceed, cudaMalloc((void **)&asmb->d_B_out, out_bytes));
+    for (CeedInt i = 0; i < num_eval_modes_out; i++) {
+      const CeedScalar *h_B_out;
 
-  CeedCallCuda(ceed, cudaMalloc((void **)&asmb->d_B_out, outBytes));
-  for (int i = 0; i < num_B_out_mats_to_load; i++) {
-    CeedEvalMode eval_mode = eval_mode_out[i];
+      CeedCallBackend(CeedOperatorGetBasisPointer(basis_out, eval_modes_out[i], identity, &h_B_out));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_out, eval_modes_out[i], &q_comp));
+      if (q_comp > 1) {
+        if (i == 0 || eval_modes_out[i] != eval_modes_out_prev) d_out = 0;
+        else h_B_out = &h_B_out[(++d_out) * elem_size_out * num_qpts_out];
+      }
+      eval_modes_out_prev = eval_modes_out[i];
+
+      CeedCallCuda(ceed, cudaMemcpy(&asmb->d_B_out[i * elem_size_out * num_qpts_out], h_B_out, elem_size_out * num_qpts_out * sizeof(CeedScalar),
+                                    cudaMemcpyHostToDevice));
+    }
 
-    if (eval_mode == CEED_EVAL_INTERP) {
-      CeedCallCuda(ceed, cudaMemcpy(&asmb->d_B_out[mat_start], interp_out, elem_size * num_qpts * sizeof(CeedScalar), cudaMemcpyHostToDevice));
-      mat_start += elem_size * num_qpts;
-    } else if (eval_mode == CEED_EVAL_GRAD) {
-      CeedCallCuda(ceed, cudaMemcpy(&asmb->d_B_out[mat_start], grad_out, dim * elem_size * num_qpts * sizeof(CeedScalar), cudaMemcpyHostToDevice));
-      mat_start += dim * elem_size * num_qpts;
+    if (identity) {
+      CeedCallBackend(CeedFree(&identity));
     }
   }
   return CEED_ERROR_SUCCESS;
@@ -1082,47 +1071,96 @@ static int CeedSingleOperatorAssembleSetup_Cuda(CeedOperator op, CeedInt use_cee
 static int CeedSingleOperatorAssemble_Cuda(CeedOperator op, CeedInt offset, CeedVector values) {
   Ceed                ceed;
   CeedSize            values_length = 0, assembled_qf_length = 0;
-  CeedInt             use_ceedsize_idx = 0;
+  CeedInt             use_ceedsize_idx = 0, num_elem_in, num_elem_out, elem_size_in, elem_size_out;
   CeedScalar         *values_array;
-  const CeedScalar   *qf_array;
-  CeedVector          assembled_qf = NULL;
-  CeedElemRestriction rstr_q       = NULL;
+  const CeedScalar   *assembled_qf_array;
+  CeedVector          assembled_qf   = NULL;
+  CeedElemRestriction assembled_rstr = NULL, rstr_in, rstr_out;
+  CeedRestrictionType rstr_type_in, rstr_type_out;
+  const bool         *orients_in = NULL, *orients_out = NULL;
+  const CeedInt8     *curl_orients_in = NULL, *curl_orients_out = NULL;
   CeedOperator_Cuda  *impl;
 
   CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
   CeedCallBackend(CeedOperatorGetData(op, &impl));
 
   // Assemble QFunction
-  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &rstr_q, CEED_REQUEST_IMMEDIATE));
-  CeedCallBackend(CeedElemRestrictionDestroy(&rstr_q));
-  CeedCallBackend(CeedVectorGetArray(values, CEED_MEM_DEVICE, &values_array));
-  values_array += offset;
-  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &qf_array));
+  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &assembled_rstr, CEED_REQUEST_IMMEDIATE));
+  CeedCallBackend(CeedElemRestrictionDestroy(&assembled_rstr));
+  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &assembled_qf_array));
 
   CeedCallBackend(CeedVectorGetLength(values, &values_length));
   CeedCallBackend(CeedVectorGetLength(assembled_qf, &assembled_qf_length));
   if ((values_length > INT_MAX) || (assembled_qf_length > INT_MAX)) use_ceedsize_idx = 1;
+
   // Setup
-  if (!impl->asmb) {
-    CeedCallBackend(CeedSingleOperatorAssembleSetup_Cuda(op, use_ceedsize_idx));
-    assert(impl->asmb != NULL);
+  if (!impl->asmb) CeedCallBackend(CeedSingleOperatorAssembleSetup_Cuda(op, use_ceedsize_idx));
+  CeedOperatorAssemble_Cuda *asmb = impl->asmb;
+
+  assert(asmb != NULL);
+
+  // Assemble element operator
+  CeedCallBackend(CeedVectorGetArray(values, CEED_MEM_DEVICE, &values_array));
+  values_array += offset;
+
+  CeedCallBackend(CeedOperatorGetActiveElemRestrictions(op, &rstr_in, &rstr_out));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr_in, &num_elem_in));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_in, &elem_size_in));
+
+  CeedCallBackend(CeedElemRestrictionGetType(rstr_in, &rstr_type_in));
+  if (rstr_type_in == CEED_RESTRICTION_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionGetOrientations(rstr_in, CEED_MEM_DEVICE, &orients_in));
+  } else if (rstr_type_in == CEED_RESTRICTION_CURL_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionGetCurlOrientations(rstr_in, CEED_MEM_DEVICE, &curl_orients_in));
+  }
+
+  if (rstr_in != rstr_out) {
+    CeedCallBackend(CeedElemRestrictionGetNumElements(rstr_out, &num_elem_out));
+    CeedCheck(num_elem_in == num_elem_out, ceed, CEED_ERROR_UNSUPPORTED,
+              "Active input and output operator restrictions must have the same number of elements");
+    CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_out, &elem_size_out));
+
+    CeedCallBackend(CeedElemRestrictionGetType(rstr_out, &rstr_type_out));
+    if (rstr_type_out == CEED_RESTRICTION_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionGetOrientations(rstr_out, CEED_MEM_DEVICE, &orients_out));
+    } else if (rstr_type_out == CEED_RESTRICTION_CURL_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionGetCurlOrientations(rstr_out, CEED_MEM_DEVICE, &curl_orients_out));
+    }
+  } else {
+    elem_size_out    = elem_size_in;
+    orients_out      = orients_in;
+    curl_orients_out = curl_orients_in;
   }
 
   // Compute B^T D B
-  const CeedInt num_elem       = impl->asmb->num_elem;
-  const CeedInt elem_per_block = impl->asmb->elem_per_block;
-  const CeedInt grid           = num_elem / elem_per_block + ((num_elem / elem_per_block * elem_per_block < num_elem) ? 1 : 0);
-  void         *args[]         = {&impl->asmb->d_B_in, &impl->asmb->d_B_out, &qf_array, &values_array};
+  CeedInt shared_mem =
+      ((curl_orients_in || curl_orients_out ? elem_size_in * elem_size_out : 0) + (curl_orients_in ? elem_size_in * asmb->block_size_y : 0)) *
+      sizeof(CeedScalar);
+  CeedInt grid   = CeedDivUpInt(num_elem_in, asmb->elems_per_block);
+  void   *args[] = {(void *)&num_elem_in, &asmb->d_B_in,     &asmb->d_B_out,      &orients_in,  &curl_orients_in,
+                    &orients_out,         &curl_orients_out, &assembled_qf_array, &values_array};
 
   CeedCallBackend(
-      CeedRunKernelDim_Cuda(ceed, impl->asmb->linearAssemble, grid, impl->asmb->block_size_x, impl->asmb->block_size_y, elem_per_block, args));
+      CeedRunKernelDimShared_Cuda(ceed, asmb->LinearAssemble, grid, asmb->block_size_x, asmb->block_size_y, asmb->elems_per_block, shared_mem, args));
 
   // Restore arrays
   CeedCallBackend(CeedVectorRestoreArray(values, &values_array));
-  CeedCallBackend(CeedVectorRestoreArrayRead(assembled_qf, &qf_array));
+  CeedCallBackend(CeedVectorRestoreArrayRead(assembled_qf, &assembled_qf_array));
 
   // Cleanup
   CeedCallBackend(CeedVectorDestroy(&assembled_qf));
+  if (rstr_type_in == CEED_RESTRICTION_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionRestoreOrientations(rstr_in, &orients_in));
+  } else if (rstr_type_in == CEED_RESTRICTION_CURL_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionRestoreCurlOrientations(rstr_in, &curl_orients_in));
+  }
+  if (rstr_in != rstr_out) {
+    if (rstr_type_out == CEED_RESTRICTION_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionRestoreOrientations(rstr_out, &orients_out));
+    } else if (rstr_type_out == CEED_RESTRICTION_CURL_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionRestoreCurlOrientations(rstr_out, &curl_orients_out));
+    }
+  }
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/backends/cuda-ref/ceed-cuda-ref-restriction.c b/backends/cuda-ref/ceed-cuda-ref-restriction.c
index 71ed2821..575ce774 100644
--- a/backends/cuda-ref/ceed-cuda-ref-restriction.c
+++ b/backends/cuda-ref/ceed-cuda-ref-restriction.c
@@ -19,22 +19,23 @@
 #include "ceed-cuda-ref.h"
 
 //------------------------------------------------------------------------------
-// Apply restriction
+// Core apply restriction code
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionApply_Cuda(CeedElemRestriction r, CeedTransposeMode t_mode, CeedVector u, CeedVector v, CeedRequest *request) {
+static inline int CeedElemRestrictionApply_Cuda_Core(CeedElemRestriction rstr, CeedTransposeMode t_mode, bool use_signs, bool use_orients,
+                                                     CeedVector u, CeedVector v, CeedRequest *request) {
   Ceed                      ceed;
-  Ceed_Cuda                *data;
-  CUfunction                kernel;
   CeedInt                   num_elem, elem_size;
+  CeedRestrictionType       rstr_type;
+  CUfunction                kernel;
   const CeedScalar         *d_u;
   CeedScalar               *d_v;
   CeedElemRestriction_Cuda *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedGetData(ceed, &data));
-  CeedElemRestrictionGetNumElements(r, &num_elem);
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetType(rstr, &rstr_type));
   const CeedInt num_nodes = impl->num_nodes;
 
   // Get vectors
@@ -50,45 +51,124 @@ static int CeedElemRestrictionApply_Cuda(CeedElemRestriction r, CeedTransposeMod
   // Restrict
   if (t_mode == CEED_NOTRANSPOSE) {
     // L-vector -> E-vector
-    if (impl->d_ind) {
-      // -- Offsets provided
-      kernel             = impl->OffsetNoTranspose;
-      void   *args[]     = {&num_elem, &impl->d_ind, &d_u, &d_v};
-      CeedInt block_size = elem_size < 1024 ? (elem_size > 32 ? elem_size : 32) : 1024;
-
-      CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
-    } else {
-      // -- Strided restriction
-      kernel             = impl->StridedNoTranspose;
-      void   *args[]     = {&num_elem, &d_u, &d_v};
-      CeedInt block_size = elem_size < 1024 ? (elem_size > 32 ? elem_size : 32) : 1024;
-
-      CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
+    const CeedInt block_size = elem_size < 1024 ? (elem_size > 32 ? elem_size : 32) : 1024;
+    const CeedInt grid       = CeedDivUpInt(num_nodes, block_size);
+
+    switch (rstr_type) {
+      case CEED_RESTRICTION_STRIDED: {
+        kernel       = impl->StridedNoTranspose;
+        void *args[] = {&num_elem, &d_u, &d_v};
+
+        CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+      } break;
+      case CEED_RESTRICTION_STANDARD: {
+        kernel       = impl->OffsetNoTranspose;
+        void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+        CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+      } break;
+      case CEED_RESTRICTION_ORIENTED: {
+        if (use_signs) {
+          kernel       = impl->OrientedNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        } else {
+          kernel       = impl->OffsetNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        }
+      } break;
+      case CEED_RESTRICTION_CURL_ORIENTED: {
+        if (use_signs && use_orients) {
+          kernel       = impl->CurlOrientedNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        } else if (use_orients) {
+          kernel       = impl->CurlOrientedUnsignedNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        } else {
+          kernel       = impl->OffsetNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        }
+      } break;
     }
   } else {
     // E-vector -> L-vector
-    if (impl->d_ind) {
-      // -- Offsets provided
-      CeedInt block_size = 32;
-
-      if (impl->OffsetTranspose) {
-        kernel       = impl->OffsetTranspose;
-        void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
-
-        CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
-      } else {
-        kernel       = impl->OffsetTransposeDet;
-        void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
-
-        CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
-      }
-    } else {
-      // -- Strided restriction
-      kernel             = impl->StridedTranspose;
-      void   *args[]     = {&num_elem, &d_u, &d_v};
-      CeedInt block_size = 32;
-
-      CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
+    const CeedInt block_size = 32;
+    const CeedInt grid       = CeedDivUpInt(num_nodes, block_size);
+
+    switch (rstr_type) {
+      case CEED_RESTRICTION_STRIDED: {
+        kernel       = impl->StridedTranspose;
+        void *args[] = {&num_elem, &d_u, &d_v};
+
+        CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+      } break;
+      case CEED_RESTRICTION_STANDARD: {
+        if (impl->OffsetTranspose) {
+          kernel       = impl->OffsetTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        } else {
+          kernel       = impl->OffsetTransposeDet;
+          void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        }
+      } break;
+      case CEED_RESTRICTION_ORIENTED: {
+        if (use_signs) {
+          kernel       = impl->OrientedTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        } else {
+          if (impl->OffsetTranspose) {
+            kernel       = impl->OffsetTranspose;
+            void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+          } else {
+            kernel       = impl->OffsetTransposeDet;
+            void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+          }
+        }
+      } break;
+      case CEED_RESTRICTION_CURL_ORIENTED: {
+        if (use_signs && use_orients) {
+          kernel       = impl->CurlOrientedTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        } else if (use_orients) {
+          kernel       = impl->CurlOrientedUnsignedTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+        } else {
+          if (impl->OffsetTranspose) {
+            kernel       = impl->OffsetTranspose;
+            void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+          } else {
+            kernel       = impl->OffsetTransposeDet;
+            void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Cuda(ceed, kernel, grid, block_size, args));
+          }
+        }
+      } break;
     }
   }
 
@@ -100,6 +180,29 @@ static int CeedElemRestrictionApply_Cuda(CeedElemRestriction r, CeedTransposeMod
   return CEED_ERROR_SUCCESS;
 }
 
+//------------------------------------------------------------------------------
+// Apply restriction
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionApply_Cuda(CeedElemRestriction rstr, CeedTransposeMode t_mode, CeedVector u, CeedVector v, CeedRequest *request) {
+  return CeedElemRestrictionApply_Cuda_Core(rstr, t_mode, true, true, u, v, request);
+}
+
+//------------------------------------------------------------------------------
+// Apply unsigned restriction
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionApplyUnsigned_Cuda(CeedElemRestriction rstr, CeedTransposeMode t_mode, CeedVector u, CeedVector v,
+                                                 CeedRequest *request) {
+  return CeedElemRestrictionApply_Cuda_Core(rstr, t_mode, false, true, u, v, request);
+}
+
+//------------------------------------------------------------------------------
+// Apply unoriented restriction
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionApplyUnoriented_Cuda(CeedElemRestriction rstr, CeedTransposeMode t_mode, CeedVector u, CeedVector v,
+                                                   CeedRequest *request) {
+  return CeedElemRestrictionApply_Cuda_Core(rstr, t_mode, false, false, u, v, request);
+}
+
 //------------------------------------------------------------------------------
 // Get offsets
 //------------------------------------------------------------------------------
@@ -118,21 +221,61 @@ static int CeedElemRestrictionGetOffsets_Cuda(CeedElemRestriction rstr, CeedMemT
   return CEED_ERROR_SUCCESS;
 }
 
+//------------------------------------------------------------------------------
+// Get orientations
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionGetOrientations_Cuda(CeedElemRestriction rstr, CeedMemType mem_type, const bool **orients) {
+  CeedElemRestriction_Cuda *impl;
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+
+  switch (mem_type) {
+    case CEED_MEM_HOST:
+      *orients = impl->h_orients;
+      break;
+    case CEED_MEM_DEVICE:
+      *orients = impl->d_orients;
+      break;
+  }
+  return CEED_ERROR_SUCCESS;
+}
+
+//------------------------------------------------------------------------------
+// Get curl-conforming orientations
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionGetCurlOrientations_Cuda(CeedElemRestriction rstr, CeedMemType mem_type, const CeedInt8 **curl_orients) {
+  CeedElemRestriction_Cuda *impl;
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+
+  switch (mem_type) {
+    case CEED_MEM_HOST:
+      *curl_orients = impl->h_curl_orients;
+      break;
+    case CEED_MEM_DEVICE:
+      *curl_orients = impl->d_curl_orients;
+      break;
+  }
+  return CEED_ERROR_SUCCESS;
+}
+
 //------------------------------------------------------------------------------
 // Destroy restriction
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionDestroy_Cuda(CeedElemRestriction r) {
+static int CeedElemRestrictionDestroy_Cuda(CeedElemRestriction rstr) {
   Ceed                      ceed;
   CeedElemRestriction_Cuda *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
   CeedCallCuda(ceed, cuModuleUnload(impl->module));
   CeedCallBackend(CeedFree(&impl->h_ind_allocated));
   CeedCallCuda(ceed, cudaFree(impl->d_ind_allocated));
   CeedCallCuda(ceed, cudaFree(impl->d_t_offsets));
   CeedCallCuda(ceed, cudaFree(impl->d_t_indices));
   CeedCallCuda(ceed, cudaFree(impl->d_l_vec_indices));
+  CeedCallBackend(CeedFree(&impl->h_orients_allocated));
+  CeedCallCuda(ceed, cudaFree(impl->d_orients_allocated));
+  CeedCallBackend(CeedFree(&impl->h_curl_orients_allocated));
+  CeedCallCuda(ceed, cudaFree(impl->d_curl_orients_allocated));
   CeedCallBackend(CeedFree(&impl));
   return CEED_ERROR_SUCCESS;
 }
@@ -140,7 +283,7 @@ static int CeedElemRestrictionDestroy_Cuda(CeedElemRestriction r) {
 //------------------------------------------------------------------------------
 // Create transpose offsets and indices
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionOffset_Cuda(const CeedElemRestriction r, const CeedInt *indices) {
+static int CeedElemRestrictionOffset_Cuda(const CeedElemRestriction rstr, const CeedInt *indices) {
   Ceed                      ceed;
   bool                     *is_node;
   CeedSize                  l_size;
@@ -148,12 +291,12 @@ static int CeedElemRestrictionOffset_Cuda(const CeedElemRestriction r, const Cee
   CeedInt                  *ind_to_offset, *l_vec_indices, *t_offsets, *t_indices;
   CeedElemRestriction_Cuda *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
-  CeedCallBackend(CeedElemRestrictionGetNumElements(r, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
-  CeedCallBackend(CeedElemRestrictionGetLVectorSize(r, &l_size));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(r, &num_comp));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
   const CeedInt size_indices = num_elem * elem_size;
 
   // Count num_nodes
@@ -221,135 +364,194 @@ static int CeedElemRestrictionOffset_Cuda(const CeedElemRestriction r, const Cee
 // Create restriction
 //------------------------------------------------------------------------------
 int CeedElemRestrictionCreate_Cuda(CeedMemType mem_type, CeedCopyMode copy_mode, const CeedInt *indices, const bool *orients,
-                                   const CeedInt8 *curl_orients, CeedElemRestriction r) {
+                                   const CeedInt8 *curl_orients, CeedElemRestriction rstr) {
   Ceed                      ceed, ceed_parent;
-  bool                      is_deterministic, is_strided;
+  bool                      is_deterministic;
   CeedInt                   num_elem, num_comp, elem_size, comp_stride = 1;
   CeedRestrictionType       rstr_type;
+  char                     *restriction_kernel_path, *restriction_kernel_source;
   CeedElemRestriction_Cuda *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedCalloc(1, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
   CeedCallBackend(CeedGetParent(ceed, &ceed_parent));
   CeedCallBackend(CeedIsDeterministic(ceed_parent, &is_deterministic));
-  CeedCallBackend(CeedElemRestrictionGetNumElements(r, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(r, &num_comp));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
   const CeedInt size       = num_elem * elem_size;
   CeedInt       strides[3] = {1, size, elem_size};
   CeedInt       layout[3]  = {1, elem_size * num_elem, elem_size};
 
-  CeedCallBackend(CeedElemRestrictionGetType(r, &rstr_type));
-  CeedCheck(rstr_type != CEED_RESTRICTION_ORIENTED && rstr_type != CEED_RESTRICTION_CURL_ORIENTED, ceed, CEED_ERROR_BACKEND,
-            "Backend does not implement CeedElemRestrictionCreateOriented or CeedElemRestrictionCreateCurlOriented");
-
   // Stride data
-  CeedCallBackend(CeedElemRestrictionIsStrided(r, &is_strided));
-  if (is_strided) {
+  CeedCallBackend(CeedElemRestrictionGetType(rstr, &rstr_type));
+  if (rstr_type == CEED_RESTRICTION_STRIDED) {
     bool has_backend_strides;
 
-    CeedCallBackend(CeedElemRestrictionHasBackendStrides(r, &has_backend_strides));
+    CeedCallBackend(CeedElemRestrictionHasBackendStrides(rstr, &has_backend_strides));
     if (!has_backend_strides) {
-      CeedCallBackend(CeedElemRestrictionGetStrides(r, &strides));
+      CeedCallBackend(CeedElemRestrictionGetStrides(rstr, &strides));
     }
   } else {
-    CeedCallBackend(CeedElemRestrictionGetCompStride(r, &comp_stride));
+    CeedCallBackend(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
   }
 
-  impl->h_ind           = NULL;
-  impl->h_ind_allocated = NULL;
-  impl->d_ind           = NULL;
-  impl->d_ind_allocated = NULL;
-  impl->d_t_indices     = NULL;
-  impl->d_t_offsets     = NULL;
-  impl->num_nodes       = size;
-  CeedCallBackend(CeedElemRestrictionSetData(r, impl));
-  CeedCallBackend(CeedElemRestrictionSetELayout(r, layout));
-
-  // Set up device indices/offset arrays
-  switch (mem_type) {
-    case CEED_MEM_HOST: {
-      switch (copy_mode) {
-        case CEED_OWN_POINTER:
-          impl->h_ind_allocated = (CeedInt *)indices;
-          impl->h_ind           = (CeedInt *)indices;
-          break;
-        case CEED_USE_POINTER:
-          impl->h_ind = (CeedInt *)indices;
-          break;
-        case CEED_COPY_VALUES:
-          if (indices != NULL) {
-            CeedCallBackend(CeedMalloc(elem_size * num_elem, &impl->h_ind_allocated));
-            memcpy(impl->h_ind_allocated, indices, elem_size * num_elem * sizeof(CeedInt));
+  CeedCallBackend(CeedCalloc(1, &impl));
+  impl->num_nodes                = size;
+  impl->h_ind                    = NULL;
+  impl->h_ind_allocated          = NULL;
+  impl->d_ind                    = NULL;
+  impl->d_ind_allocated          = NULL;
+  impl->d_t_indices              = NULL;
+  impl->d_t_offsets              = NULL;
+  impl->h_orients                = NULL;
+  impl->h_orients_allocated      = NULL;
+  impl->d_orients                = NULL;
+  impl->d_orients_allocated      = NULL;
+  impl->h_curl_orients           = NULL;
+  impl->h_curl_orients_allocated = NULL;
+  impl->d_curl_orients           = NULL;
+  impl->d_curl_orients_allocated = NULL;
+  CeedCallBackend(CeedElemRestrictionSetData(rstr, impl));
+  CeedCallBackend(CeedElemRestrictionSetELayout(rstr, layout));
+
+  // Set up device offset/orientation arrays
+  if (rstr_type != CEED_RESTRICTION_STRIDED) {
+    switch (mem_type) {
+      case CEED_MEM_HOST: {
+        switch (copy_mode) {
+          case CEED_OWN_POINTER:
+            impl->h_ind_allocated = (CeedInt *)indices;
+            impl->h_ind           = (CeedInt *)indices;
+            break;
+          case CEED_USE_POINTER:
+            impl->h_ind = (CeedInt *)indices;
+            break;
+          case CEED_COPY_VALUES:
+            CeedCallBackend(CeedMalloc(size, &impl->h_ind_allocated));
+            memcpy(impl->h_ind_allocated, indices, size * sizeof(CeedInt));
             impl->h_ind = impl->h_ind_allocated;
-          }
-          break;
-      }
-      if (indices != NULL) {
+            break;
+        }
         CeedCallCuda(ceed, cudaMalloc((void **)&impl->d_ind, size * sizeof(CeedInt)));
         impl->d_ind_allocated = impl->d_ind;  // We own the device memory
         CeedCallCuda(ceed, cudaMemcpy(impl->d_ind, indices, size * sizeof(CeedInt), cudaMemcpyHostToDevice));
-        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Cuda(r, indices));
-      }
-      break;
-    }
-    case CEED_MEM_DEVICE: {
-      switch (copy_mode) {
-        case CEED_COPY_VALUES:
-          if (indices != NULL) {
+        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Cuda(rstr, indices));
+      } break;
+      case CEED_MEM_DEVICE: {
+        switch (copy_mode) {
+          case CEED_COPY_VALUES:
             CeedCallCuda(ceed, cudaMalloc((void **)&impl->d_ind, size * sizeof(CeedInt)));
             impl->d_ind_allocated = impl->d_ind;  // We own the device memory
             CeedCallCuda(ceed, cudaMemcpy(impl->d_ind, indices, size * sizeof(CeedInt), cudaMemcpyDeviceToDevice));
+            break;
+          case CEED_OWN_POINTER:
+            impl->d_ind           = (CeedInt *)indices;
+            impl->d_ind_allocated = impl->d_ind;
+            break;
+          case CEED_USE_POINTER:
+            impl->d_ind = (CeedInt *)indices;
+            break;
+        }
+        CeedCallBackend(CeedMalloc(size, &impl->h_ind_allocated));
+        CeedCallCuda(ceed, cudaMemcpy(impl->h_ind_allocated, impl->d_ind, size * sizeof(CeedInt), cudaMemcpyDeviceToHost));
+        impl->h_ind = impl->h_ind_allocated;
+        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Cuda(rstr, indices));
+      } break;
+    }
+
+    // Orientation data
+    if (rstr_type == CEED_RESTRICTION_ORIENTED) {
+      switch (mem_type) {
+        case CEED_MEM_HOST: {
+          switch (copy_mode) {
+            case CEED_OWN_POINTER:
+              impl->h_orients_allocated = (bool *)orients;
+              impl->h_orients           = (bool *)orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->h_orients = (bool *)orients;
+              break;
+            case CEED_COPY_VALUES:
+              CeedCallBackend(CeedMalloc(size, &impl->h_orients_allocated));
+              memcpy(impl->h_orients_allocated, orients, size * sizeof(bool));
+              impl->h_orients = impl->h_orients_allocated;
+              break;
           }
-          break;
-        case CEED_OWN_POINTER:
-          impl->d_ind           = (CeedInt *)indices;
-          impl->d_ind_allocated = impl->d_ind;
-          break;
-        case CEED_USE_POINTER:
-          impl->d_ind = (CeedInt *)indices;
+          CeedCallCuda(ceed, cudaMalloc((void **)&impl->d_orients, size * sizeof(bool)));
+          impl->d_orients_allocated = impl->d_orients;  // We own the device memory
+          CeedCallCuda(ceed, cudaMemcpy(impl->d_orients, orients, size * sizeof(bool), cudaMemcpyHostToDevice));
+        } break;
+        case CEED_MEM_DEVICE: {
+          switch (copy_mode) {
+            case CEED_COPY_VALUES:
+              CeedCallCuda(ceed, cudaMalloc((void **)&impl->d_orients, size * sizeof(bool)));
+              impl->d_orients_allocated = impl->d_orients;  // We own the device memory
+              CeedCallCuda(ceed, cudaMemcpy(impl->d_orients, orients, size * sizeof(bool), cudaMemcpyDeviceToDevice));
+              break;
+            case CEED_OWN_POINTER:
+              impl->d_orients           = (bool *)orients;
+              impl->d_orients_allocated = impl->d_orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->d_orients = (bool *)orients;
+              break;
+          }
+          CeedCallBackend(CeedMalloc(size, &impl->h_orients_allocated));
+          CeedCallCuda(ceed, cudaMemcpy(impl->h_orients_allocated, impl->d_orients, size * sizeof(bool), cudaMemcpyDeviceToHost));
+          impl->h_orients = impl->h_orients_allocated;
+        } break;
       }
-      if (indices != NULL) {
-        CeedCallBackend(CeedMalloc(elem_size * num_elem, &impl->h_ind_allocated));
-        CeedCallCuda(ceed, cudaMemcpy(impl->h_ind_allocated, impl->d_ind, elem_size * num_elem * sizeof(CeedInt), cudaMemcpyDeviceToHost));
-        impl->h_ind = impl->h_ind_allocated;
-        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Cuda(r, indices));
+    } else if (rstr_type == CEED_RESTRICTION_CURL_ORIENTED) {
+      switch (mem_type) {
+        case CEED_MEM_HOST: {
+          switch (copy_mode) {
+            case CEED_OWN_POINTER:
+              impl->h_curl_orients_allocated = (CeedInt8 *)curl_orients;
+              impl->h_curl_orients           = (CeedInt8 *)curl_orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->h_curl_orients = (CeedInt8 *)curl_orients;
+              break;
+            case CEED_COPY_VALUES:
+              CeedCallBackend(CeedMalloc(3 * size, &impl->h_curl_orients_allocated));
+              memcpy(impl->h_curl_orients_allocated, curl_orients, 3 * size * sizeof(CeedInt8));
+              impl->h_curl_orients = impl->h_curl_orients_allocated;
+              break;
+          }
+          CeedCallCuda(ceed, cudaMalloc((void **)&impl->d_curl_orients, 3 * size * sizeof(CeedInt8)));
+          impl->d_curl_orients_allocated = impl->d_curl_orients;  // We own the device memory
+          CeedCallCuda(ceed, cudaMemcpy(impl->d_curl_orients, curl_orients, 3 * size * sizeof(CeedInt8), cudaMemcpyHostToDevice));
+        } break;
+        case CEED_MEM_DEVICE: {
+          switch (copy_mode) {
+            case CEED_COPY_VALUES:
+              CeedCallCuda(ceed, cudaMalloc((void **)&impl->d_curl_orients, 3 * size * sizeof(CeedInt8)));
+              impl->d_curl_orients_allocated = impl->d_curl_orients;  // We own the device memory
+              CeedCallCuda(ceed, cudaMemcpy(impl->d_curl_orients, curl_orients, 3 * size * sizeof(CeedInt8), cudaMemcpyDeviceToDevice));
+              break;
+            case CEED_OWN_POINTER:
+              impl->d_curl_orients           = (CeedInt8 *)curl_orients;
+              impl->d_curl_orients_allocated = impl->d_curl_orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->d_curl_orients = (CeedInt8 *)curl_orients;
+              break;
+          }
+          CeedCallBackend(CeedMalloc(3 * size, &impl->h_curl_orients_allocated));
+          CeedCallCuda(ceed, cudaMemcpy(impl->h_curl_orients_allocated, impl->d_curl_orients, 3 * size * sizeof(CeedInt8), cudaMemcpyDeviceToHost));
+          impl->h_curl_orients = impl->h_curl_orients_allocated;
+        } break;
       }
-      break;
     }
-    // LCOV_EXCL_START
-    default:
-      return CeedError(ceed, CEED_ERROR_BACKEND, "Only MemType = HOST or DEVICE supported");
-      // LCOV_EXCL_STOP
   }
 
-  // Compile CUDA kernels (add atomicAdd function for old NVidia architectures)
-  CeedInt num_nodes = impl->num_nodes;
-  char   *restriction_kernel_path, *restriction_kernel_source = NULL;
-
+  // Compile CUDA kernels
   CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/cuda/cuda-ref-restriction.h", &restriction_kernel_path));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Restriction Kernel Source -----\n");
-  if (!is_deterministic) {
-    struct cudaDeviceProp prop;
-    Ceed_Cuda            *ceed_data;
-
-    CeedCallBackend(CeedGetData(ceed, &ceed_data));
-    CeedCallBackend(cudaGetDeviceProperties(&prop, ceed_data->device_id));
-    if ((prop.major < 6) && (CEED_SCALAR_TYPE != CEED_SCALAR_FP32)) {
-      char *atomic_add_path;
-
-      CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/cuda/cuda-atomic-add-fallback.h", &atomic_add_path));
-      CeedCallBackend(CeedLoadSourceToBuffer(ceed, atomic_add_path, &restriction_kernel_source));
-      CeedCallBackend(CeedLoadSourceToInitializedBuffer(ceed, restriction_kernel_path, &restriction_kernel_source));
-      CeedCallBackend(CeedFree(&atomic_add_path));
-    }
-  }
-  if (!restriction_kernel_source) {
-    CeedCallBackend(CeedLoadSourceToBuffer(ceed, restriction_kernel_path, &restriction_kernel_source));
-  }
+  CeedCallBackend(CeedLoadSourceToBuffer(ceed, restriction_kernel_path, &restriction_kernel_source));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Restriction Kernel Source Complete! -----\n");
   CeedCallBackend(CeedCompile_Cuda(ceed, restriction_kernel_source, &impl->module, 8, "RSTR_ELEM_SIZE", elem_size, "RSTR_NUM_ELEM", num_elem,
-                                   "RSTR_NUM_COMP", num_comp, "RSTR_NUM_NODES", num_nodes, "RSTR_COMP_STRIDE", comp_stride, "RSTR_STRIDE_NODES",
+                                   "RSTR_NUM_COMP", num_comp, "RSTR_NUM_NODES", impl->num_nodes, "RSTR_COMP_STRIDE", comp_stride, "RSTR_STRIDE_NODES",
                                    strides[0], "RSTR_STRIDE_COMP", strides[1], "RSTR_STRIDE_ELEM", strides[2]));
   CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "StridedNoTranspose", &impl->StridedNoTranspose));
   CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "StridedTranspose", &impl->StridedTranspose));
@@ -359,15 +561,23 @@ int CeedElemRestrictionCreate_Cuda(CeedMemType mem_type, CeedCopyMode copy_mode,
   } else {
     CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "OffsetTransposeDet", &impl->OffsetTransposeDet));
   }
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "OrientedNoTranspose", &impl->OrientedNoTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "OrientedTranspose", &impl->OrientedTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "CurlOrientedNoTranspose", &impl->CurlOrientedNoTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "CurlOrientedUnsignedNoTranspose", &impl->CurlOrientedUnsignedNoTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "CurlOrientedTranspose", &impl->CurlOrientedTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, impl->module, "CurlOrientedUnsignedTranspose", &impl->CurlOrientedUnsignedTranspose));
   CeedCallBackend(CeedFree(&restriction_kernel_path));
   CeedCallBackend(CeedFree(&restriction_kernel_source));
 
   // Register backend functions
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "Apply", CeedElemRestrictionApply_Cuda));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "ApplyUnsigned", CeedElemRestrictionApply_Cuda));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "ApplyUnoriented", CeedElemRestrictionApply_Cuda));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "GetOffsets", CeedElemRestrictionGetOffsets_Cuda));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "Destroy", CeedElemRestrictionDestroy_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Apply", CeedElemRestrictionApply_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnsigned", CeedElemRestrictionApplyUnsigned_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnoriented", CeedElemRestrictionApplyUnoriented_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOffsets", CeedElemRestrictionGetOffsets_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOrientations", CeedElemRestrictionGetOrientations_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetCurlOrientations", CeedElemRestrictionGetCurlOrientations_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Destroy", CeedElemRestrictionDestroy_Cuda));
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/backends/cuda-ref/ceed-cuda-ref-vector.c b/backends/cuda-ref/ceed-cuda-ref-vector.c
index 284bf476..7bd62f16 100644
--- a/backends/cuda-ref/ceed-cuda-ref-vector.c
+++ b/backends/cuda-ref/ceed-cuda-ref-vector.c
@@ -41,6 +41,7 @@ static inline int CeedVectorNeedSync_Cuda(const CeedVector vec, CeedMemType mem_
 static inline int CeedVectorSyncH2D_Cuda(const CeedVector vec) {
   Ceed             ceed;
   CeedSize         length;
+  size_t           bytes;
   CeedVector_Cuda *impl;
 
   CeedCallBackend(CeedVectorGetCeed(vec, &ceed));
@@ -49,8 +50,7 @@ static inline int CeedVectorSyncH2D_Cuda(const CeedVector vec) {
   CeedCheck(impl->h_array, ceed, CEED_ERROR_BACKEND, "No valid host data to sync to device");
 
   CeedCallBackend(CeedVectorGetLength(vec, &length));
-  size_t bytes = length * sizeof(CeedScalar);
-
+  bytes = length * sizeof(CeedScalar);
   if (impl->d_array_borrowed) {
     impl->d_array = impl->d_array_borrowed;
   } else if (impl->d_array_owned) {
@@ -192,9 +192,10 @@ static int CeedVectorSetArrayHost_Cuda(const CeedVector vec, const CeedCopyMode
       impl->h_array          = impl->h_array_owned;
       if (array) {
         CeedSize length;
+        size_t   bytes;
 
         CeedCallBackend(CeedVectorGetLength(vec, &length));
-        size_t bytes = length * sizeof(CeedScalar);
+        bytes = length * sizeof(CeedScalar);
         memcpy(impl->h_array, array, bytes);
       }
     } break;
@@ -225,10 +226,10 @@ static int CeedVectorSetArrayDevice_Cuda(const CeedVector vec, const CeedCopyMod
   switch (copy_mode) {
     case CEED_COPY_VALUES: {
       CeedSize length;
+      size_t   bytes;
 
       CeedCallBackend(CeedVectorGetLength(vec, &length));
-      size_t bytes = length * sizeof(CeedScalar);
-
+      bytes = length * sizeof(CeedScalar);
       if (!impl->d_array_owned) {
         CeedCallCuda(ceed, cudaMalloc((void **)&impl->d_array_owned, bytes));
       }
@@ -430,11 +431,14 @@ static int CeedVectorGetArrayWrite_Cuda(const CeedVector vec, const CeedMemType
 // Get the norm of a CeedVector
 //------------------------------------------------------------------------------
 static int CeedVectorNorm_Cuda(CeedVector vec, CeedNormType type, CeedScalar *norm) {
-  Ceed              ceed;
-  cublasHandle_t    handle;
-  CeedSize          length;
+  Ceed     ceed;
+  CeedSize length;
+#if CUDA_VERSION < 12000
+  CeedSize num_calls;
+#endif
   const CeedScalar *d_array;
   CeedVector_Cuda  *impl;
+  cublasHandle_t    handle;
 
   CeedCallBackend(CeedVectorGetCeed(vec, &ceed));
   CeedCallBackend(CeedVectorGetData(vec, &impl));
@@ -445,8 +449,7 @@ static int CeedVectorNorm_Cuda(CeedVector vec, CeedNormType type, CeedScalar *no
   // With CUDA 12, we can use the 64-bit integer interface. Prior to that,
   // we need to check if the vector is too long to handle with int32,
   // and if so, divide it into subsections for repeated cuBLAS calls.
-  CeedSize num_calls = length / INT_MAX;
-
+  num_calls = length / INT_MAX;
   if (length % INT_MAX > 0) num_calls += 1;
 #endif
 
diff --git a/backends/cuda-ref/ceed-cuda-ref.h b/backends/cuda-ref/ceed-cuda-ref.h
index 26ab4ba5..c60ca79b 100644
--- a/backends/cuda-ref/ceed-cuda-ref.h
+++ b/backends/cuda-ref/ceed-cuda-ref.h
@@ -30,6 +30,12 @@ typedef struct {
   CUfunction OffsetNoTranspose;
   CUfunction OffsetTranspose;
   CUfunction OffsetTransposeDet;
+  CUfunction OrientedNoTranspose;
+  CUfunction OrientedTranspose;
+  CUfunction CurlOrientedNoTranspose;
+  CUfunction CurlOrientedTranspose;
+  CUfunction CurlOrientedUnsignedNoTranspose;
+  CUfunction CurlOrientedUnsignedTranspose;
   CeedInt    num_nodes;
   CeedInt   *h_ind;
   CeedInt   *h_ind_allocated;
@@ -38,6 +44,14 @@ typedef struct {
   CeedInt   *d_t_offsets;
   CeedInt   *d_t_indices;
   CeedInt   *d_l_vec_indices;
+  bool      *h_orients;
+  bool      *h_orients_allocated;
+  bool      *d_orients;
+  bool      *d_orients_allocated;
+  CeedInt8  *h_curl_orients;
+  CeedInt8  *h_curl_orients_allocated;
+  CeedInt8  *d_curl_orients;
+  CeedInt8  *d_curl_orients_allocated;
 } CeedElemRestriction_Cuda;
 
 typedef struct {
@@ -80,21 +94,19 @@ typedef struct {
 
 typedef struct {
   CUmodule            module;
-  CUfunction          linearDiagonal;
-  CUfunction          linearPointBlock;
-  CeedBasis           basis_in, basis_out;
-  CeedElemRestriction diag_rstr, point_block_rstr;
+  CUfunction          LinearDiagonal;
+  CUfunction          LinearPointBlock;
+  CeedElemRestriction diag_rstr, point_block_diag_rstr;
   CeedVector          elem_diag, point_block_elem_diag;
-  CeedInt             num_e_mode_in, num_e_mode_out, num_nodes;
-  CeedEvalMode       *h_e_mode_in, *h_e_mode_out;
-  CeedEvalMode       *d_e_mode_in, *d_e_mode_out;
-  CeedScalar         *d_identity, *d_interp_in, *d_interp_out, *d_grad_in, *d_grad_out;
+  CeedEvalMode       *d_eval_modes_in, *d_eval_modes_out;
+  CeedScalar         *d_identity, *d_interp_in, *d_grad_in, *d_div_in, *d_curl_in;
+  CeedScalar         *d_interp_out, *d_grad_out, *d_div_out, *d_curl_out;
 } CeedOperatorDiag_Cuda;
 
 typedef struct {
   CUmodule    module;
-  CUfunction  linearAssemble;
-  CeedInt     num_elem, block_size_x, block_size_y, elem_per_block;
+  CUfunction  LinearAssemble;
+  CeedInt     block_size_x, block_size_y, elems_per_block;
   CeedScalar *d_B_in, *d_B_out;
 } CeedOperatorAssemble_Cuda;
 
diff --git a/backends/cuda-shared/ceed-cuda-shared-basis.c b/backends/cuda-shared/ceed-cuda-shared-basis.c
index 305ea669..4dc8594d 100644
--- a/backends/cuda-shared/ceed-cuda-shared-basis.c
+++ b/backends/cuda-shared/ceed-cuda-shared-basis.c
@@ -146,22 +146,23 @@ int CeedBasisApplyTensor_Cuda_shared(CeedBasis basis, const CeedInt num_elem, Ce
     } break;
     case CEED_EVAL_WEIGHT: {
       CeedInt Q_1d;
+      CeedInt block_size = 32;
 
       CeedCallBackend(CeedBasisGetNumQuadraturePoints1D(basis, &Q_1d));
       void *weight_args[] = {(void *)&num_elem, (void *)&data->d_q_weight_1d, &d_v};
       if (dim == 1) {
-        const CeedInt elems_per_block = 32 / Q_1d;
+        const CeedInt elems_per_block = block_size / Q_1d;
         const CeedInt grid_size       = num_elem / elems_per_block + ((num_elem / elems_per_block * elems_per_block < num_elem) ? 1 : 0);
 
         CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Weight, grid_size, Q_1d, elems_per_block, 1, weight_args));
       } else if (dim == 2) {
-        const CeedInt opt_elems       = 32 / (Q_1d * Q_1d);
+        const CeedInt opt_elems       = block_size / (Q_1d * Q_1d);
         const CeedInt elems_per_block = opt_elems > 0 ? opt_elems : 1;
         const CeedInt grid_size       = num_elem / elems_per_block + ((num_elem / elems_per_block * elems_per_block < num_elem) ? 1 : 0);
 
         CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Weight, grid_size, Q_1d, Q_1d, elems_per_block, weight_args));
       } else if (dim == 3) {
-        const CeedInt opt_elems       = 32 / (Q_1d * Q_1d);
+        const CeedInt opt_elems       = block_size / (Q_1d * Q_1d);
         const CeedInt elems_per_block = opt_elems > 0 ? opt_elems : 1;
         const CeedInt grid_size       = num_elem / elems_per_block + ((num_elem / elems_per_block * elems_per_block < num_elem) ? 1 : 0);
 
diff --git a/backends/hip-ref/ceed-hip-ref-operator.c b/backends/hip-ref/ceed-hip-ref-operator.c
index 90dfd691..36cdd82f 100644
--- a/backends/hip-ref/ceed-hip-ref-operator.c
+++ b/backends/hip-ref/ceed-hip-ref-operator.c
@@ -53,15 +53,18 @@ static int CeedOperatorDestroy_Hip(CeedOperator op) {
 
     CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
     CeedCallHip(ceed, hipModuleUnload(impl->diag->module));
-    CeedCallBackend(CeedFree(&impl->diag->h_e_mode_in));
-    CeedCallBackend(CeedFree(&impl->diag->h_e_mode_out));
-    CeedCallHip(ceed, hipFree(impl->diag->d_e_mode_in));
-    CeedCallHip(ceed, hipFree(impl->diag->d_e_mode_out));
+    CeedCallHip(ceed, hipFree(impl->diag->d_eval_modes_in));
+    CeedCallHip(ceed, hipFree(impl->diag->d_eval_modes_out));
     CeedCallHip(ceed, hipFree(impl->diag->d_identity));
     CeedCallHip(ceed, hipFree(impl->diag->d_interp_in));
     CeedCallHip(ceed, hipFree(impl->diag->d_interp_out));
     CeedCallHip(ceed, hipFree(impl->diag->d_grad_in));
     CeedCallHip(ceed, hipFree(impl->diag->d_grad_out));
+    CeedCallHip(ceed, hipFree(impl->diag->d_div_in));
+    CeedCallHip(ceed, hipFree(impl->diag->d_div_out));
+    CeedCallHip(ceed, hipFree(impl->diag->d_curl_in));
+    CeedCallHip(ceed, hipFree(impl->diag->d_curl_out));
+    CeedCallBackend(CeedElemRestrictionDestroy(&impl->diag->diag_rstr));
     CeedCallBackend(CeedElemRestrictionDestroy(&impl->diag->point_block_diag_rstr));
     CeedCallBackend(CeedVectorDestroy(&impl->diag->elem_diag));
     CeedCallBackend(CeedVectorDestroy(&impl->diag->point_block_elem_diag));
@@ -102,30 +105,29 @@ static int CeedOperatorSetupFields_Hip(CeedQFunction qf, CeedOperator op, bool i
 
   // Loop over fields
   for (CeedInt i = 0; i < num_fields; i++) {
-    bool                is_strided, skip_restriction;
-    CeedSize            q_size;
-    CeedInt             dim, size;
-    CeedEvalMode        e_mode;
-    CeedVector          vec;
-    CeedElemRestriction elem_rstr;
-    CeedBasis           basis;
+    bool         is_strided = false, skip_restriction = false;
+    CeedSize     q_size;
+    CeedInt      size;
+    CeedEvalMode eval_mode;
+    CeedBasis    basis;
 
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &e_mode));
-    is_strided       = false;
-    skip_restriction = false;
-    if (e_mode != CEED_EVAL_WEIGHT) {
-      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &elem_rstr));
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
+    if (eval_mode != CEED_EVAL_WEIGHT) {
+      CeedElemRestriction elem_rstr;
 
       // Check whether this field can skip the element restriction:
-      // must be passive input, with e_mode NONE, and have a strided restriction with CEED_STRIDES_BACKEND.
+      // Must be passive input, with eval_mode NONE, and have a strided restriction with CEED_STRIDES_BACKEND.
+      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &elem_rstr));
 
       // First, check whether the field is input or output:
       if (is_input) {
-        // Check for passive input:
+        CeedVector vec;
+
+        // Check for passive input
         CeedCallBackend(CeedOperatorFieldGetVector(op_fields[i], &vec));
         if (vec != CEED_VECTOR_ACTIVE) {
-          // Check e_mode
-          if (e_mode == CEED_EVAL_NONE) {
+          // Check eval_mode
+          if (eval_mode == CEED_EVAL_NONE) {
             // Check for strided restriction
             CeedCallBackend(CeedElemRestrictionIsStrided(elem_rstr, &is_strided));
             if (is_strided) {
@@ -143,21 +145,17 @@ static int CeedOperatorSetupFields_Hip(CeedQFunction qf, CeedOperator op, bool i
       }
     }
 
-    switch (e_mode) {
+    switch (eval_mode) {
       case CEED_EVAL_NONE:
         CeedCallBackend(CeedQFunctionFieldGetSize(qf_fields[i], &size));
         q_size = (CeedSize)num_elem * Q * size;
         CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
         break;
       case CEED_EVAL_INTERP:
-        CeedCallBackend(CeedQFunctionFieldGetSize(qf_fields[i], &size));
-        q_size = (CeedSize)num_elem * Q * size;
-        CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
-        break;
       case CEED_EVAL_GRAD:
-        CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis));
+      case CEED_EVAL_DIV:
+      case CEED_EVAL_CURL:
         CeedCallBackend(CeedQFunctionFieldGetSize(qf_fields[i], &size));
-        CeedCallBackend(CeedBasisGetDimension(basis, &dim));
         q_size = (CeedSize)num_elem * Q * size;
         CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
         break;
@@ -167,10 +165,6 @@ static int CeedOperatorSetupFields_Hip(CeedQFunction qf, CeedOperator op, bool i
         CeedCallBackend(CeedVectorCreate(ceed, q_size, &q_vecs[i]));
         CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, CEED_EVAL_WEIGHT, CEED_VECTOR_NONE, q_vecs[i]));
         break;
-      case CEED_EVAL_DIV:
-        break;  // TODO: Not implemented
-      case CEED_EVAL_CURL:
-        break;  // TODO: Not implemented
     }
   }
   return CEED_ERROR_SUCCESS;
@@ -201,17 +195,14 @@ static int CeedOperatorSetup_Hip(CeedOperator op) {
 
   // Allocate
   CeedCallBackend(CeedCalloc(num_input_fields + num_output_fields, &impl->e_vecs));
-
   CeedCallBackend(CeedCalloc(CEED_FIELD_MAX, &impl->q_vecs_in));
   CeedCallBackend(CeedCalloc(CEED_FIELD_MAX, &impl->q_vecs_out));
-
   impl->num_inputs  = num_input_fields;
   impl->num_outputs = num_output_fields;
 
   // Set up infield and outfield e_vecs and q_vecs
   // Infields
   CeedCallBackend(CeedOperatorSetupFields_Hip(qf, op, true, impl->e_vecs, impl->q_vecs_in, 0, num_input_fields, Q, num_elem));
-
   // Outfields
   CeedCallBackend(CeedOperatorSetupFields_Hip(qf, op, false, impl->e_vecs, impl->q_vecs_out, num_input_fields, num_output_fields, Q, num_elem));
 
@@ -226,7 +217,7 @@ static inline int CeedOperatorSetupInputs_Hip(CeedInt num_input_fields, CeedQFun
                                               CeedVector in_vec, const bool skip_active, CeedScalar *e_data[2 * CEED_FIELD_MAX],
                                               CeedOperator_Hip *impl, CeedRequest *request) {
   for (CeedInt i = 0; i < num_input_fields; i++) {
-    CeedEvalMode        e_mode;
+    CeedEvalMode        eval_mode;
     CeedVector          vec;
     CeedElemRestriction elem_rstr;
 
@@ -237,8 +228,8 @@ static inline int CeedOperatorSetupInputs_Hip(CeedInt num_input_fields, CeedQFun
       else vec = in_vec;
     }
 
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_WEIGHT) {  // Skip
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_WEIGHT) {  // Skip
     } else {
       // Get input vector
       CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
@@ -267,7 +258,7 @@ static inline int CeedOperatorInputBasis_Hip(CeedInt num_elem, CeedQFunctionFiel
                                              CeedOperator_Hip *impl) {
   for (CeedInt i = 0; i < num_input_fields; i++) {
     CeedInt             elem_size, size;
-    CeedEvalMode        e_mode;
+    CeedEvalMode        eval_mode;
     CeedElemRestriction elem_rstr;
     CeedBasis           basis;
 
@@ -278,30 +269,25 @@ static inline int CeedOperatorInputBasis_Hip(CeedInt num_elem, CeedQFunctionFiel
       CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
       if (vec == CEED_VECTOR_ACTIVE) continue;
     }
-    // Get elem_size, e_mode, size
+    // Get elem_size, eval_mode, size
     CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_input_fields[i], &elem_rstr));
     CeedCallBackend(CeedElemRestrictionGetElementSize(elem_rstr, &elem_size));
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &e_mode));
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &eval_mode));
     CeedCallBackend(CeedQFunctionFieldGetSize(qf_input_fields[i], &size));
     // Basis action
-    switch (e_mode) {
+    switch (eval_mode) {
       case CEED_EVAL_NONE:
         CeedCallBackend(CeedVectorSetArray(impl->q_vecs_in[i], CEED_MEM_DEVICE, CEED_USE_POINTER, e_data[i]));
         break;
       case CEED_EVAL_INTERP:
-        CeedCallBackend(CeedOperatorFieldGetBasis(op_input_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, CEED_EVAL_INTERP, impl->e_vecs[i], impl->q_vecs_in[i]));
-        break;
       case CEED_EVAL_GRAD:
+      case CEED_EVAL_DIV:
+      case CEED_EVAL_CURL:
         CeedCallBackend(CeedOperatorFieldGetBasis(op_input_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, CEED_EVAL_GRAD, impl->e_vecs[i], impl->q_vecs_in[i]));
+        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_NOTRANSPOSE, eval_mode, impl->e_vecs[i], impl->q_vecs_in[i]));
         break;
       case CEED_EVAL_WEIGHT:
         break;  // No action
-      case CEED_EVAL_DIV:
-        break;  // TODO: Not implemented
-      case CEED_EVAL_CURL:
-        break;  // TODO: Not implemented
     }
   }
   return CEED_ERROR_SUCCESS;
@@ -313,15 +299,16 @@ static inline int CeedOperatorInputBasis_Hip(CeedInt num_elem, CeedQFunctionFiel
 static inline int CeedOperatorRestoreInputs_Hip(CeedInt num_input_fields, CeedQFunctionField *qf_input_fields, CeedOperatorField *op_input_fields,
                                                 const bool skip_active, CeedScalar *e_data[2 * CEED_FIELD_MAX], CeedOperator_Hip *impl) {
   for (CeedInt i = 0; i < num_input_fields; i++) {
-    CeedEvalMode e_mode;
+    CeedEvalMode eval_mode;
     CeedVector   vec;
+
     // Skip active input
     if (skip_active) {
       CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
       if (vec == CEED_VECTOR_ACTIVE) continue;
     }
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_WEIGHT) {  // Skip
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_input_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_WEIGHT) {  // Skip
     } else {
       if (!impl->e_vecs[i]) {  // This was a skip_restriction case
         CeedCallBackend(CeedOperatorFieldGetVector(op_input_fields[i], &vec));
@@ -363,10 +350,10 @@ static int CeedOperatorApplyAdd_Hip(CeedOperator op, CeedVector in_vec, CeedVect
 
   // Output pointers, as necessary
   for (CeedInt i = 0; i < num_output_fields; i++) {
-    CeedEvalMode e_mode;
+    CeedEvalMode eval_mode;
 
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_NONE) {
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_NONE) {
       // Set the output Q-Vector to use the E-Vector data directly.
       CeedCallBackend(CeedVectorGetArrayWrite(impl->e_vecs[i + impl->num_inputs], CEED_MEM_DEVICE, &e_data[i + num_input_fields]));
       CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[i], CEED_MEM_DEVICE, CEED_USE_POINTER, e_data[i + num_input_fields]));
@@ -378,26 +365,25 @@ static int CeedOperatorApplyAdd_Hip(CeedOperator op, CeedVector in_vec, CeedVect
 
   // Output basis apply if needed
   for (CeedInt i = 0; i < num_output_fields; i++) {
-    CeedEvalMode        e_mode;
+    CeedEvalMode        eval_mode;
     CeedElemRestriction elem_rstr;
     CeedBasis           basis;
 
-    // Get elem_size, e_mode, size
+    // Get elem_size, eval_mode, size
     CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_output_fields[i], &elem_rstr));
     CeedCallBackend(CeedElemRestrictionGetElementSize(elem_rstr, &elem_size));
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &e_mode));
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &eval_mode));
     CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[i], &size));
     // Basis action
-    switch (e_mode) {
+    switch (eval_mode) {
       case CEED_EVAL_NONE:
-        break;
+        break;  // No action
       case CEED_EVAL_INTERP:
-        CeedCallBackend(CeedOperatorFieldGetBasis(op_output_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_TRANSPOSE, CEED_EVAL_INTERP, impl->q_vecs_out[i], impl->e_vecs[i + impl->num_inputs]));
-        break;
       case CEED_EVAL_GRAD:
+      case CEED_EVAL_DIV:
+      case CEED_EVAL_CURL:
         CeedCallBackend(CeedOperatorFieldGetBasis(op_output_fields[i], &basis));
-        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_TRANSPOSE, CEED_EVAL_GRAD, impl->q_vecs_out[i], impl->e_vecs[i + impl->num_inputs]));
+        CeedCallBackend(CeedBasisApply(basis, num_elem, CEED_TRANSPOSE, eval_mode, impl->q_vecs_out[i], impl->e_vecs[i + impl->num_inputs]));
         break;
       // LCOV_EXCL_START
       case CEED_EVAL_WEIGHT: {
@@ -405,25 +391,20 @@ static int CeedOperatorApplyAdd_Hip(CeedOperator op, CeedVector in_vec, CeedVect
 
         CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
         return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_WEIGHT cannot be an output evaluation mode");
-        break;  // Should not occur
+        // LCOV_EXCL_STOP
       }
-      case CEED_EVAL_DIV:
-        break;  // TODO: Not implemented
-      case CEED_EVAL_CURL:
-        break;  // TODO: Not implemented
-                // LCOV_EXCL_STOP
     }
   }
 
   // Output restriction
   for (CeedInt i = 0; i < num_output_fields; i++) {
-    CeedEvalMode        e_mode;
+    CeedEvalMode        eval_mode;
     CeedVector          vec;
     CeedElemRestriction elem_rstr;
 
     // Restore evec
-    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &e_mode));
-    if (e_mode == CEED_EVAL_NONE) {
+    CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_output_fields[i], &eval_mode));
+    if (eval_mode == CEED_EVAL_NONE) {
       CeedCallBackend(CeedVectorRestoreArray(impl->e_vecs[i + impl->num_inputs], &e_data[i + num_input_fields]));
     }
     // Get output vector
@@ -442,16 +423,14 @@ static int CeedOperatorApplyAdd_Hip(CeedOperator op, CeedVector in_vec, CeedVect
 }
 
 //------------------------------------------------------------------------------
-// Core code for assembling linear QFunction
+// Linear QFunction Assembly Core
 //------------------------------------------------------------------------------
 static inline int CeedOperatorLinearAssembleQFunctionCore_Hip(CeedOperator op, bool build_objects, CeedVector *assembled, CeedElemRestriction *rstr,
                                                               CeedRequest *request) {
   Ceed                ceed, ceed_parent;
-  bool                is_identity_qf;
-  CeedSize            q_size;
   CeedInt             num_active_in, num_active_out, Q, num_elem, num_input_fields, num_output_fields, size;
   CeedScalar         *assembled_array, *e_data[2 * CEED_FIELD_MAX] = {NULL};
-  CeedVector         *active_in;
+  CeedVector         *active_inputs;
   CeedQFunctionField *qf_input_fields, *qf_output_fields;
   CeedQFunction       qf;
   CeedOperatorField  *op_input_fields, *op_output_fields;
@@ -460,22 +439,17 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Hip(CeedOperator op, b
   CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
   CeedCallBackend(CeedOperatorGetFallbackParentCeed(op, &ceed_parent));
   CeedCallBackend(CeedOperatorGetData(op, &impl));
-  CeedCallBackend(CeedOperatorGetQFunction(op, &qf));
   CeedCallBackend(CeedOperatorGetNumQuadraturePoints(op, &Q));
   CeedCallBackend(CeedOperatorGetNumElements(op, &num_elem));
+  CeedCallBackend(CeedOperatorGetQFunction(op, &qf));
   CeedCallBackend(CeedQFunctionGetFields(qf, NULL, &qf_input_fields, NULL, &qf_output_fields));
   CeedCallBackend(CeedOperatorGetFields(op, &num_input_fields, &op_input_fields, &num_output_fields, &op_output_fields));
-  active_in      = impl->qf_active_in;
-  num_active_in  = impl->num_active_in;
-  num_active_out = impl->num_active_out;
+  active_inputs = impl->qf_active_in;
+  num_active_in = impl->num_active_in, num_active_out = impl->num_active_out;
 
   // Setup
   CeedCallBackend(CeedOperatorSetup_Hip(op));
 
-  // Check for identity
-  CeedCallBackend(CeedQFunctionIsIdentity(qf, &is_identity_qf));
-  CeedCheck(!is_identity_qf, ceed, CEED_ERROR_BACKEND, "Assembling identity QFunctions not supported");
-
   // Input Evecs and Restriction
   CeedCallBackend(CeedOperatorSetupInputs_Hip(num_input_fields, qf_input_fields, op_input_fields, NULL, true, e_data, impl, request));
 
@@ -492,19 +466,20 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Hip(CeedOperator op, b
         CeedCallBackend(CeedQFunctionFieldGetSize(qf_input_fields[i], &size));
         CeedCallBackend(CeedVectorSetValue(impl->q_vecs_in[i], 0.0));
         CeedCallBackend(CeedVectorGetArray(impl->q_vecs_in[i], CEED_MEM_DEVICE, &q_vec_array));
-        CeedCallBackend(CeedRealloc(num_active_in + size, &active_in));
+        CeedCallBackend(CeedRealloc(num_active_in + size, &active_inputs));
         for (CeedInt field = 0; field < size; field++) {
-          q_size = (CeedSize)Q * num_elem;
-          CeedCallBackend(CeedVectorCreate(ceed, q_size, &active_in[num_active_in + field]));
+          CeedSize q_size = (CeedSize)Q * num_elem;
+
+          CeedCallBackend(CeedVectorCreate(ceed, q_size, &active_inputs[num_active_in + field]));
           CeedCallBackend(
-              CeedVectorSetArray(active_in[num_active_in + field], CEED_MEM_DEVICE, CEED_USE_POINTER, &q_vec_array[field * Q * num_elem]));
+              CeedVectorSetArray(active_inputs[num_active_in + field], CEED_MEM_DEVICE, CEED_USE_POINTER, &q_vec_array[field * Q * num_elem]));
         }
         num_active_in += size;
         CeedCallBackend(CeedVectorRestoreArray(impl->q_vecs_in[i], &q_vec_array));
       }
     }
     impl->num_active_in = num_active_in;
-    impl->qf_active_in  = active_in;
+    impl->qf_active_in  = active_inputs;
   }
 
   // Count number of active output fields
@@ -528,10 +503,10 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Hip(CeedOperator op, b
 
   // Build objects if needed
   if (build_objects) {
-    // Create output restriction
     CeedSize l_size     = (CeedSize)num_elem * Q * num_active_in * num_active_out;
     CeedInt  strides[3] = {1, num_elem * Q, Q}; /* *NOPAD* */
 
+    // Create output restriction
     CeedCallBackend(CeedElemRestrictionCreateStrided(ceed_parent, num_elem, Q, num_active_in * num_active_out,
                                                      num_active_in * num_active_out * num_elem * Q, strides, rstr));
     // Create assembled vector
@@ -546,9 +521,9 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Hip(CeedOperator op, b
   // Assemble QFunction
   for (CeedInt in = 0; in < num_active_in; in++) {
     // Set Inputs
-    CeedCallBackend(CeedVectorSetValue(active_in[in], 1.0));
+    CeedCallBackend(CeedVectorSetValue(active_inputs[in], 1.0));
     if (num_active_in > 1) {
-      CeedCallBackend(CeedVectorSetValue(active_in[(in + num_active_in - 1) % num_active_in], 0.0));
+      CeedCallBackend(CeedVectorSetValue(active_inputs[(in + num_active_in - 1) % num_active_in], 0.0));
     }
     // Set Outputs
     for (CeedInt out = 0; out < num_output_fields; out++) {
@@ -567,7 +542,7 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Hip(CeedOperator op, b
     CeedCallBackend(CeedQFunctionApply(qf, Q * num_elem, impl->q_vecs_in, impl->q_vecs_out));
   }
 
-  // Un-set output Qvecs to prevent accidental overwrite of Assembled
+  // Un-set output q_vecs to prevent accidental overwrite of Assembled
   for (CeedInt out = 0; out < num_output_fields; out++) {
     CeedVector vec;
 
@@ -602,47 +577,14 @@ static int CeedOperatorLinearAssembleQFunctionUpdate_Hip(CeedOperator op, CeedVe
 }
 
 //------------------------------------------------------------------------------
-// Create point block restriction
-//------------------------------------------------------------------------------
-static int CreatePBRestriction(CeedElemRestriction rstr, CeedElemRestriction *point_block_rstr) {
-  Ceed           ceed;
-  CeedSize       l_size;
-  CeedInt        num_elem, num_comp, elem_size, comp_stride, *point_block_offsets;
-  const CeedInt *offsets;
-
-  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetOffsets(rstr, CEED_MEM_HOST, &offsets));
-
-  // Expand offsets
-  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
-  CeedCallBackend(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
-  CeedCallBackend(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
-  CeedInt shift = num_comp;
-
-  if (comp_stride != 1) shift *= num_comp;
-  CeedCallBackend(CeedCalloc(num_elem * elem_size, &point_block_offsets));
-  for (CeedInt i = 0; i < num_elem * elem_size; i++) point_block_offsets[i] = offsets[i] * shift;
-
-  // Create new restriction
-  CeedCallBackend(CeedElemRestrictionCreate(ceed, num_elem, elem_size, num_comp * num_comp, 1, l_size * num_comp, CEED_MEM_HOST, CEED_OWN_POINTER,
-                                            point_block_offsets, point_block_rstr));
-
-  // Cleanup
-  CeedCallBackend(CeedElemRestrictionRestoreOffsets(rstr, &offsets));
-  return CEED_ERROR_SUCCESS;
-}
-
-//------------------------------------------------------------------------------
-// Assemble diagonal setup
+// Assemble Diagonal Setup
 //------------------------------------------------------------------------------
-static inline int CeedOperatorAssembleDiagonalSetup_Hip(CeedOperator op, const bool is_point_block, CeedInt use_ceedsize_idx) {
+static inline int CeedOperatorAssembleDiagonalSetup_Hip(CeedOperator op, CeedInt use_ceedsize_idx) {
   Ceed                ceed;
   char               *diagonal_kernel_path, *diagonal_kernel_source;
-  CeedInt             num_input_fields, num_output_fields, num_e_mode_in = 0, num_comp = 0, dim = 1, num_e_mode_out = 0;
-  CeedEvalMode       *e_mode_in = NULL, *e_mode_out = NULL;
-  CeedElemRestriction rstr_in = NULL, rstr_out = NULL;
+  CeedInt             num_input_fields, num_output_fields, num_eval_modes_in = 0, num_eval_modes_out = 0;
+  CeedInt             num_comp, q_comp, num_nodes, num_qpts;
+  CeedEvalMode       *eval_modes_in = NULL, *eval_modes_out = NULL;
   CeedBasis           basis_in = NULL, basis_out = NULL;
   CeedQFunctionField *qf_fields;
   CeedQFunction       qf;
@@ -661,33 +603,20 @@ static inline int CeedOperatorAssembleDiagonalSetup_Hip(CeedOperator op, const b
 
     CeedCallBackend(CeedOperatorFieldGetVector(op_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
-      CeedEvalMode        e_mode;
-      CeedElemRestriction rstr;
-
-      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis_in));
-      CeedCallBackend(CeedBasisGetNumComponents(basis_in, &num_comp));
-      CeedCallBackend(CeedBasisGetDimension(basis_in, &dim));
-      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &rstr));
-      CeedCheck(!rstr_in || rstr_in == rstr, ceed, CEED_ERROR_BACKEND,
-                "Backend does not implement multi-field non-composite operator diagonal assembly");
-      rstr_in = rstr;
-      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &e_mode));
-      switch (e_mode) {
-        case CEED_EVAL_NONE:
-        case CEED_EVAL_INTERP:
-          CeedCallBackend(CeedRealloc(num_e_mode_in + 1, &e_mode_in));
-          e_mode_in[num_e_mode_in] = e_mode;
-          num_e_mode_in += 1;
-          break;
-        case CEED_EVAL_GRAD:
-          CeedCallBackend(CeedRealloc(num_e_mode_in + dim, &e_mode_in));
-          for (CeedInt d = 0; d < dim; d++) e_mode_in[num_e_mode_in + d] = e_mode;
-          num_e_mode_in += dim;
-          break;
-        case CEED_EVAL_WEIGHT:
-        case CEED_EVAL_DIV:
-        case CEED_EVAL_CURL:
-          break;  // Caught by QF Assembly
+      CeedBasis    basis;
+      CeedEvalMode eval_mode;
+
+      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis));
+      CeedCheck(!basis_in || basis_in == basis, ceed, CEED_ERROR_BACKEND,
+                "Backend does not implement operator diagonal assembly with multiple active bases");
+      basis_in = basis;
+      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_in, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_in + q_comp, &eval_modes_in));
+        for (CeedInt d = 0; d < q_comp; d++) eval_modes_in[num_eval_modes_in + d] = eval_mode;
+        num_eval_modes_in += q_comp;
       }
     }
   }
@@ -700,31 +629,20 @@ static inline int CeedOperatorAssembleDiagonalSetup_Hip(CeedOperator op, const b
 
     CeedCallBackend(CeedOperatorFieldGetVector(op_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
-      CeedEvalMode        e_mode;
-      CeedElemRestriction rstr;
-
-      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis_out));
-      CeedCallBackend(CeedOperatorFieldGetElemRestriction(op_fields[i], &rstr));
-      CeedCheck(!rstr_out || rstr_out == rstr, ceed, CEED_ERROR_BACKEND,
-                "Backend does not implement multi-field non-composite operator diagonal assembly");
-      rstr_out = rstr;
-      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &e_mode));
-      switch (e_mode) {
-        case CEED_EVAL_NONE:
-        case CEED_EVAL_INTERP:
-          CeedCallBackend(CeedRealloc(num_e_mode_out + 1, &e_mode_out));
-          e_mode_out[num_e_mode_out] = e_mode;
-          num_e_mode_out += 1;
-          break;
-        case CEED_EVAL_GRAD:
-          CeedCallBackend(CeedRealloc(num_e_mode_out + dim, &e_mode_out));
-          for (CeedInt d = 0; d < dim; d++) e_mode_out[num_e_mode_out + d] = e_mode;
-          num_e_mode_out += dim;
-          break;
-        case CEED_EVAL_WEIGHT:
-        case CEED_EVAL_DIV:
-        case CEED_EVAL_CURL:
-          break;  // Caught by QF Assembly
+      CeedBasis    basis;
+      CeedEvalMode eval_mode;
+
+      CeedCallBackend(CeedOperatorFieldGetBasis(op_fields[i], &basis));
+      CeedCheck(!basis_out || basis_out == basis, ceed, CEED_ERROR_BACKEND,
+                "Backend does not implement operator diagonal assembly with multiple active bases");
+      basis_out = basis;
+      CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_out, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_out + q_comp, &eval_modes_out));
+        for (CeedInt d = 0; d < q_comp; d++) eval_modes_out[num_eval_modes_out + d] = eval_mode;
+        num_eval_modes_out += q_comp;
       }
     }
   }
@@ -734,140 +652,189 @@ static inline int CeedOperatorAssembleDiagonalSetup_Hip(CeedOperator op, const b
   CeedCallBackend(CeedCalloc(1, &impl->diag));
   CeedOperatorDiag_Hip *diag = impl->diag;
 
-  diag->basis_in       = basis_in;
-  diag->basis_out      = basis_out;
-  diag->h_e_mode_in    = e_mode_in;
-  diag->h_e_mode_out   = e_mode_out;
-  diag->num_e_mode_in  = num_e_mode_in;
-  diag->num_e_mode_out = num_e_mode_out;
-
   // Assemble kernel
+  CeedCallBackend(CeedBasisGetNumNodes(basis_in, &num_nodes));
+  CeedCallBackend(CeedBasisGetNumComponents(basis_in, &num_comp));
+  if (basis_in == CEED_BASIS_NONE) num_qpts = num_nodes;
+  else CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts));
   CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/hip/hip-ref-operator-assemble-diagonal.h", &diagonal_kernel_path));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Diagonal Assembly Kernel Source -----\n");
   CeedCallBackend(CeedLoadSourceToBuffer(ceed, diagonal_kernel_path, &diagonal_kernel_source));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Diagonal Assembly Source Complete! -----\n");
-  CeedInt num_modes, num_qpts;
-  CeedCallBackend(CeedBasisGetNumNodes(basis_in, &num_modes));
-  CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts));
-  diag->num_modes = num_modes;
-  CeedCallBackend(CeedCompile_Hip(ceed, diagonal_kernel_source, &diag->module, 6, "NUMEMODEIN", num_e_mode_in, "NUMEMODEOUT", num_e_mode_out,
-                                  "NNODES", num_modes, "NQPTS", num_qpts, "NCOMP", num_comp, "CEEDSIZE", use_ceedsize_idx));
-  CeedCallBackend(CeedGetKernel_Hip(ceed, diag->module, "linearDiagonal", &diag->linearDiagonal));
-  CeedCallBackend(CeedGetKernel_Hip(ceed, diag->module, "linearPointBlockDiagonal", &diag->linearPointBlock));
+  CeedCallHip(ceed,
+              CeedCompile_Hip(ceed, diagonal_kernel_source, &diag->module, 6, "NUM_EVAL_MODES_IN", num_eval_modes_in, "NUM_EVAL_MODES_OUT",
+                              num_eval_modes_out, "NUM_COMP", num_comp, "NUM_NODES", num_nodes, "NUM_QPTS", num_qpts, "CEED_SIZE", use_ceedsize_idx));
+  CeedCallHip(ceed, CeedGetKernel_Hip(ceed, diag->module, "LinearDiagonal", &diag->LinearDiagonal));
+  CeedCallHip(ceed, CeedGetKernel_Hip(ceed, diag->module, "LinearPointBlockDiagonal", &diag->LinearPointBlock));
   CeedCallBackend(CeedFree(&diagonal_kernel_path));
   CeedCallBackend(CeedFree(&diagonal_kernel_source));
 
   // Basis matrices
-  const CeedInt     q_bytes      = num_qpts * sizeof(CeedScalar);
-  const CeedInt     interp_bytes = q_bytes * num_modes;
-  const CeedInt     grad_bytes   = q_bytes * num_modes * dim;
-  const CeedInt     e_mode_bytes = sizeof(CeedEvalMode);
-  const CeedScalar *interp_in, *interp_out, *grad_in, *grad_out;
+  const CeedInt interp_bytes    = num_nodes * num_qpts * sizeof(CeedScalar);
+  const CeedInt eval_mode_bytes = sizeof(CeedEvalMode);
+  bool          has_eval_none   = false;
 
   // CEED_EVAL_NONE
-  CeedScalar *identity     = NULL;
-  bool        is_eval_none = false;
-
-  for (CeedInt i = 0; i < num_e_mode_in; i++) is_eval_none = is_eval_none || (e_mode_in[i] == CEED_EVAL_NONE);
-  for (CeedInt i = 0; i < num_e_mode_out; i++) is_eval_none = is_eval_none || (e_mode_out[i] == CEED_EVAL_NONE);
-  if (is_eval_none) {
-    CeedCallBackend(CeedCalloc(num_qpts * num_modes, &identity));
-    for (CeedInt i = 0; i < (num_modes < num_qpts ? num_modes : num_qpts); i++) identity[i * num_modes + i] = 1.0;
+  for (CeedInt i = 0; i < num_eval_modes_in; i++) has_eval_none = has_eval_none || (eval_modes_in[i] == CEED_EVAL_NONE);
+  for (CeedInt i = 0; i < num_eval_modes_out; i++) has_eval_none = has_eval_none || (eval_modes_out[i] == CEED_EVAL_NONE);
+  if (has_eval_none) {
+    CeedScalar *identity = NULL;
+
+    CeedCallBackend(CeedCalloc(num_nodes * num_qpts, &identity));
+    for (CeedInt i = 0; i < (num_nodes < num_qpts ? num_nodes : num_qpts); i++) identity[i * num_nodes + i] = 1.0;
     CeedCallHip(ceed, hipMalloc((void **)&diag->d_identity, interp_bytes));
     CeedCallHip(ceed, hipMemcpy(diag->d_identity, identity, interp_bytes, hipMemcpyHostToDevice));
+    CeedCallBackend(CeedFree(&identity));
+  }
+
+  // CEED_EVAL_INTERP, CEED_EVAL_GRAD, CEED_EVAL_DIV, and CEED_EVAL_CURL
+  for (CeedInt in = 0; in < 2; in++) {
+    CeedFESpace fespace;
+    CeedBasis   basis = in ? basis_in : basis_out;
+
+    CeedCallBackend(CeedBasisGetFESpace(basis, &fespace));
+    switch (fespace) {
+      case CEED_FE_SPACE_H1: {
+        CeedInt           q_comp_interp, q_comp_grad;
+        const CeedScalar *interp, *grad;
+        CeedScalar       *d_interp, *d_grad;
+
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_GRAD, &q_comp_grad));
+
+        CeedCallBackend(CeedBasisGetInterp(basis, &interp));
+        CeedCallHip(ceed, hipMalloc((void **)&d_interp, interp_bytes * q_comp_interp));
+        CeedCallHip(ceed, hipMemcpy(d_interp, interp, interp_bytes * q_comp_interp, hipMemcpyHostToDevice));
+        CeedCallBackend(CeedBasisGetGrad(basis, &grad));
+        CeedCallHip(ceed, hipMalloc((void **)&d_grad, interp_bytes * q_comp_grad));
+        CeedCallHip(ceed, hipMemcpy(d_grad, grad, interp_bytes * q_comp_grad, hipMemcpyHostToDevice));
+        if (in) {
+          diag->d_interpin = d_interp;
+          diag->d_gradin   = d_grad;
+        } else {
+          diag->d_interpout = d_interp;
+          diag->d_gradout   = d_grad;
+        }
+      } break;
+      case CEED_FE_SPACE_HDIV: {
+        CeedInt           q_comp_interp, q_comp_div;
+        const CeedScalar *interp, *div;
+        CeedScalar       *d_interp, *d_div;
+
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_DIV, &q_comp_div));
+
+        CeedCallBackend(CeedBasisGetInterp(basis, &interp));
+        CeedCallHip(ceed, hipMalloc((void **)&d_interp, interp_bytes * q_comp_interp));
+        CeedCallHip(ceed, hipMemcpy(d_interp, interp, interp_bytes * q_comp_interp, hipMemcpyHostToDevice));
+        CeedCallBackend(CeedBasisGetDiv(basis, &div));
+        CeedCallHip(ceed, hipMalloc((void **)&d_div, interp_bytes * q_comp_div));
+        CeedCallHip(ceed, hipMemcpy(d_div, div, interp_bytes * q_comp_div, hipMemcpyHostToDevice));
+        if (in) {
+          diag->d_interpin = d_interp;
+          diag->d_divin    = d_div;
+        } else {
+          diag->d_interpout = d_interp;
+          diag->d_divout    = d_div;
+        }
+      } break;
+      case CEED_FE_SPACE_HCURL: {
+        CeedInt           q_comp_interp, q_comp_curl;
+        const CeedScalar *interp, *curl;
+        CeedScalar       *d_interp, *d_curl;
+
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+        CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_CURL, &q_comp_curl));
+
+        CeedCallBackend(CeedBasisGetInterp(basis, &interp));
+        CeedCallHip(ceed, hipMalloc((void **)&d_interp, interp_bytes * q_comp_interp));
+        CeedCallHip(ceed, hipMemcpy(d_interp, interp, interp_bytes * q_comp_interp, hipMemcpyHostToDevice));
+        CeedCallBackend(CeedBasisGetCurl(basis, &curl));
+        CeedCallHip(ceed, hipMalloc((void **)&d_curl, interp_bytes * q_comp_curl));
+        CeedCallHip(ceed, hipMemcpy(d_curl, curl, interp_bytes * q_comp_curl, hipMemcpyHostToDevice));
+        if (in) {
+          diag->d_interpin = d_interp;
+          diag->d_curlin   = d_curl;
+        } else {
+          diag->d_interpout = d_interp;
+          diag->d_curlout   = d_curl;
+        }
+      } break;
+    }
   }
 
-  // CEED_EVAL_INTERP
-  CeedCallBackend(CeedBasisGetInterp(basis_in, &interp_in));
-  CeedCallHip(ceed, hipMalloc((void **)&diag->d_interp_in, interp_bytes));
-  CeedCallHip(ceed, hipMemcpy(diag->d_interp_in, interp_in, interp_bytes, hipMemcpyHostToDevice));
-  CeedCallBackend(CeedBasisGetInterp(basis_out, &interp_out));
-  CeedCallHip(ceed, hipMalloc((void **)&diag->d_interp_out, interp_bytes));
-  CeedCallHip(ceed, hipMemcpy(diag->d_interp_out, interp_out, interp_bytes, hipMemcpyHostToDevice));
-
-  // CEED_EVAL_GRAD
-  CeedCallBackend(CeedBasisGetGrad(basis_in, &grad_in));
-  CeedCallHip(ceed, hipMalloc((void **)&diag->d_grad_in, grad_bytes));
-  CeedCallHip(ceed, hipMemcpy(diag->d_grad_in, grad_in, grad_bytes, hipMemcpyHostToDevice));
-  CeedCallBackend(CeedBasisGetGrad(basis_out, &grad_out));
-  CeedCallHip(ceed, hipMalloc((void **)&diag->d_grad_out, grad_bytes));
-  CeedCallHip(ceed, hipMemcpy(diag->d_grad_out, grad_out, grad_bytes, hipMemcpyHostToDevice));
-
-  // Arrays of e_modes
-  CeedCallHip(ceed, hipMalloc((void **)&diag->d_e_mode_in, num_e_mode_in * e_mode_bytes));
-  CeedCallHip(ceed, hipMemcpy(diag->d_e_mode_in, e_mode_in, num_e_mode_in * e_mode_bytes, hipMemcpyHostToDevice));
-  CeedCallHip(ceed, hipMalloc((void **)&diag->d_e_mode_out, num_e_mode_out * e_mode_bytes));
-  CeedCallHip(ceed, hipMemcpy(diag->d_e_mode_out, e_mode_out, num_e_mode_out * e_mode_bytes, hipMemcpyHostToDevice));
-
-  // Restriction
-  diag->diag_rstr = rstr_out;
+  // Arrays of eval_modes
+  CeedCallHip(ceed, hipMalloc((void **)&diag->d_eval_modes_in, num_eval_modes_in * eval_mode_bytes));
+  CeedCallHip(ceed, hipMemcpy(diag->d_eval_modes_in, eval_modes_in, num_eval_modes_in * eval_mode_bytes, hipMemcpyHostToDevice));
+  CeedCallHip(ceed, hipMalloc((void **)&diag->d_eval_modes_out, num_eval_modes_out * eval_mode_bytes));
+  CeedCallHip(ceed, hipMemcpy(diag->d_eval_modes_out, eval_modes_out, num_eval_modes_out * eval_mode_bytes, hipMemcpyHostToDevice));
+  CeedCallBackend(CeedFree(&eval_modes_in));
+  CeedCallBackend(CeedFree(&eval_modes_out));
   return CEED_ERROR_SUCCESS;
 }
 
 //------------------------------------------------------------------------------
-// Assemble diagonal common code
+// Assemble Diagonal Core
 //------------------------------------------------------------------------------
 static inline int CeedOperatorAssembleDiagonalCore_Hip(CeedOperator op, CeedVector assembled, CeedRequest *request, const bool is_point_block) {
   Ceed                ceed;
-  CeedSize            assembled_length = 0, assembled_qf_length = 0;
-  CeedInt             use_ceedsize_idx = 0, num_elem;
+  CeedSize            assembled_length, assembled_qf_length;
+  CeedInt             use_ceedsize_idx = 0, num_elem, num_nodes;
   CeedScalar         *elem_diag_array;
   const CeedScalar   *assembled_qf_array;
-  CeedVector          assembled_qf = NULL;
-  CeedElemRestriction rstr         = NULL;
+  CeedVector          assembled_qf   = NULL, elem_diag;
+  CeedElemRestriction assembled_rstr = NULL, rstr_in, rstr_out, diag_rstr;
   CeedOperator_Hip   *impl;
 
   CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
   CeedCallBackend(CeedOperatorGetData(op, &impl));
 
   // Assemble QFunction
-  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &rstr, request));
-  CeedCallBackend(CeedElemRestrictionDestroy(&rstr));
+  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &assembled_rstr, request));
+  CeedCallBackend(CeedElemRestrictionDestroy(&assembled_rstr));
+  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &assembled_qf_array));
 
   CeedCallBackend(CeedVectorGetLength(assembled, &assembled_length));
   CeedCallBackend(CeedVectorGetLength(assembled_qf, &assembled_qf_length));
   if ((assembled_length > INT_MAX) || (assembled_qf_length > INT_MAX)) use_ceedsize_idx = 1;
 
   // Setup
-  if (!impl->diag) CeedCallBackend(CeedOperatorAssembleDiagonalSetup_Hip(op, is_point_block, use_ceedsize_idx));
+  if (!impl->diag) CeedCallBackend(CeedOperatorAssembleDiagonalSetup_Hip(op, use_ceedsize_idx));
   CeedOperatorDiag_Hip *diag = impl->diag;
-  assert(diag != NULL);
 
-  // Restriction
-  if (is_point_block && !diag->point_block_diag_rstr) {
-    CeedElemRestriction point_block_diag_rstr;
-
-    CeedCallBackend(CreatePBRestriction(diag->diag_rstr, &point_block_diag_rstr));
-    diag->point_block_diag_rstr = point_block_diag_rstr;
-  }
-  CeedElemRestriction diag_rstr = is_point_block ? diag->point_block_diag_rstr : diag->diag_rstr;
-
-  // Create diagonal vector
-  CeedVector elem_diag = is_point_block ? diag->point_block_elem_diag : diag->elem_diag;
+  assert(diag != NULL);
 
-  if (!elem_diag) {
-    // Element diagonal vector
-    CeedCallBackend(CeedElemRestrictionCreateVector(diag_rstr, NULL, &elem_diag));
-    if (is_point_block) diag->point_block_elem_diag = elem_diag;
-    else diag->elem_diag = elem_diag;
+  // Restriction and diagonal vector
+  CeedCallBackend(CeedOperatorGetActiveElemRestrictions(op, &rstr_in, &rstr_out));
+  CeedCheck(rstr_in == rstr_out, ceed, CEED_ERROR_BACKEND,
+            "Cannot assemble operator diagonal with different input and output active element restrictions");
+  if (!is_point_block && !diag->diag_rstr) {
+    CeedCallBackend(CeedElemRestrictionCreateUnsignedCopy(rstr_out, &diag->diag_rstr));
+    CeedCallBackend(CeedElemRestrictionCreateVector(diag->diag_rstr, NULL, &diag->elem_diag));
+  } else if (is_point_block && !diag->point_block_diag_rstr) {
+    CeedCallBackend(CeedOperatorCreateActivePointBlockRestriction(rstr_out, &diag->point_block_diag_rstr));
+    CeedCallBackend(CeedElemRestrictionCreateVector(diag->point_block_diag_rstr, NULL, &diag->point_block_elem_diag));
   }
+  diag_rstr = is_point_block ? diag->point_block_diag_rstr : diag->diag_rstr;
+  elem_diag = is_point_block ? diag->point_block_elem_diag : diag->elem_diag;
   CeedCallBackend(CeedVectorSetValue(elem_diag, 0.0));
 
   // Assemble element operator diagonals
   CeedCallBackend(CeedVectorGetArray(elem_diag, CEED_MEM_DEVICE, &elem_diag_array));
-  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &assembled_qf_array));
   CeedCallBackend(CeedElemRestrictionGetNumElements(diag_rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(diag_rstr, &num_nodes));
 
   // Compute the diagonal of B^T D B
-  int   elem_per_block = 1;
-  int   grid           = num_elem / elem_per_block + ((num_elem / elem_per_block * elem_per_block < num_elem) ? 1 : 0);
-  void *args[]         = {(void *)&num_elem, &diag->d_identity,  &diag->d_interp_in,  &diag->d_grad_in,    &diag->d_interp_out,
-                          &diag->d_grad_out, &diag->d_e_mode_in, &diag->d_e_mode_out, &assembled_qf_array, &elem_diag_array};
+  CeedInt elems_per_block = 1;
+  CeedInt grid            = CeedDivUpInt(num_elem, elems_per_block);
+  void   *args[]          = {(void *)&num_elem,      &diag->d_identity,       &diag->d_interp_in,  &diag->d_grad_in, &diag->d_div_in,
+                             &diag->d_curl_in,       &diag->d_interp_out,     &diag->d_grad_out,   &diag->d_div_out, &diag->d_curl_out,
+                             &diag->d_eval_modes_in, &diag->d_eval_modes_out, &assembled_qf_array, &elem_diag_array};
 
   if (is_point_block) {
-    CeedCallBackend(CeedRunKernelDim_Hip(ceed, diag->linearPointBlock, grid, diag->num_modes, 1, elem_per_block, args));
+    CeedCallBackend(CeedRunKernelDim_Hip(ceed, diag->LinearPointBlock, grid, num_nodes, 1, elems_per_block, args));
   } else {
-    CeedCallBackend(CeedRunKernelDim_Hip(ceed, diag->linearDiagonal, grid, diag->num_modes, 1, elem_per_block, args));
+    CeedCallBackend(CeedRunKernelDim_Hip(ceed, diag->LinearDiagonal, grid, num_nodes, 1, elems_per_block, args));
   }
 
   // Restore arrays
@@ -899,13 +866,14 @@ static int CeedOperatorLinearAssembleAddPointBlockDiagonal_Hip(CeedOperator op,
 }
 
 //------------------------------------------------------------------------------
-// Single operator assembly setup
+// Single Operator Assembly Setup
 //------------------------------------------------------------------------------
 static int CeedSingleOperatorAssembleSetup_Hip(CeedOperator op, CeedInt use_ceedsize_idx) {
-  Ceed    ceed;
-  CeedInt num_input_fields, num_output_fields, num_e_mode_in = 0, dim = 1, num_B_in_mats_to_load = 0, size_B_in = 0, num_qpts = 0, elem_size = 0,
-                                               num_e_mode_out = 0, num_B_out_mats_to_load = 0, size_B_out = 0, num_elem, num_comp;
-  CeedEvalMode       *eval_mode_in = NULL, *eval_mode_out = NULL;
+  Ceed                ceed;
+  char               *assembly_kernel_path, *assembly_kernel_source;
+  CeedInt             num_input_fields, num_output_fields, num_eval_modes_in = 0, num_eval_modes_out = 0;
+  CeedInt             elem_size_in, num_qpts_in, num_comp_in, elem_size_out, num_qpts_out, num_comp_out, q_comp;
+  CeedEvalMode       *eval_modes_in = NULL, *eval_modes_out = NULL;
   CeedElemRestriction rstr_in = NULL, rstr_out = NULL;
   CeedBasis           basis_in = NULL, basis_out = NULL;
   CeedQFunctionField *qf_fields;
@@ -922,34 +890,30 @@ static int CeedSingleOperatorAssembleSetup_Hip(CeedOperator op, CeedInt use_ceed
   // Determine active input basis eval mode
   CeedCallBackend(CeedOperatorGetQFunction(op, &qf));
   CeedCallBackend(CeedQFunctionGetFields(qf, NULL, &qf_fields, NULL, NULL));
-  // Note that the kernel will treat each dimension of a gradient action separately;
-  // i.e., when an active input has a CEED_EVAL_GRAD mode, num_e_mode_in will increment by dim.
-  // However, for the purposes of loading the B matrices, it will be treated as one mode, and we will load/copy the entire gradient matrix at once, so
-  // num_B_in_mats_to_load will be incremented by 1.
   for (CeedInt i = 0; i < num_input_fields; i++) {
     CeedVector vec;
 
     CeedCallBackend(CeedOperatorFieldGetVector(input_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
+      CeedBasis    basis;
       CeedEvalMode eval_mode;
 
-      CeedCallBackend(CeedOperatorFieldGetBasis(input_fields[i], &basis_in));
-      CeedCallBackend(CeedBasisGetDimension(basis_in, &dim));
-      CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts));
+      CeedCallBackend(CeedOperatorFieldGetBasis(input_fields[i], &basis));
+      CeedCheck(!basis_in || basis_in == basis, ceed, CEED_ERROR_BACKEND, "Backend does not implement operator assembly with multiple active bases");
+      basis_in = basis;
       CeedCallBackend(CeedOperatorFieldGetElemRestriction(input_fields[i], &rstr_in));
-      CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_in, &elem_size));
+      CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_in, &elem_size_in));
+      if (basis_in == CEED_BASIS_NONE) num_qpts_in = elem_size_in;
+      else CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts_in));
       CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
-      if (eval_mode != CEED_EVAL_NONE) {
-        CeedCallBackend(CeedRealloc(num_B_in_mats_to_load + 1, &eval_mode_in));
-        eval_mode_in[num_B_in_mats_to_load] = eval_mode;
-        num_B_in_mats_to_load += 1;
-        if (eval_mode == CEED_EVAL_GRAD) {
-          num_e_mode_in += dim;
-          size_B_in += dim * elem_size * num_qpts;
-        } else {
-          num_e_mode_in += 1;
-          size_B_in += elem_size * num_qpts;
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_in, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF Assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_in + q_comp, &eval_modes_in));
+        for (CeedInt d = 0; d < q_comp; d++) {
+          eval_modes_in[num_eval_modes_in + d] = eval_mode;
         }
+        num_eval_modes_in += q_comp;
       }
     }
   }
@@ -961,106 +925,133 @@ static int CeedSingleOperatorAssembleSetup_Hip(CeedOperator op, CeedInt use_ceed
 
     CeedCallBackend(CeedOperatorFieldGetVector(output_fields[i], &vec));
     if (vec == CEED_VECTOR_ACTIVE) {
+      CeedBasis    basis;
       CeedEvalMode eval_mode;
 
-      CeedCallBackend(CeedOperatorFieldGetBasis(output_fields[i], &basis_out));
+      CeedCallBackend(CeedOperatorFieldGetBasis(output_fields[i], &basis));
+      CeedCheck(!basis_out || basis_out == basis, ceed, CEED_ERROR_BACKEND,
+                "Backend does not implement operator assembly with multiple active bases");
+      basis_out = basis;
       CeedCallBackend(CeedOperatorFieldGetElemRestriction(output_fields[i], &rstr_out));
-      CeedCheck(!rstr_out || rstr_out == rstr_in, ceed, CEED_ERROR_BACKEND, "Backend does not implement multi-field non-composite operator assembly");
+      CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_out, &elem_size_out));
+      if (basis_out == CEED_BASIS_NONE) num_qpts_out = elem_size_out;
+      else CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis_out, &num_qpts_out));
+      CeedCheck(num_qpts_in == num_qpts_out, ceed, CEED_ERROR_UNSUPPORTED,
+                "Active input and output bases must have the same number of quadrature points");
       CeedCallBackend(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
-      if (eval_mode != CEED_EVAL_NONE) {
-        CeedCallBackend(CeedRealloc(num_B_out_mats_to_load + 1, &eval_mode_out));
-        eval_mode_out[num_B_out_mats_to_load] = eval_mode;
-        num_B_out_mats_to_load += 1;
-        if (eval_mode == CEED_EVAL_GRAD) {
-          num_e_mode_out += dim;
-          size_B_out += dim * elem_size * num_qpts;
-        } else {
-          num_e_mode_out += 1;
-          size_B_out += elem_size * num_qpts;
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_out, eval_mode, &q_comp));
+      if (eval_mode != CEED_EVAL_WEIGHT) {
+        // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF Assembly
+        CeedCallBackend(CeedRealloc(num_eval_modes_out + q_comp, &eval_modes_out));
+        for (CeedInt d = 0; d < q_comp; d++) {
+          eval_modes_out[num_eval_modes_out + d] = eval_mode;
         }
+        num_eval_modes_out += q_comp;
       }
     }
   }
-
-  CeedCheck(num_e_mode_in > 0 && num_e_mode_out > 0, ceed, CEED_ERROR_UNSUPPORTED, "Cannot assemble operator without inputs/outputs");
-
-  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr_in, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr_in, &num_comp));
+  CeedCheck(num_eval_modes_in > 0 && num_eval_modes_out > 0, ceed, CEED_ERROR_UNSUPPORTED, "Cannot assemble operator without inputs/outputs");
 
   CeedCallBackend(CeedCalloc(1, &impl->asmb));
   CeedOperatorAssemble_Hip *asmb = impl->asmb;
-  asmb->num_elem                 = num_elem;
+  asmb->elems_per_block          = 1;
+  asmb->block_size_x             = elem_size_in;
+  asmb->block_size_y             = elem_size_out;
+
+  bool fallback = asmb->block_size_x * asmb->block_size_y * asmb->elems_per_block > 1024;
+
+  if (fallback) {
+    // Use fallback kernel with 1D threadblock
+    asmb->block_size_y = 1;
+  }
 
   // Compile kernels
-  int elem_per_block   = 1;
-  asmb->elem_per_block = elem_per_block;
-  CeedInt block_size   = elem_size * elem_size * elem_per_block;
-  char   *assembly_kernel_path, *assembly_kernel_source;
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr_in, &num_comp_in));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr_out, &num_comp_out));
   CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/hip/hip-ref-operator-assemble.h", &assembly_kernel_path));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Assembly Kernel Source -----\n");
   CeedCallBackend(CeedLoadSourceToBuffer(ceed, assembly_kernel_path, &assembly_kernel_source));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Assembly Source Complete! -----\n");
-  bool fallback = block_size > 1024;
-  if (fallback) {  // Use fallback kernel with 1D threadblock
-    block_size         = elem_size * elem_per_block;
-    asmb->block_size_x = elem_size;
-    asmb->block_size_y = 1;
-  } else {  // Use kernel with 2D threadblock
-    asmb->block_size_x = elem_size;
-    asmb->block_size_y = elem_size;
-  }
-  CeedCallBackend(CeedCompile_Hip(ceed, assembly_kernel_source, &asmb->module, 8, "NELEM", num_elem, "NUMEMODEIN", num_e_mode_in, "NUMEMODEOUT",
-                                  num_e_mode_out, "NQPTS", num_qpts, "NNODES", elem_size, "BLOCK_SIZE", block_size, "NCOMP", num_comp, "CEEDSIZE",
+  CeedCallBackend(CeedCompile_Hip(ceed, assembly_kernel_source, &asmb->module, 10, "NUM_EVAL_MODES_IN", num_eval_modes_in, "NUM_EVAL_MODES_OUT",
+                                  num_eval_modes_out, "NUM_COMP_IN", num_comp_in, "NUM_COMP_OUT", num_comp_out, "NUM_NODES_IN", elem_size_in,
+                                  "NUM_NODES_OUT", elem_size_out, "NUM_QPTS", num_qpts_in, "BLOCK_SIZE",
+                                  asmb->block_size_x * asmb->block_size_y * asmb->elems_per_block, "BLOCK_SIZE_Y", asmb->block_size_y, "CEED_SIZE",
                                   use_ceedsize_idx));
-  CeedCallBackend(CeedGetKernel_Hip(ceed, asmb->module, fallback ? "linearAssembleFallback" : "linearAssemble", &asmb->linearAssemble));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, asmb->module, "LinearAssemble", &asmb->LinearAssemble));
   CeedCallBackend(CeedFree(&assembly_kernel_path));
   CeedCallBackend(CeedFree(&assembly_kernel_source));
 
-  // Build 'full' B matrices (not 1D arrays used for tensor-product matrices)
-  const CeedScalar *interp_in, *grad_in;
-  CeedCallBackend(CeedBasisGetInterp(basis_in, &interp_in));
-  CeedCallBackend(CeedBasisGetGrad(basis_in, &grad_in));
-
-  // Load into B_in, in order that they will be used in eval_mode
-  const CeedInt in_bytes  = size_B_in * sizeof(CeedScalar);
-  CeedInt       mat_start = 0;
-
-  CeedCallHip(ceed, hipMalloc((void **)&asmb->d_B_in, in_bytes));
-  for (int i = 0; i < num_B_in_mats_to_load; i++) {
-    CeedEvalMode eval_mode = eval_mode_in[i];
-    if (eval_mode == CEED_EVAL_INTERP) {
-      CeedCallHip(ceed, hipMemcpy(&asmb->d_B_in[mat_start], interp_in, elem_size * num_qpts * sizeof(CeedScalar), hipMemcpyHostToDevice));
-      mat_start += elem_size * num_qpts;
-    } else if (eval_mode == CEED_EVAL_GRAD) {
-      CeedCallHip(ceed, hipMemcpy(&asmb->d_B_in[mat_start], grad_in, dim * elem_size * num_qpts * sizeof(CeedScalar), hipMemcpyHostToDevice));
-      mat_start += dim * elem_size * num_qpts;
+  // Load into B_in, in order that they will be used in eval_modes_in
+  {
+    const CeedInt in_bytes           = elem_size_in * num_qpts_in * num_eval_modes_in * sizeof(CeedScalar);
+    CeedInt       d_in               = 0;
+    CeedEvalMode  eval_modes_in_prev = CEED_EVAL_NONE;
+    bool          has_eval_none      = false;
+    CeedScalar   *identity           = NULL;
+
+    for (CeedInt i = 0; i < num_eval_modes_in; i++) {
+      has_eval_none = has_eval_none || (eval_modes_in[i] == CEED_EVAL_NONE);
+    }
+    if (has_eval_none) {
+      CeedCallBackend(CeedCalloc(elem_size_in * num_qpts_in, &identity));
+      for (CeedInt i = 0; i < (elem_size_in < num_qpts_in ? elem_size_in : num_qpts_in); i++) identity[i * elem_size_in + i] = 1.0;
     }
-  }
 
-  const CeedScalar *interp_out, *grad_out;
+    CeedCallHip(ceed, hipMalloc((void **)&asmb->d_B_in, in_bytes));
+    for (CeedInt i = 0; i < num_eval_modes_in; i++) {
+      const CeedScalar *h_B_in;
 
-  // Note that this function currently assumes 1 basis, so this should always be true for now
-  if (basis_out == basis_in) {
-    interp_out = interp_in;
-    grad_out   = grad_in;
-  } else {
-    CeedCallBackend(CeedBasisGetInterp(basis_out, &interp_out));
-    CeedCallBackend(CeedBasisGetGrad(basis_out, &grad_out));
+      CeedCallBackend(CeedOperatorGetBasisPointer(basis_in, eval_modes_in[i], identity, &h_B_in));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_in, eval_modes_in[i], &q_comp));
+      if (q_comp > 1) {
+        if (i == 0 || eval_modes_in[i] != eval_modes_in_prev) d_in = 0;
+        else h_B_in = &h_B_in[(++d_in) * elem_size_in * num_qpts_in];
+      }
+      eval_modes_in_prev = eval_modes_in[i];
+
+      CeedCallHip(ceed, hipMemcpy(&asmb->d_B_in[i * elem_size_in * num_qpts_in], h_B_in, elem_size_in * num_qpts_in * sizeof(CeedScalar),
+                                  hipMemcpyHostToDevice));
+    }
+
+    if (identity) {
+      CeedCallBackend(CeedFree(&identity));
+    }
   }
 
-  // Load into B_out, in order that they will be used in eval_mode
-  const CeedInt out_bytes = size_B_out * sizeof(CeedScalar);
-
-  mat_start = 0;
-  CeedCallHip(ceed, hipMalloc((void **)&asmb->d_B_out, out_bytes));
-  for (int i = 0; i < num_B_out_mats_to_load; i++) {
-    CeedEvalMode eval_mode = eval_mode_out[i];
-    if (eval_mode == CEED_EVAL_INTERP) {
-      CeedCallHip(ceed, hipMemcpy(&asmb->d_B_out[mat_start], interp_out, elem_size * num_qpts * sizeof(CeedScalar), hipMemcpyHostToDevice));
-      mat_start += elem_size * num_qpts;
-    } else if (eval_mode == CEED_EVAL_GRAD) {
-      CeedCallHip(ceed, hipMemcpy(&asmb->d_B_out[mat_start], grad_out, dim * elem_size * num_qpts * sizeof(CeedScalar), hipMemcpyHostToDevice));
-      mat_start += dim * elem_size * num_qpts;
+  // Load into B_out, in order that they will be used in eval_modes_out
+  {
+    const CeedInt out_bytes           = elem_size_out * num_qpts_out * num_eval_modes_out * sizeof(CeedScalar);
+    CeedInt       d_out               = 0;
+    CeedEvalMode  eval_modes_out_prev = CEED_EVAL_NONE;
+    bool          has_eval_none       = false;
+    CeedScalar   *identity            = NULL;
+
+    for (CeedInt i = 0; i < num_eval_modes_out; i++) {
+      has_eval_none = has_eval_none || (eval_modes_out[i] == CEED_EVAL_NONE);
+    }
+    if (has_eval_none) {
+      CeedCallBackend(CeedCalloc(elem_size_out * num_qpts_out, &identity));
+      for (CeedInt i = 0; i < (elem_size_out < num_qpts_out ? elem_size_out : num_qpts_out); i++) identity[i * elem_size_out + i] = 1.0;
+    }
+
+    CeedCallHip(ceed, hipMalloc((void **)&asmb->d_B_out, out_bytes));
+    for (CeedInt i = 0; i < num_eval_modes_out; i++) {
+      const CeedScalar *h_B_out;
+
+      CeedCallBackend(CeedOperatorGetBasisPointer(basis_out, eval_modes_out[i], identity, &h_B_out));
+      CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis_out, eval_modes_out[i], &q_comp));
+      if (q_comp > 1) {
+        if (i == 0 || eval_modes_out[i] != eval_modes_out_prev) d_out = 0;
+        else h_B_out = &h_B_out[(++d_out) * elem_size_out * num_qpts_out];
+      }
+      eval_modes_out_prev = eval_modes_out[i];
+
+      CeedCallHip(ceed, hipMemcpy(&asmb->d_B_out[i * elem_size_out * num_qpts_out], h_B_out, elem_size_out * num_qpts_out * sizeof(CeedScalar),
+                                  hipMemcpyHostToDevice));
+    }
+
+    if (identity) {
+      CeedCallBackend(CeedFree(&identity));
     }
   }
   return CEED_ERROR_SUCCESS;
@@ -1077,47 +1068,96 @@ static int CeedSingleOperatorAssembleSetup_Hip(CeedOperator op, CeedInt use_ceed
 static int CeedSingleOperatorAssemble_Hip(CeedOperator op, CeedInt offset, CeedVector values) {
   Ceed                ceed;
   CeedSize            values_length = 0, assembled_qf_length = 0;
-  CeedInt             use_ceedsize_idx = 0;
+  CeedInt             use_ceedsize_idx = 0, num_elem_in, num_elem_out, elem_size_in, elem_size_out;
   CeedScalar         *values_array;
-  const CeedScalar   *qf_array;
-  CeedVector          assembled_qf = NULL;
-  CeedElemRestriction rstr_q       = NULL;
+  const CeedScalar   *assembled_qf_array;
+  CeedVector          assembled_qf   = NULL;
+  CeedElemRestriction assembled_rstr = NULL, rstr_in, rstr_out;
+  CeedRestrictionType rstr_type_in, rstr_type_out;
+  const bool         *orients_in = NULL, *orients_out = NULL;
+  const CeedInt8     *curl_orients_in = NULL, *curl_orients_out = NULL;
   CeedOperator_Hip   *impl;
 
   CeedCallBackend(CeedOperatorGetCeed(op, &ceed));
   CeedCallBackend(CeedOperatorGetData(op, &impl));
 
   // Assemble QFunction
-  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &rstr_q, CEED_REQUEST_IMMEDIATE));
-  CeedCallBackend(CeedElemRestrictionDestroy(&rstr_q));
-  CeedCallBackend(CeedVectorGetArray(values, CEED_MEM_DEVICE, &values_array));
-  values_array += offset;
-  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &qf_array));
+  CeedCallBackend(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &assembled_rstr, CEED_REQUEST_IMMEDIATE));
+  CeedCallBackend(CeedElemRestrictionDestroy(&assembled_rstr));
+  CeedCallBackend(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_DEVICE, &assembled_qf_array));
 
   CeedCallBackend(CeedVectorGetLength(values, &values_length));
   CeedCallBackend(CeedVectorGetLength(assembled_qf, &assembled_qf_length));
   if ((values_length > INT_MAX) || (assembled_qf_length > INT_MAX)) use_ceedsize_idx = 1;
+
   // Setup
-  if (!impl->asmb) {
-    CeedCallBackend(CeedSingleOperatorAssembleSetup_Hip(op, use_ceedsize_idx));
-    assert(impl->asmb != NULL);
+  if (!impl->asmb) CeedCallBackend(CeedSingleOperatorAssembleSetup_Hip(op, use_ceedsize_idx));
+  CeedOperatorAssemble_Hip *asmb = impl->asmb;
+
+  assert(asmb != NULL);
+
+  // Assemble element operator
+  CeedCallBackend(CeedVectorGetArray(values, CEED_MEM_DEVICE, &values_array));
+  values_array += offset;
+
+  CeedCallBackend(CeedOperatorGetActiveElemRestrictions(op, &rstr_in, &rstr_out));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr_in, &num_elem_in));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_in, &elem_size_in));
+
+  CeedCallBackend(CeedElemRestrictionGetType(rstr_in, &rstr_type_in));
+  if (rstr_type_in == CEED_RESTRICTION_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionGetOrientations(rstr_in, CEED_MEM_DEVICE, &orients_in));
+  } else if (rstr_type_in == CEED_RESTRICTION_CURL_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionGetCurlOrientations(rstr_in, CEED_MEM_DEVICE, &curl_orients_in));
+  }
+
+  if (rstr_in != rstr_out) {
+    CeedCallBackend(CeedElemRestrictionGetNumElements(rstr_out, &num_elem_out));
+    CeedCheck(num_elem_in == num_elem_out, ceed, CEED_ERROR_UNSUPPORTED,
+              "Active input and output operator restrictions must have the same number of elements");
+    CeedCallBackend(CeedElemRestrictionGetElementSize(rstr_out, &elem_size_out));
+
+    CeedCallBackend(CeedElemRestrictionGetType(rstr_out, &rstr_type_out));
+    if (rstr_type_out == CEED_RESTRICTION_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionGetOrientations(rstr_out, CEED_MEM_DEVICE, &orients_out));
+    } else if (rstr_type_out == CEED_RESTRICTION_CURL_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionGetCurlOrientations(rstr_out, CEED_MEM_DEVICE, &curl_orients_out));
+    }
+  } else {
+    elem_size_out    = elem_size_in;
+    orients_out      = orients_in;
+    curl_orients_out = curl_orients_in;
   }
 
   // Compute B^T D B
-  const CeedInt num_elem       = impl->asmb->num_elem;
-  const CeedInt elem_per_block = impl->asmb->elem_per_block;
-  const CeedInt grid           = num_elem / elem_per_block + ((num_elem / elem_per_block * elem_per_block < num_elem) ? 1 : 0);
-  void         *args[]         = {&impl->asmb->d_B_in, &impl->asmb->d_B_out, &qf_array, &values_array};
+  CeedInt shared_mem =
+      ((curl_orients_in || curl_orients_out ? elem_size_in * elem_size_out : 0) + (curl_orients_in ? elem_size_in * asmb->block_size_y : 0)) *
+      sizeof(CeedScalar);
+  CeedInt grid   = CeedDivUpInt(num_elem_in, asmb->elems_per_block);
+  void   *args[] = {(void *)&num_elem_in, &asmb->d_B_in,     &asmb->d_B_out,      &orients_in,  &curl_orients_in,
+                    &orients_out,         &curl_orients_out, &assembled_qf_array, &values_array};
 
   CeedCallBackend(
-      CeedRunKernelDim_Hip(ceed, impl->asmb->linearAssemble, grid, impl->asmb->block_size_x, impl->asmb->block_size_y, elem_per_block, args));
+      CeedRunKernelDimShared_Hip(ceed, asmb->LinearAssemble, grid, asmb->block_size_x, asmb->block_size_y, asmb->elems_per_block, shared_mem, args));
 
   // Restore arrays
   CeedCallBackend(CeedVectorRestoreArray(values, &values_array));
-  CeedCallBackend(CeedVectorRestoreArrayRead(assembled_qf, &qf_array));
+  CeedCallBackend(CeedVectorRestoreArrayRead(assembled_qf, &assembled_qf_array));
 
   // Cleanup
   CeedCallBackend(CeedVectorDestroy(&assembled_qf));
+  if (rstr_type_in == CEED_RESTRICTION_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionRestoreOrientations(rstr_in, &orients_in));
+  } else if (rstr_type_in == CEED_RESTRICTION_CURL_ORIENTED) {
+    CeedCallBackend(CeedElemRestrictionRestoreCurlOrientations(rstr_in, &curl_orients_in));
+  }
+  if (rstr_in != rstr_out) {
+    if (rstr_type_out == CEED_RESTRICTION_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionRestoreOrientations(rstr_out, &orients_out));
+    } else if (rstr_type_out == CEED_RESTRICTION_CURL_ORIENTED) {
+      CeedCallBackend(CeedElemRestrictionRestoreCurlOrientations(rstr_out, &curl_orients_out));
+    }
+  }
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/backends/hip-ref/ceed-hip-ref-restriction.c b/backends/hip-ref/ceed-hip-ref-restriction.c
index 0dd11b16..2df567a4 100644
--- a/backends/hip-ref/ceed-hip-ref-restriction.c
+++ b/backends/hip-ref/ceed-hip-ref-restriction.c
@@ -18,22 +18,23 @@
 #include "ceed-hip-ref.h"
 
 //------------------------------------------------------------------------------
-// Apply restriction
+// Core apply restriction code
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionApply_Hip(CeedElemRestriction r, CeedTransposeMode t_mode, CeedVector u, CeedVector v, CeedRequest *request) {
+static inline int CeedElemRestrictionApply_Hip_Core(CeedElemRestriction rstr, CeedTransposeMode t_mode, bool use_signs, bool use_orients,
+                                                    CeedVector u, CeedVector v, CeedRequest *request) {
   Ceed                     ceed;
-  Ceed_Hip                *data;
   CeedInt                  num_elem, elem_size;
+  CeedRestrictionType      rstr_type;
+  CUfunction               kernel;
   const CeedScalar        *d_u;
   CeedScalar              *d_v;
   CeedElemRestriction_Hip *impl;
-  hipFunction_t            kernel;
 
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedGetData(ceed, &data));
-  CeedElemRestrictionGetNumElements(r, &num_elem);
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetType(rstr, &rstr_type));
   const CeedInt num_nodes = impl->num_nodes;
 
   // Get vectors
@@ -49,45 +50,124 @@ static int CeedElemRestrictionApply_Hip(CeedElemRestriction r, CeedTransposeMode
   // Restrict
   if (t_mode == CEED_NOTRANSPOSE) {
     // L-vector -> E-vector
-    if (impl->d_ind) {
-      // -- Offsets provided
-      kernel             = impl->OffsetNoTranspose;
-      void   *args[]     = {&num_elem, &impl->d_ind, &d_u, &d_v};
-      CeedInt block_size = elem_size < 256 ? (elem_size > 64 ? elem_size : 64) : 256;
-
-      CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
-    } else {
-      // -- Strided restriction
-      kernel             = impl->StridedNoTranspose;
-      void   *args[]     = {&num_elem, &d_u, &d_v};
-      CeedInt block_size = elem_size < 256 ? (elem_size > 64 ? elem_size : 64) : 256;
-
-      CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
+    const CeedInt block_size = elem_size < 256 ? (elem_size > 64 ? elem_size : 64) : 256;
+    const CeedInt grid       = CeedDivUpInt(num_nodes, block_size);
+
+    switch (rstr_type) {
+      case CEED_RESTRICTION_STRIDED: {
+        kernel       = impl->StridedNoTranspose;
+        void *args[] = {&num_elem, &d_u, &d_v};
+
+        CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+      } break;
+      case CEED_RESTRICTION_STANDARD: {
+        kernel       = impl->OffsetNoTranspose;
+        void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+        CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+      } break;
+      case CEED_RESTRICTION_ORIENTED: {
+        if (use_signs) {
+          kernel       = impl->OrientedNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        } else {
+          kernel       = impl->OffsetNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        }
+      } break;
+      case CEED_RESTRICTION_CURL_ORIENTED: {
+        if (use_signs && use_orients) {
+          kernel       = impl->CurlOrientedNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        } else if (use_orients) {
+          kernel       = impl->CurlOrientedUnsignedNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        } else {
+          kernel       = impl->OffsetNoTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        }
+      } break;
     }
   } else {
     // E-vector -> L-vector
-    if (impl->d_ind) {
-      // -- Offsets provided
-      CeedInt block_size = 64;
-
-      if (impl->OffsetTranspose) {
-        kernel       = impl->OffsetTranspose;
-        void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
-
-        CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
-      } else {
-        kernel       = impl->OffsetTransposeDet;
-        void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
-
-        CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
-      }
-    } else {
-      // -- Strided restriction
-      kernel             = impl->StridedTranspose;
-      void   *args[]     = {&num_elem, &d_u, &d_v};
-      CeedInt block_size = 64;
-
-      CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, CeedDivUpInt(num_nodes, block_size), block_size, args));
+    const CeedInt block_size = 64;
+    const CeedInt grid       = CeedDivUpInt(num_nodes, block_size);
+
+    switch (rstr_type) {
+      case CEED_RESTRICTION_STRIDED: {
+        kernel       = impl->StridedTranspose;
+        void *args[] = {&num_elem, &d_u, &d_v};
+
+        CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+      } break;
+      case CEED_RESTRICTION_STANDARD: {
+        if (impl->OffsetTranspose) {
+          kernel       = impl->OffsetTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        } else {
+          kernel       = impl->OffsetTransposeDet;
+          void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        }
+      } break;
+      case CEED_RESTRICTION_ORIENTED: {
+        if (use_signs) {
+          kernel       = impl->OrientedTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        } else {
+          if (impl->OffsetTranspose) {
+            kernel       = impl->OffsetTranspose;
+            void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+          } else {
+            kernel       = impl->OffsetTransposeDet;
+            void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+          }
+        }
+      } break;
+      case CEED_RESTRICTION_CURL_ORIENTED: {
+        if (use_signs && use_orients) {
+          kernel       = impl->CurlOrientedTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        } else if (use_orients) {
+          kernel       = impl->CurlOrientedUnsignedTranspose;
+          void *args[] = {&num_elem, &impl->d_ind, &impl->d_curl_orients, &d_u, &d_v};
+
+          CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+        } else {
+          if (impl->OffsetTranspose) {
+            kernel       = impl->OffsetTranspose;
+            void *args[] = {&num_elem, &impl->d_ind, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+          } else {
+            kernel       = impl->OffsetTransposeDet;
+            void *args[] = {&impl->d_l_vec_indices, &impl->d_t_indices, &impl->d_t_offsets, &d_u, &d_v};
+
+            CeedCallBackend(CeedRunKernel_Hip(ceed, kernel, grid, block_size, args));
+          }
+        }
+      } break;
     }
   }
 
@@ -99,6 +179,29 @@ static int CeedElemRestrictionApply_Hip(CeedElemRestriction r, CeedTransposeMode
   return CEED_ERROR_SUCCESS;
 }
 
+//------------------------------------------------------------------------------
+// Apply restriction
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionApply_Hip(CeedElemRestriction rstr, CeedTransposeMode t_mode, CeedVector u, CeedVector v, CeedRequest *request) {
+  return CeedElemRestrictionApply_Hip_Core(rstr, t_mode, true, true, u, v, request);
+}
+
+//------------------------------------------------------------------------------
+// Apply unsigned restriction
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionApplyUnsigned_Hip(CeedElemRestriction rstr, CeedTransposeMode t_mode, CeedVector u, CeedVector v,
+                                                CeedRequest *request) {
+  return CeedElemRestrictionApply_Hip_Core(rstr, t_mode, false, true, u, v, request);
+}
+
+//------------------------------------------------------------------------------
+// Apply unoriented restriction
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionApplyUnoriented_Hip(CeedElemRestriction rstr, CeedTransposeMode t_mode, CeedVector u, CeedVector v,
+                                                  CeedRequest *request) {
+  return CeedElemRestrictionApply_Hip_Core(rstr, t_mode, false, false, u, v, request);
+}
+
 //------------------------------------------------------------------------------
 // Get offsets
 //------------------------------------------------------------------------------
@@ -117,21 +220,61 @@ static int CeedElemRestrictionGetOffsets_Hip(CeedElemRestriction rstr, CeedMemTy
   return CEED_ERROR_SUCCESS;
 }
 
+//------------------------------------------------------------------------------
+// Get orientations
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionGetOrientations_Hip(CeedElemRestriction rstr, CeedMemType mem_type, const bool **orients) {
+  CeedElemRestriction_Hip *impl;
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+
+  switch (mem_type) {
+    case CEED_MEM_HOST:
+      *orients = impl->h_orients;
+      break;
+    case CEED_MEM_DEVICE:
+      *orients = impl->d_orients;
+      break;
+  }
+  return CEED_ERROR_SUCCESS;
+}
+
+//------------------------------------------------------------------------------
+// Get curl-conforming orientations
+//------------------------------------------------------------------------------
+static int CeedElemRestrictionGetCurlOrientations_Hip(CeedElemRestriction rstr, CeedMemType mem_type, const CeedInt8 **curl_orients) {
+  CeedElemRestriction_Hip *impl;
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+
+  switch (mem_type) {
+    case CEED_MEM_HOST:
+      *curl_orients = impl->h_curl_orients;
+      break;
+    case CEED_MEM_DEVICE:
+      *curl_orients = impl->d_curl_orients;
+      break;
+  }
+  return CEED_ERROR_SUCCESS;
+}
+
 //------------------------------------------------------------------------------
 // Destroy restriction
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionDestroy_Hip(CeedElemRestriction r) {
+static int CeedElemRestrictionDestroy_Hip(CeedElemRestriction rstr) {
   Ceed                     ceed;
   CeedElemRestriction_Hip *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
   CeedCallHip(ceed, hipModuleUnload(impl->module));
   CeedCallBackend(CeedFree(&impl->h_ind_allocated));
   CeedCallHip(ceed, hipFree(impl->d_ind_allocated));
   CeedCallHip(ceed, hipFree(impl->d_t_offsets));
   CeedCallHip(ceed, hipFree(impl->d_t_indices));
   CeedCallHip(ceed, hipFree(impl->d_l_vec_indices));
+  CeedCallBackend(CeedFree(&impl->h_orients_allocated));
+  CeedCallHip(ceed, hipFree(impl->d_orients_allocated));
+  CeedCallBackend(CeedFree(&impl->h_curl_orients_allocated));
+  CeedCallHip(ceed, hipFree(impl->d_curl_orients_allocated));
   CeedCallBackend(CeedFree(&impl));
   return CEED_ERROR_SUCCESS;
 }
@@ -139,23 +282,25 @@ static int CeedElemRestrictionDestroy_Hip(CeedElemRestriction r) {
 //------------------------------------------------------------------------------
 // Create transpose offsets and indices
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionOffset_Hip(const CeedElemRestriction r, const CeedInt *indices) {
+static int CeedElemRestrictionOffset_Hip(const CeedElemRestriction rstr, const CeedInt *indices) {
   Ceed                     ceed;
   bool                    *is_node;
   CeedSize                 l_size;
-  CeedInt                  num_elem, elem_size, num_comp, num_nodes = 0, *ind_to_offset, *l_vec_indices, *t_offsets, *t_indices;
+  CeedInt                  num_elem, elem_size, num_comp, num_nodes = 0;
+  CeedInt                 *ind_to_offset, *l_vec_indices, *t_offsets, *t_indices;
   CeedElemRestriction_Hip *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
-  CeedCallBackend(CeedElemRestrictionGetNumElements(r, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
-  CeedCallBackend(CeedElemRestrictionGetLVectorSize(r, &l_size));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(r, &num_comp));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
   const CeedInt size_indices = num_elem * elem_size;
 
   // Count num_nodes
   CeedCallBackend(CeedCalloc(l_size, &is_node));
+
   for (CeedInt i = 0; i < size_indices; i++) is_node[indices[i]] = 1;
   for (CeedInt i = 0; i < l_size; i++) num_nodes += is_node[i];
   impl->num_nodes = num_nodes;
@@ -218,119 +363,195 @@ static int CeedElemRestrictionOffset_Hip(const CeedElemRestriction r, const Ceed
 // Create restriction
 //------------------------------------------------------------------------------
 int CeedElemRestrictionCreate_Hip(CeedMemType mem_type, CeedCopyMode copy_mode, const CeedInt *indices, const bool *orients,
-                                  const CeedInt8 *curl_orients, CeedElemRestriction r) {
+                                  const CeedInt8 *curl_orients, CeedElemRestriction rstr) {
   Ceed                     ceed, ceed_parent;
-  bool                     is_deterministic, is_strided;
-  char                    *restriction_kernel_path, *restriction_kernel_source;
+  bool                     is_deterministic;
   CeedInt                  num_elem, num_comp, elem_size, comp_stride = 1;
   CeedRestrictionType      rstr_type;
+  char                    *restriction_kernel_path, *restriction_kernel_source;
   CeedElemRestriction_Hip *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedCalloc(1, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
   CeedCallBackend(CeedGetParent(ceed, &ceed_parent));
   CeedCallBackend(CeedIsDeterministic(ceed_parent, &is_deterministic));
-  CeedCallBackend(CeedElemRestrictionGetNumElements(r, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(r, &num_comp));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
-  CeedInt size       = num_elem * elem_size;
-  CeedInt strides[3] = {1, size, elem_size};
-  CeedInt layout[3]  = {1, elem_size * num_elem, elem_size};
-
-  CeedCallBackend(CeedElemRestrictionGetType(r, &rstr_type));
-  CeedCheck(rstr_type != CEED_RESTRICTION_ORIENTED && rstr_type != CEED_RESTRICTION_CURL_ORIENTED, ceed, CEED_ERROR_BACKEND,
-            "Backend does not implement CeedElemRestrictionCreateOriented or CeedElemRestrictionCreateCurlOriented");
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  const CeedInt size       = num_elem * elem_size;
+  CeedInt       strides[3] = {1, size, elem_size};
+  CeedInt       layout[3]  = {1, elem_size * num_elem, elem_size};
 
   // Stride data
-  CeedCallBackend(CeedElemRestrictionIsStrided(r, &is_strided));
-  if (is_strided) {
+  CeedCallBackend(CeedElemRestrictionGetType(rstr, &rstr_type));
+  if (rstr_type == CEED_RESTRICTION_STRIDED) {
     bool has_backend_strides;
 
-    CeedCallBackend(CeedElemRestrictionHasBackendStrides(r, &has_backend_strides));
+    CeedCallBackend(CeedElemRestrictionHasBackendStrides(rstr, &has_backend_strides));
     if (!has_backend_strides) {
-      CeedCallBackend(CeedElemRestrictionGetStrides(r, &strides));
+      CeedCallBackend(CeedElemRestrictionGetStrides(rstr, &strides));
     }
   } else {
-    CeedCallBackend(CeedElemRestrictionGetCompStride(r, &comp_stride));
+    CeedCallBackend(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
   }
 
-  impl->h_ind           = NULL;
-  impl->h_ind_allocated = NULL;
-  impl->d_ind           = NULL;
-  impl->d_ind_allocated = NULL;
-  impl->d_t_indices     = NULL;
-  impl->d_t_offsets     = NULL;
-  impl->num_nodes       = size;
-  CeedCallBackend(CeedElemRestrictionSetData(r, impl));
-  CeedCallBackend(CeedElemRestrictionSetELayout(r, layout));
-
-  // Set up device indices/offset arrays
-  switch (mem_type) {
-    case CEED_MEM_HOST: {
-      switch (copy_mode) {
-        case CEED_OWN_POINTER:
-          impl->h_ind_allocated = (CeedInt *)indices;
-          impl->h_ind           = (CeedInt *)indices;
-          break;
-        case CEED_USE_POINTER:
-          impl->h_ind = (CeedInt *)indices;
-          break;
-        case CEED_COPY_VALUES:
-          if (indices != NULL) {
-            CeedCallBackend(CeedMalloc(elem_size * num_elem, &impl->h_ind_allocated));
-            memcpy(impl->h_ind_allocated, indices, elem_size * num_elem * sizeof(CeedInt));
+  CeedCallBackend(CeedCalloc(1, &impl));
+  impl->num_nodes                = size;
+  impl->h_ind                    = NULL;
+  impl->h_ind_allocated          = NULL;
+  impl->d_ind                    = NULL;
+  impl->d_ind_allocated          = NULL;
+  impl->d_t_indices              = NULL;
+  impl->d_t_offsets              = NULL;
+  impl->h_orients                = NULL;
+  impl->h_orients_allocated      = NULL;
+  impl->d_orients                = NULL;
+  impl->d_orients_allocated      = NULL;
+  impl->h_curl_orients           = NULL;
+  impl->h_curl_orients_allocated = NULL;
+  impl->d_curl_orients           = NULL;
+  impl->d_curl_orients_allocated = NULL;
+  CeedCallBackend(CeedElemRestrictionSetData(rstr, impl));
+  CeedCallBackend(CeedElemRestrictionSetELayout(rstr, layout));
+
+  // Set up device offset/orientation arrays
+  if (rstr_type != CEED_RESTRICTION_STRIDED) {
+    switch (mem_type) {
+      case CEED_MEM_HOST: {
+        switch (copy_mode) {
+          case CEED_OWN_POINTER:
+            impl->h_ind_allocated = (CeedInt *)indices;
+            impl->h_ind           = (CeedInt *)indices;
+            break;
+          case CEED_USE_POINTER:
+            impl->h_ind = (CeedInt *)indices;
+            break;
+          case CEED_COPY_VALUES:
+            CeedCallBackend(CeedMalloc(size, &impl->h_ind_allocated));
+            memcpy(impl->h_ind_allocated, indices, size * sizeof(CeedInt));
             impl->h_ind = impl->h_ind_allocated;
-          }
-          break;
-      }
-      if (indices != NULL) {
+            break;
+        }
         CeedCallHip(ceed, hipMalloc((void **)&impl->d_ind, size * sizeof(CeedInt)));
         impl->d_ind_allocated = impl->d_ind;  // We own the device memory
         CeedCallHip(ceed, hipMemcpy(impl->d_ind, indices, size * sizeof(CeedInt), hipMemcpyHostToDevice));
-        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Hip(r, indices));
-      }
-      break;
-    }
-    case CEED_MEM_DEVICE: {
-      switch (copy_mode) {
-        case CEED_COPY_VALUES:
-          if (indices != NULL) {
+        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Hip(rstr, indices));
+      } break;
+      case CEED_MEM_DEVICE: {
+        switch (copy_mode) {
+          case CEED_COPY_VALUES:
             CeedCallHip(ceed, hipMalloc((void **)&impl->d_ind, size * sizeof(CeedInt)));
             impl->d_ind_allocated = impl->d_ind;  // We own the device memory
             CeedCallHip(ceed, hipMemcpy(impl->d_ind, indices, size * sizeof(CeedInt), hipMemcpyDeviceToDevice));
+            break;
+          case CEED_OWN_POINTER:
+            impl->d_ind           = (CeedInt *)indices;
+            impl->d_ind_allocated = impl->d_ind;
+            break;
+          case CEED_USE_POINTER:
+            impl->d_ind = (CeedInt *)indices;
+            break;
+        }
+        CeedCallBackend(CeedMalloc(size, &impl->h_ind_allocated));
+        CeedCallHip(ceed, hipMemcpy(impl->h_ind_allocated, impl->d_ind, size * sizeof(CeedInt), hipMemcpyDeviceToHost));
+        impl->h_ind = impl->h_ind_allocated;
+        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Hip(rstr, indices));
+      } break;
+    }
+
+    // Orientation data
+    if (rstr_type == CEED_RESTRICTION_ORIENTED) {
+      switch (mem_type) {
+        case CEED_MEM_HOST: {
+          switch (copy_mode) {
+            case CEED_OWN_POINTER:
+              impl->h_orients_allocated = (bool *)orients;
+              impl->h_orients           = (bool *)orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->h_orients = (bool *)orients;
+              break;
+            case CEED_COPY_VALUES:
+              CeedCallBackend(CeedMalloc(size, &impl->h_orients_allocated));
+              memcpy(impl->h_orients_allocated, orients, size * sizeof(bool));
+              impl->h_orients = impl->h_orients_allocated;
+              break;
           }
-          break;
-        case CEED_OWN_POINTER:
-          impl->d_ind           = (CeedInt *)indices;
-          impl->d_ind_allocated = impl->d_ind;
-          break;
-        case CEED_USE_POINTER:
-          impl->d_ind = (CeedInt *)indices;
+          CeedCallHip(ceed, hipMalloc((void **)&impl->d_orients, size * sizeof(bool)));
+          impl->d_orients_allocated = impl->d_orients;  // We own the device memory
+          CeedCallHip(ceed, hipMemcpy(impl->d_orients, orients, size * sizeof(bool), hipMemcpyHostToDevice));
+        } break;
+        case CEED_MEM_DEVICE: {
+          switch (copy_mode) {
+            case CEED_COPY_VALUES:
+              CeedCallHip(ceed, hipMalloc((void **)&impl->d_orients, size * sizeof(bool)));
+              impl->d_orients_allocated = impl->d_orients;  // We own the device memory
+              CeedCallHip(ceed, hipMemcpy(impl->d_orients, orients, size * sizeof(bool), hipMemcpyDeviceToDevice));
+              break;
+            case CEED_OWN_POINTER:
+              impl->d_orients           = (bool *)orients;
+              impl->d_orients_allocated = impl->d_orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->d_orients = (bool *)orients;
+              break;
+          }
+          CeedCallBackend(CeedMalloc(size, &impl->h_orients_allocated));
+          CeedCallHip(ceed, hipMemcpy(impl->h_orients_allocated, impl->d_orients, size * sizeof(bool), hipMemcpyDeviceToHost));
+          impl->h_orients = impl->h_orients_allocated;
+        } break;
       }
-      if (indices != NULL) {
-        CeedCallBackend(CeedMalloc(elem_size * num_elem, &impl->h_ind_allocated));
-        CeedCallHip(ceed, hipMemcpy(impl->h_ind_allocated, impl->d_ind, elem_size * num_elem * sizeof(CeedInt), hipMemcpyDeviceToHost));
-        impl->h_ind = impl->h_ind_allocated;
-        if (is_deterministic) CeedCallBackend(CeedElemRestrictionOffset_Hip(r, indices));
+    } else if (rstr_type == CEED_RESTRICTION_CURL_ORIENTED) {
+      switch (mem_type) {
+        case CEED_MEM_HOST: {
+          switch (copy_mode) {
+            case CEED_OWN_POINTER:
+              impl->h_curl_orients_allocated = (CeedInt8 *)curl_orients;
+              impl->h_curl_orients           = (CeedInt8 *)curl_orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->h_curl_orients = (CeedInt8 *)curl_orients;
+              break;
+            case CEED_COPY_VALUES:
+              CeedCallBackend(CeedMalloc(3 * size, &impl->h_curl_orients_allocated));
+              memcpy(impl->h_curl_orients_allocated, curl_orients, 3 * size * sizeof(CeedInt8));
+              impl->h_curl_orients = impl->h_curl_orients_allocated;
+              break;
+          }
+          CeedCallHip(ceed, hipMalloc((void **)&impl->d_curl_orients, 3 * size * sizeof(CeedInt8)));
+          impl->d_curl_orients_allocated = impl->d_curl_orients;  // We own the device memory
+          CeedCallHip(ceed, hipMemcpy(impl->d_curl_orients, curl_orients, 3 * size * sizeof(CeedInt8), hipMemcpyHostToDevice));
+        } break;
+        case CEED_MEM_DEVICE: {
+          switch (copy_mode) {
+            case CEED_COPY_VALUES:
+              CeedCallHip(ceed, hipMalloc((void **)&impl->d_curl_orients, 3 * size * sizeof(CeedInt8)));
+              impl->d_curl_orients_allocated = impl->d_curl_orients;  // We own the device memory
+              CeedCallHip(ceed, hipMemcpy(impl->d_curl_orients, curl_orients, 3 * size * sizeof(CeedInt8), hipMemcpyDeviceToDevice));
+              break;
+            case CEED_OWN_POINTER:
+              impl->d_curl_orients           = (CeedInt8 *)curl_orients;
+              impl->d_curl_orients_allocated = impl->d_curl_orients;
+              break;
+            case CEED_USE_POINTER:
+              impl->d_curl_orients = (CeedInt8 *)curl_orients;
+              break;
+          }
+          CeedCallBackend(CeedMalloc(3 * size, &impl->h_curl_orients_allocated));
+          CeedCallHip(ceed, hipMemcpy(impl->h_curl_orients_allocated, impl->d_curl_orients, 3 * size * sizeof(CeedInt8), hipMemcpyDeviceToHost));
+          impl->h_curl_orients = impl->h_curl_orients_allocated;
+        } break;
       }
-      break;
     }
-    // LCOV_EXCL_START
-    default:
-      return CeedError(ceed, CEED_ERROR_BACKEND, "Only MemType = HOST or DEVICE supported");
-      // LCOV_EXCL_STOP
   }
 
   // Compile HIP kernels
-  CeedInt num_nodes = impl->num_nodes;
-
   CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/hip/hip-ref-restriction.h", &restriction_kernel_path));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Restriction Kernel Source -----\n");
   CeedCallBackend(CeedLoadSourceToBuffer(ceed, restriction_kernel_path, &restriction_kernel_source));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Restriction Kernel Source Complete! -----\n");
-  CeedCallBackend(CeedCompile_Hip(ceed, restriction_kernel_source, &impl->module, 8, "RESTR_ELEM_SIZE", elem_size, "RESTR_NUM_ELEM", num_elem,
-                                  "RESTR_NUM_COMP", num_comp, "RESTR_NUM_NODES", num_nodes, "RESTR_COMP_STRIDE", comp_stride, "RESTR_STRIDE_NODES",
-                                  strides[0], "RESTR_STRIDE_COMP", strides[1], "RESTR_STRIDE_ELEM", strides[2]));
+  CeedCallBackend(CeedCompile_Hip(ceed, restriction_kernel_source, &impl->module, 8, "RSTR_ELEM_SIZE", elem_size, "RSTR_NUM_ELEM", num_elem,
+                                  "RSTR_NUM_COMP", num_comp, "RSTR_NUM_NODES", impl->num_nodes, "RSTR_COMP_STRIDE", comp_stride, "RSTR_STRIDE_NODES",
+                                  strides[0], "RSTR_STRIDE_COMP", strides[1], "RSTR_STRIDE_ELEM", strides[2]));
   CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "StridedNoTranspose", &impl->StridedNoTranspose));
   CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "StridedTranspose", &impl->StridedTranspose));
   CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "OffsetNoTranspose", &impl->OffsetNoTranspose));
@@ -339,15 +560,23 @@ int CeedElemRestrictionCreate_Hip(CeedMemType mem_type, CeedCopyMode copy_mode,
   } else {
     CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "OffsetTransposeDet", &impl->OffsetTransposeDet));
   }
+  CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "OrientedNoTranspose", &impl->OrientedNoTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "OrientedTranspose", &impl->OrientedTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "CurlOrientedNoTranspose", &impl->CurlOrientedNoTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "CurlOrientedUnsignedNoTranspose", &impl->CurlOrientedUnsignedNoTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "CurlOrientedTranspose", &impl->CurlOrientedTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, impl->module, "CurlOrientedUnsignedTranspose", &impl->CurlOrientedUnsignedTranspose));
   CeedCallBackend(CeedFree(&restriction_kernel_path));
   CeedCallBackend(CeedFree(&restriction_kernel_source));
 
   // Register backend functions
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "Apply", CeedElemRestrictionApply_Hip));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "ApplyUnsigned", CeedElemRestrictionApply_Hip));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "ApplyUnoriented", CeedElemRestrictionApply_Hip));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "GetOffsets", CeedElemRestrictionGetOffsets_Hip));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", r, "Destroy", CeedElemRestrictionDestroy_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Apply", CeedElemRestrictionApply_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnsigned", CeedElemRestrictionApplyUnsigned_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnoriented", CeedElemRestrictionApplyUnoriented_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOffsets", CeedElemRestrictionGetOffsets_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOrientations", CeedElemRestrictionGetOrientations_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetCurlOrientations", CeedElemRestrictionGetCurlOrientations_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Destroy", CeedElemRestrictionDestroy_Hip));
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/backends/hip-ref/ceed-hip-ref-vector.c b/backends/hip-ref/ceed-hip-ref-vector.c
index 32bf1a30..5b110113 100644
--- a/backends/hip-ref/ceed-hip-ref-vector.c
+++ b/backends/hip-ref/ceed-hip-ref-vector.c
@@ -41,16 +41,16 @@ static inline int CeedVectorNeedSync_Hip(const CeedVector vec, CeedMemType mem_t
 static inline int CeedVectorSyncH2D_Hip(const CeedVector vec) {
   Ceed            ceed;
   CeedSize        length;
+  size_t          bytes;
   CeedVector_Hip *impl;
 
   CeedCallBackend(CeedVectorGetCeed(vec, &ceed));
   CeedCallBackend(CeedVectorGetData(vec, &impl));
 
-  CeedCallBackend(CeedVectorGetLength(vec, &length));
-  size_t bytes = length * sizeof(CeedScalar);
-
   CeedCheck(impl->h_array, ceed, CEED_ERROR_BACKEND, "No valid host data to sync to device");
 
+  CeedCallBackend(CeedVectorGetLength(vec, &length));
+  bytes = length * sizeof(CeedScalar);
   if (impl->d_array_borrowed) {
     impl->d_array = impl->d_array_borrowed;
   } else if (impl->d_array_owned) {
@@ -69,6 +69,7 @@ static inline int CeedVectorSyncH2D_Hip(const CeedVector vec) {
 static inline int CeedVectorSyncD2H_Hip(const CeedVector vec) {
   Ceed            ceed;
   CeedSize        length;
+  size_t          bytes;
   CeedVector_Hip *impl;
 
   CeedCallBackend(CeedVectorGetCeed(vec, &ceed));
@@ -82,14 +83,14 @@ static inline int CeedVectorSyncD2H_Hip(const CeedVector vec) {
     impl->h_array = impl->h_array_owned;
   } else {
     CeedSize length;
+
     CeedCallBackend(CeedVectorGetLength(vec, &length));
     CeedCallBackend(CeedCalloc(length, &impl->h_array_owned));
     impl->h_array = impl->h_array_owned;
   }
 
   CeedCallBackend(CeedVectorGetLength(vec, &length));
-  size_t bytes = length * sizeof(CeedScalar);
-
+  bytes = length * sizeof(CeedScalar);
   CeedCallHip(ceed, hipMemcpy(impl->h_array, impl->d_array, bytes, hipMemcpyDeviceToHost));
   return CEED_ERROR_SUCCESS;
 }
@@ -191,9 +192,10 @@ static int CeedVectorSetArrayHost_Hip(const CeedVector vec, const CeedCopyMode c
       impl->h_array          = impl->h_array_owned;
       if (array) {
         CeedSize length;
+        size_t   bytes;
 
         CeedCallBackend(CeedVectorGetLength(vec, &length));
-        size_t bytes = length * sizeof(CeedScalar);
+        bytes = length * sizeof(CeedScalar);
         memcpy(impl->h_array, array, bytes);
       }
     } break;
@@ -224,10 +226,10 @@ static int CeedVectorSetArrayDevice_Hip(const CeedVector vec, const CeedCopyMode
   switch (copy_mode) {
     case CEED_COPY_VALUES: {
       CeedSize length;
+      size_t   bytes;
 
       CeedCallBackend(CeedVectorGetLength(vec, &length));
-      size_t bytes = length * sizeof(CeedScalar);
-
+      bytes = length * sizeof(CeedScalar);
       if (!impl->d_array_owned) {
         CeedCallHip(ceed, hipMalloc((void **)&impl->d_array_owned, bytes));
       }
@@ -434,7 +436,7 @@ static int CeedVectorGetArrayWrite_Hip(const CeedVector vec, const CeedMemType m
 //------------------------------------------------------------------------------
 static int CeedVectorNorm_Hip(CeedVector vec, CeedNormType type, CeedScalar *norm) {
   Ceed              ceed;
-  CeedSize          length;
+  CeedSize          length, num_calls;
   const CeedScalar *d_array;
   CeedVector_Hip   *impl;
   hipblasHandle_t   handle;
@@ -446,8 +448,7 @@ static int CeedVectorNorm_Hip(CeedVector vec, CeedNormType type, CeedScalar *nor
 
   // Is the vector too long to handle with int32? If so, we will divide
   // it up into "int32-sized" subsections and make repeated BLAS calls.
-  CeedSize num_calls = length / INT_MAX;
-
+  num_calls = length / INT_MAX;
   if (length % INT_MAX > 0) num_calls += 1;
 
   // Compute norm
diff --git a/backends/hip-ref/ceed-hip-ref.h b/backends/hip-ref/ceed-hip-ref.h
index 634bb68d..fbc6e55a 100644
--- a/backends/hip-ref/ceed-hip-ref.h
+++ b/backends/hip-ref/ceed-hip-ref.h
@@ -34,6 +34,12 @@ typedef struct {
   hipFunction_t OffsetNoTranspose;
   hipFunction_t OffsetTranspose;
   hipFunction_t OffsetTransposeDet;
+  hipFunction_t OrientedNoTranspose;
+  hipFunction_t OrientedTranspose;
+  hipFunction_t CurlOrientedNoTranspose;
+  hipFunction_t CurlOrientedTranspose;
+  hipFunction_t CurlOrientedUnsignedNoTranspose;
+  hipFunction_t CurlOrientedUnsignedTranspose;
   CeedInt       num_nodes;
   CeedInt      *h_ind;
   CeedInt      *h_ind_allocated;
@@ -42,6 +48,14 @@ typedef struct {
   CeedInt      *d_t_offsets;
   CeedInt      *d_t_indices;
   CeedInt      *d_l_vec_indices;
+  bool         *h_orients;
+  bool         *h_orients_allocated;
+  bool         *d_orients;
+  bool         *d_orients_allocated;
+  CeedInt8     *h_curl_orients;
+  CeedInt8     *h_curl_orients_allocated;
+  CeedInt8     *d_curl_orients;
+  CeedInt8     *d_curl_orients_allocated;
 } CeedElemRestriction_Hip;
 
 typedef struct {
@@ -84,21 +98,19 @@ typedef struct {
 
 typedef struct {
   hipModule_t         module;
-  hipFunction_t       linearDiagonal;
-  hipFunction_t       linearPointBlock;
-  CeedBasis           basis_in, basis_out;
+  hipFunction_t       LinearDiagonal;
+  hipFunction_t       LinearPointBlock;
   CeedElemRestriction diag_rstr, point_block_diag_rstr;
   CeedVector          elem_diag, point_block_elem_diag;
-  CeedInt             num_e_mode_in, num_e_mode_out, num_modes;
-  CeedEvalMode       *h_e_mode_in, *h_e_mode_out;
-  CeedEvalMode       *d_e_mode_in, *d_e_mode_out;
-  CeedScalar         *d_identity, *d_interp_in, *d_interp_out, *d_grad_in, *d_grad_out;
+  CeedEvalMode       *d_eval_modes_in, *d_eval_modes_out;
+  CeedScalar         *d_identity, *d_interp_in, *d_grad_in, *d_div_in, *d_curl_in;
+  CeedScalar         *d_interp_out, *d_grad_out, *d_div_out, *d_curl_out;
 } CeedOperatorDiag_Hip;
 
 typedef struct {
   hipModule_t   module;
-  hipFunction_t linearAssemble;
-  CeedInt       num_elem, block_size_x, block_size_y, elem_per_block;
+  hipFunction_t LinearAssemble;
+  CeedInt       block_size_x, block_size_y, elem_per_block;
   CeedScalar   *d_B_in, *d_B_out;
 } CeedOperatorAssemble_Hip;
 
diff --git a/backends/hip-shared/ceed-hip-shared-basis.c b/backends/hip-shared/ceed-hip-shared-basis.c
index 5e6218f7..b9c1cef5 100644
--- a/backends/hip-shared/ceed-hip-shared-basis.c
+++ b/backends/hip-shared/ceed-hip-shared-basis.c
@@ -81,7 +81,6 @@ static int ComputeBasisThreadBlockSizes(const CeedInt dim, const CeedInt P_1d, c
       block_sizes[2] = CeedIntMax(256, ComputeBlockSizeFromRequirement(required));
     }
   }
-
   return CEED_ERROR_SUCCESS;
 }
 
@@ -276,8 +275,8 @@ int CeedBasisCreateTensorH1_Hip_shared(CeedInt dim, CeedInt P_1d, CeedInt Q_1d,
   Ceed                  ceed;
   char                 *basis_kernel_path, *basis_kernel_source;
   CeedInt               num_comp;
-  const CeedInt         q_bytes = Q_1d * sizeof(CeedScalar);
-  const CeedInt         i_bytes = q_bytes * P_1d;
+  const CeedInt         q_bytes      = Q_1d * sizeof(CeedScalar);
+  const CeedInt         interp_bytes = q_bytes * P_1d;
   CeedBasis_Hip_shared *data;
 
   CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
@@ -286,10 +285,10 @@ int CeedBasisCreateTensorH1_Hip_shared(CeedInt dim, CeedInt P_1d, CeedInt Q_1d,
   // Copy basis data to GPU
   CeedCallHip(ceed, hipMalloc((void **)&data->d_q_weight_1d, q_bytes));
   CeedCallHip(ceed, hipMemcpy(data->d_q_weight_1d, q_weight_1d, q_bytes, hipMemcpyHostToDevice));
-  CeedCallHip(ceed, hipMalloc((void **)&data->d_interp_1d, i_bytes));
-  CeedCallHip(ceed, hipMemcpy(data->d_interp_1d, interp_1d, i_bytes, hipMemcpyHostToDevice));
-  CeedCallHip(ceed, hipMalloc((void **)&data->d_grad_1d, i_bytes));
-  CeedCallHip(ceed, hipMemcpy(data->d_grad_1d, grad_1d, i_bytes, hipMemcpyHostToDevice));
+  CeedCallHip(ceed, hipMalloc((void **)&data->d_interp_1d, interp_bytes));
+  CeedCallHip(ceed, hipMemcpy(data->d_interp_1d, interp_1d, interp_bytes, hipMemcpyHostToDevice));
+  CeedCallHip(ceed, hipMalloc((void **)&data->d_grad_1d, interp_bytes));
+  CeedCallHip(ceed, hipMemcpy(data->d_grad_1d, grad_1d, interp_bytes, hipMemcpyHostToDevice));
 
   // Compute collocated gradient and copy to GPU
   data->d_collo_grad_1d    = NULL;
diff --git a/backends/occa/ceed-occa-basis.hpp b/backends/occa/ceed-occa-basis.hpp
index d820b389..28217bfc 100644
--- a/backends/occa/ceed-occa-basis.hpp
+++ b/backends/occa/ceed-occa-basis.hpp
@@ -27,21 +27,21 @@ class Basis : public CeedObject {
 
   virtual ~Basis();
 
-  static Basis* getBasis(CeedBasis basis, const bool assertValid = true);
+  static Basis *getBasis(CeedBasis basis, const bool assertValid = true);
 
-  static Basis* from(CeedBasis basis);
-  static Basis* from(CeedOperatorField operatorField);
+  static Basis *from(CeedBasis basis);
+  static Basis *from(CeedOperatorField operatorField);
 
   int setCeedFields(CeedBasis basis);
 
   virtual bool isTensorBasis() const = 0;
 
-  virtual const char* getFunctionSource() const = 0;
+  virtual const char *getFunctionSource() const = 0;
 
-  virtual int apply(const CeedInt elementCount, CeedTransposeMode tmode, CeedEvalMode emode, Vector* u, Vector* v) = 0;
+  virtual int apply(const CeedInt elementCount, CeedTransposeMode tmode, CeedEvalMode emode, Vector *u, Vector *v) = 0;
 
   //---[ Ceed Callbacks ]-----------
-  static int registerCeedFunction(Ceed ceed, CeedBasis basis, const char* fname, ceed::occa::ceedFunction f);
+  static int registerCeedFunction(Ceed ceed, CeedBasis basis, const char *fname, ceed::occa::ceedFunction f);
 
   static int ceedApply(CeedBasis basis, const CeedInt nelem, CeedTransposeMode tmode, CeedEvalMode emode, CeedVector u, CeedVector v);
 
diff --git a/backends/occa/ceed-occa-context.cpp b/backends/occa/ceed-occa-context.cpp
index f8533563..81781601 100644
--- a/backends/occa/ceed-occa-context.cpp
+++ b/backends/occa/ceed-occa-context.cpp
@@ -15,13 +15,13 @@ Context::Context(::occa::device device_) : device(device_) {
   _usingGpuDevice        = (mode == "CUDA" || mode == "HIP" || mode == "OpenCL");
 }
 
-Context* Context::from(Ceed ceed) {
+Context *Context::from(Ceed ceed) {
   if (!ceed) {
     return NULL;
   }
 
-  Context* context;
-  CeedGetData(ceed, (void**)&context);
+  Context *context;
+  CeedGetData(ceed, (void **)&context);
   return context;
 }
 
diff --git a/backends/occa/ceed-occa-context.hpp b/backends/occa/ceed-occa-context.hpp
index 4e580cba..90600b0d 100644
--- a/backends/occa/ceed-occa-context.hpp
+++ b/backends/occa/ceed-occa-context.hpp
@@ -22,7 +22,7 @@ class Context {
 
   Context(::occa::device device_);
 
-  static Context* from(Ceed ceed);
+  static Context *from(Ceed ceed);
 
   bool usingCpuDevice() const;
   bool usingGpuDevice() const;
diff --git a/backends/occa/ceed-occa-operator-args.cpp b/backends/occa/ceed-occa-operator-args.cpp
index b36e4cf7..c1bd9919 100644
--- a/backends/occa/ceed-occa-operator-args.cpp
+++ b/backends/occa/ceed-occa-operator-args.cpp
@@ -39,10 +39,10 @@ void OperatorArgs::setupArgs(CeedOperator op) {
   }
 }
 
-const OperatorField& OperatorArgs::getOpField(const bool isInput, const int index) const { return isInput ? opInputs[index] : opOutputs[index]; }
+const OperatorField &OperatorArgs::getOpField(const bool isInput, const int index) const { return isInput ? opInputs[index] : opOutputs[index]; }
 
-const OperatorField& OperatorArgs::getOpInput(const int index) const { return opInputs[index]; }
+const OperatorField &OperatorArgs::getOpInput(const int index) const { return opInputs[index]; }
 
-const OperatorField& OperatorArgs::getOpOutput(const int index) const { return opOutputs[index]; }
+const OperatorField &OperatorArgs::getOpOutput(const int index) const { return opOutputs[index]; }
 }  // namespace occa
 }  // namespace ceed
diff --git a/backends/occa/ceed-occa-operator-args.hpp b/backends/occa/ceed-occa-operator-args.hpp
index 78a33bbc..19158e43 100644
--- a/backends/occa/ceed-occa-operator-args.hpp
+++ b/backends/occa/ceed-occa-operator-args.hpp
@@ -28,11 +28,11 @@ class OperatorArgs : public QFunctionArgs {
 
   void setupArgs(CeedOperator op);
 
-  const OperatorField& getOpField(const bool isInput, const int index) const;
+  const OperatorField &getOpField(const bool isInput, const int index) const;
 
-  const OperatorField& getOpInput(const int index) const;
+  const OperatorField &getOpInput(const int index) const;
 
-  const OperatorField& getOpOutput(const int index) const;
+  const OperatorField &getOpOutput(const int index) const;
 };
 }  // namespace occa
 }  // namespace ceed
diff --git a/backends/occa/ceed-occa-qfunction-args.cpp b/backends/occa/ceed-occa-qfunction-args.cpp
index 5f4acbbc..2368dbb2 100644
--- a/backends/occa/ceed-occa-qfunction-args.cpp
+++ b/backends/occa/ceed-occa-qfunction-args.cpp
@@ -43,11 +43,11 @@ int QFunctionArgs::inputCount() const { return _inputCount; }
 
 int QFunctionArgs::outputCount() const { return _outputCount; }
 
-const QFunctionField& QFunctionArgs::getQfField(const bool isInput, const int index) const { return isInput ? qfInputs[index] : qfOutputs[index]; }
+const QFunctionField &QFunctionArgs::getQfField(const bool isInput, const int index) const { return isInput ? qfInputs[index] : qfOutputs[index]; }
 
-const QFunctionField& QFunctionArgs::getQfInput(const int index) const { return qfInputs[index]; }
+const QFunctionField &QFunctionArgs::getQfInput(const int index) const { return qfInputs[index]; }
 
-const QFunctionField& QFunctionArgs::getQfOutput(const int index) const { return qfOutputs[index]; }
+const QFunctionField &QFunctionArgs::getQfOutput(const int index) const { return qfOutputs[index]; }
 
 CeedEvalMode QFunctionArgs::getEvalMode(const bool isInput, const int index) const {
   return isInput ? qfInputs[index].evalMode : qfOutputs[index].evalMode;
diff --git a/backends/occa/ceed-occa-qfunction-args.hpp b/backends/occa/ceed-occa-qfunction-args.hpp
index 95131c98..9f46cc58 100644
--- a/backends/occa/ceed-occa-qfunction-args.hpp
+++ b/backends/occa/ceed-occa-qfunction-args.hpp
@@ -37,11 +37,11 @@ class QFunctionArgs : public CeedObject {
   int inputCount() const;
   int outputCount() const;
 
-  const QFunctionField& getQfField(const bool isInput, const int index) const;
+  const QFunctionField &getQfField(const bool isInput, const int index) const;
 
-  const QFunctionField& getQfInput(const int index) const;
+  const QFunctionField &getQfInput(const int index) const;
 
-  const QFunctionField& getQfOutput(const int index) const;
+  const QFunctionField &getQfOutput(const int index) const;
 
   CeedEvalMode getEvalMode(const bool isInput, const int index) const;
 
diff --git a/backends/occa/ceed-occa-types.hpp b/backends/occa/ceed-occa-types.hpp
index 121a8b12..4fac4d86 100644
--- a/backends/occa/ceed-occa-types.hpp
+++ b/backends/occa/ceed-occa-types.hpp
@@ -45,7 +45,7 @@
     return CeedError(ceed, CEED_ERROR_BACKEND, error.c_str()); \
   } while (0)
 
-#define CeedOccaCastRegisterFunction(func) (ceed::occa::ceedFunction)(void*) func
+#define CeedOccaCastRegisterFunction(func) (ceed::occa::ceedFunction)(void *) func
 
 #define CeedOccaRegisterBaseFunction(name, func) CeedCallBackend(registerCeedFunction(ceed, name, CeedOccaCastRegisterFunction(func)));
 
diff --git a/backends/occa/kernels/set-value.cpp b/backends/occa/kernels/set-value.cpp
index dbac90e8..67f433f4 100644
--- a/backends/occa/kernels/set-value.cpp
+++ b/backends/occa/kernels/set-value.cpp
@@ -14,9 +14,9 @@
 // Expects the following constants to be defined:
 // - BLOCK_SIZE : CeedInt
 
-const char* occa_set_value_source = STRINGIFY_SOURCE(
+const char *occa_set_value_source = STRINGIFY_SOURCE(
 
-    @kernel void setValue(CeedScalar* ptr, const CeedScalar value, const CeedInt count) {
+    @kernel void setValue(CeedScalar *ptr, const CeedScalar value, const CeedInt count) {
       @tile(BLOCK_SIZE, @outer, @inner) for (CeedInt i = 0; i < count; ++i) {
         ptr[i] = value;
       }
diff --git a/backends/opt/ceed-opt-operator.c b/backends/opt/ceed-opt-operator.c
index 5b38087a..95d941f9 100644
--- a/backends/opt/ceed-opt-operator.c
+++ b/backends/opt/ceed-opt-operator.c
@@ -461,8 +461,8 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Opt(CeedOperator op, b
   // Setup
   CeedCallBackend(CeedOperatorSetup_Opt(op));
 
-  // Check for identity
-  CeedCheck(!impl->is_identity_qf, ceed, CEED_ERROR_BACKEND, "Assembling identity QFunctions not supported");
+  // Check for restriction only operator
+  CeedCheck(!impl->is_identity_rstr_op, ceed, CEED_ERROR_BACKEND, "Assembling restriction only operators is not supported");
 
   // Input Evecs and Restriction
   CeedCallBackend(CeedOperatorSetupInputs_Opt(num_input_fields, qf_input_fields, op_input_fields, NULL, e_data, impl, request));
@@ -541,7 +541,6 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Opt(CeedOperator op, b
     CeedCallBackend(CeedElemRestrictionCreateStrided(ceed, num_elem, Q, num_active_in * num_active_out, num_active_in * num_active_out * num_elem * Q,
                                                      strides, rstr));
     // Create assembled vector
-
     CeedCallBackend(CeedVectorCreate(ceed, l_size, assembled));
   }
 
@@ -561,21 +560,46 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Opt(CeedOperator op, b
       if (num_active_in > 1) {
         CeedCallBackend(CeedVectorSetValue(active_in[(in + num_active_in - 1) % num_active_in], 0.0));
       }
-      // Set Outputs
+      if (!impl->is_identity_qf) {
+        // Set Outputs
+        for (CeedInt out = 0; out < num_output_fields; out++) {
+          CeedVector vec;
+
+          // Get output vector
+          CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
+          // Check if active output
+          if (vec == CEED_VECTOR_ACTIVE) {
+            CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[out], CEED_MEM_HOST, CEED_USE_POINTER, l_vec_array));
+            CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[out], &size));
+            l_vec_array += size * Q * block_size;  // Advance the pointer by the size of the output
+          }
+        }
+        // Apply QFunction
+        CeedCallBackend(CeedQFunctionApply(qf, Q * block_size, impl->q_vecs_in, impl->q_vecs_out));
+      } else {
+        const CeedScalar *q_vec_array;
+
+        // Copy Identity Outputs
+        CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[0], &size));
+        CeedCallBackend(CeedVectorGetArrayRead(impl->q_vecs_out[0], CEED_MEM_HOST, &q_vec_array));
+        for (CeedInt i = 0; i < size * Q * block_size; i++) l_vec_array[i] = q_vec_array[i];
+        CeedCallBackend(CeedVectorRestoreArrayRead(impl->q_vecs_out[0], &q_vec_array));
+        l_vec_array += size * Q * block_size;
+      }
+    }
+
+    // Assemble QFunction
+    if (!impl->is_identity_qf) {
       for (CeedInt out = 0; out < num_output_fields; out++) {
         CeedVector vec;
 
         // Get output vector
         CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
         // Check if active output
-        if (vec == CEED_VECTOR_ACTIVE) {
-          CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[out], CEED_MEM_HOST, CEED_USE_POINTER, l_vec_array));
-          CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[out], &size));
-          l_vec_array += size * Q * block_size;  // Advance the pointer by the size of the output
+        if (vec == CEED_VECTOR_ACTIVE && num_elem > 0) {
+          CeedCallBackend(CeedVectorTakeArray(impl->q_vecs_out[out], CEED_MEM_HOST, NULL));
         }
       }
-      // Apply QFunction
-      CeedCallBackend(CeedQFunctionApply(qf, Q * block_size, impl->q_vecs_in, impl->q_vecs_out));
     }
 
     // Assemble into assembled vector
diff --git a/backends/ref/ceed-ref-operator.c b/backends/ref/ceed-ref-operator.c
index 240c6de7..0b92d6fb 100644
--- a/backends/ref/ceed-ref-operator.c
+++ b/backends/ref/ceed-ref-operator.c
@@ -403,8 +403,8 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Ref(CeedOperator op, b
   // Setup
   CeedCallBackend(CeedOperatorSetup_Ref(op));
 
-  // Check for identity
-  CeedCheck(!impl->is_identity_qf, ceed, CEED_ERROR_BACKEND, "Assembling identity QFunctions not supported");
+  // Check for restriction only operator
+  CeedCheck(!impl->is_identity_rstr_op, ceed, CEED_ERROR_BACKEND, "Assembling restriction only operators is not supported");
 
   // Input Evecs and Restriction
   CeedCallBackend(CeedOperatorSetupInputs_Ref(num_input_fields, qf_input_fields, op_input_fields, NULL, true, e_data_full, impl, request));
@@ -482,33 +482,46 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Ref(CeedOperator op, b
       if (num_active_in > 1) {
         CeedCallBackend(CeedVectorSetValue(active_in[(in + num_active_in - 1) % num_active_in], 0.0));
       }
-      // Set Outputs
-      for (CeedInt out = 0; out < num_output_fields; out++) {
-        CeedVector vec;
-
-        // Get output vector
-        CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
-        // Check if active output
-        if (vec == CEED_VECTOR_ACTIVE) {
-          CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[out], CEED_MEM_HOST, CEED_USE_POINTER, assembled_array));
-          CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[out], &size));
-          assembled_array += size * Q;  // Advance the pointer by the size of the output
+      if (!impl->is_identity_qf) {
+        // Set Outputs
+        for (CeedInt out = 0; out < num_output_fields; out++) {
+          CeedVector vec;
+
+          // Get output vector
+          CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
+          // Check if active output
+          if (vec == CEED_VECTOR_ACTIVE) {
+            CeedCallBackend(CeedVectorSetArray(impl->q_vecs_out[out], CEED_MEM_HOST, CEED_USE_POINTER, assembled_array));
+            CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[out], &size));
+            assembled_array += size * Q;  // Advance the pointer by the size of the output
+          }
         }
+        // Apply QFunction
+        CeedCallBackend(CeedQFunctionApply(qf, Q, impl->q_vecs_in, impl->q_vecs_out));
+      } else {
+        const CeedScalar *q_vec_array;
+
+        // Copy Identity Outputs
+        CeedCallBackend(CeedQFunctionFieldGetSize(qf_output_fields[0], &size));
+        CeedCallBackend(CeedVectorGetArrayRead(impl->q_vecs_out[0], CEED_MEM_HOST, &q_vec_array));
+        for (CeedInt i = 0; i < size * Q; i++) assembled_array[i] = q_vec_array[i];
+        CeedCallBackend(CeedVectorRestoreArrayRead(impl->q_vecs_out[0], &q_vec_array));
+        assembled_array += size * Q;
       }
-      // Apply QFunction
-      CeedCallBackend(CeedQFunctionApply(qf, Q, impl->q_vecs_in, impl->q_vecs_out));
     }
   }
 
   // Un-set output Qvecs to prevent accidental overwrite of Assembled
-  for (CeedInt out = 0; out < num_output_fields; out++) {
-    CeedVector vec;
+  if (!impl->is_identity_qf) {
+    for (CeedInt out = 0; out < num_output_fields; out++) {
+      CeedVector vec;
 
-    // Get output vector
-    CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
-    // Check if active output
-    if (vec == CEED_VECTOR_ACTIVE && num_elem > 0) {
-      CeedCallBackend(CeedVectorTakeArray(impl->q_vecs_out[out], CEED_MEM_HOST, NULL));
+      // Get output vector
+      CeedCallBackend(CeedOperatorFieldGetVector(op_output_fields[out], &vec));
+      // Check if active output
+      if (vec == CEED_VECTOR_ACTIVE && num_elem > 0) {
+        CeedCallBackend(CeedVectorTakeArray(impl->q_vecs_out[out], CEED_MEM_HOST, NULL));
+      }
     }
   }
 
diff --git a/backends/ref/ceed-ref-restriction.c b/backends/ref/ceed-ref-restriction.c
index 4edd0d13..861dc3d7 100644
--- a/backends/ref/ceed-ref-restriction.c
+++ b/backends/ref/ceed-ref-restriction.c
@@ -55,9 +55,9 @@ static inline int CeedElemRestrictionApplyStridedNoTranspose_Ref_Core(CeedElemRe
   return CEED_ERROR_SUCCESS;
 }
 
-static inline int CeedElemRestrictionApplyStandardNoTranspose_Ref_Core(CeedElemRestriction rstr, const CeedInt num_comp, const CeedInt block_size,
-                                                                       const CeedInt comp_stride, CeedInt start, CeedInt stop, CeedInt num_elem,
-                                                                       CeedInt elem_size, CeedInt v_offset, const CeedScalar *uu, CeedScalar *vv) {
+static inline int CeedElemRestrictionApplyOffsetNoTranspose_Ref_Core(CeedElemRestriction rstr, const CeedInt num_comp, const CeedInt block_size,
+                                                                     const CeedInt comp_stride, CeedInt start, CeedInt stop, CeedInt num_elem,
+                                                                     CeedInt elem_size, CeedInt v_offset, const CeedScalar *uu, CeedScalar *vv) {
   // Default restriction with offsets
   CeedElemRestriction_Ref *impl;
 
@@ -211,9 +211,9 @@ static inline int CeedElemRestrictionApplyStridedTranspose_Ref_Core(CeedElemRest
   return CEED_ERROR_SUCCESS;
 }
 
-static inline int CeedElemRestrictionApplyStandardTranspose_Ref_Core(CeedElemRestriction rstr, const CeedInt num_comp, const CeedInt block_size,
-                                                                     const CeedInt comp_stride, CeedInt start, CeedInt stop, CeedInt num_elem,
-                                                                     CeedInt elem_size, CeedInt v_offset, const CeedScalar *uu, CeedScalar *vv) {
+static inline int CeedElemRestrictionApplyOffsetTranspose_Ref_Core(CeedElemRestriction rstr, const CeedInt num_comp, const CeedInt block_size,
+                                                                   const CeedInt comp_stride, CeedInt start, CeedInt stop, CeedInt num_elem,
+                                                                   CeedInt elem_size, CeedInt v_offset, const CeedScalar *uu, CeedScalar *vv) {
   // Default restriction with offsets
   CeedElemRestriction_Ref *impl;
 
@@ -370,16 +370,16 @@ static inline int CeedElemRestrictionApply_Ref_Core(CeedElemRestriction rstr, co
             CeedElemRestrictionApplyStridedTranspose_Ref_Core(rstr, num_comp, block_size, start, stop, num_elem, elem_size, v_offset, uu, vv));
         break;
       case CEED_RESTRICTION_STANDARD:
-        CeedCallBackend(CeedElemRestrictionApplyStandardTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem, elem_size,
-                                                                           v_offset, uu, vv));
+        CeedCallBackend(CeedElemRestrictionApplyOffsetTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem, elem_size,
+                                                                         v_offset, uu, vv));
         break;
       case CEED_RESTRICTION_ORIENTED:
         if (use_signs) {
           CeedCallBackend(CeedElemRestrictionApplyOrientedTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
                                                                              elem_size, v_offset, uu, vv));
         } else {
-          CeedCallBackend(CeedElemRestrictionApplyStandardTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
-                                                                             elem_size, v_offset, uu, vv));
+          CeedCallBackend(CeedElemRestrictionApplyOffsetTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem, elem_size,
+                                                                           v_offset, uu, vv));
         }
         break;
       case CEED_RESTRICTION_CURL_ORIENTED:
@@ -390,8 +390,8 @@ static inline int CeedElemRestrictionApply_Ref_Core(CeedElemRestriction rstr, co
           CeedCallBackend(CeedElemRestrictionApplyCurlOrientedUnsignedTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop,
                                                                                          num_elem, elem_size, v_offset, uu, vv));
         } else {
-          CeedCallBackend(CeedElemRestrictionApplyStandardTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
-                                                                             elem_size, v_offset, uu, vv));
+          CeedCallBackend(CeedElemRestrictionApplyOffsetTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem, elem_size,
+                                                                           v_offset, uu, vv));
         }
         break;
     }
@@ -407,16 +407,16 @@ static inline int CeedElemRestrictionApply_Ref_Core(CeedElemRestriction rstr, co
             CeedElemRestrictionApplyStridedNoTranspose_Ref_Core(rstr, num_comp, block_size, start, stop, num_elem, elem_size, v_offset, uu, vv));
         break;
       case CEED_RESTRICTION_STANDARD:
-        CeedCallBackend(CeedElemRestrictionApplyStandardNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
-                                                                             elem_size, v_offset, uu, vv));
+        CeedCallBackend(CeedElemRestrictionApplyOffsetNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem, elem_size,
+                                                                           v_offset, uu, vv));
         break;
       case CEED_RESTRICTION_ORIENTED:
         if (use_signs) {
           CeedCallBackend(CeedElemRestrictionApplyOrientedNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
                                                                                elem_size, v_offset, uu, vv));
         } else {
-          CeedCallBackend(CeedElemRestrictionApplyStandardNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
-                                                                               elem_size, v_offset, uu, vv));
+          CeedCallBackend(CeedElemRestrictionApplyOffsetNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
+                                                                             elem_size, v_offset, uu, vv));
         }
         break;
       case CEED_RESTRICTION_CURL_ORIENTED:
@@ -427,8 +427,8 @@ static inline int CeedElemRestrictionApply_Ref_Core(CeedElemRestriction rstr, co
           CeedCallBackend(CeedElemRestrictionApplyCurlOrientedUnsignedNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop,
                                                                                            num_elem, elem_size, v_offset, uu, vv));
         } else {
-          CeedCallBackend(CeedElemRestrictionApplyStandardNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
-                                                                               elem_size, v_offset, uu, vv));
+          CeedCallBackend(CeedElemRestrictionApplyOffsetNoTranspose_Ref_Core(rstr, num_comp, block_size, comp_stride, start, stop, num_elem,
+                                                                             elem_size, v_offset, uu, vv));
         }
         break;
     }
@@ -666,7 +666,10 @@ int CeedElemRestrictionCreate_Ref(CeedMemType mem_type, CeedCopyMode copy_mode,
   CeedInt layout[3] = {1, elem_size, elem_size * num_comp};
 
   CeedCheck(mem_type == CEED_MEM_HOST, ceed, CEED_ERROR_BACKEND, "Only MemType = HOST supported");
+
   CeedCallBackend(CeedCalloc(1, &impl));
+  CeedCallBackend(CeedElemRestrictionSetData(rstr, impl));
+  CeedCallBackend(CeedElemRestrictionSetELayout(rstr, layout));
 
   // Offsets data
   CeedCallBackend(CeedElemRestrictionGetType(rstr, &rstr_type));
@@ -735,17 +738,6 @@ int CeedElemRestrictionCreate_Ref(CeedMemType mem_type, CeedCopyMode copy_mode,
     }
   }
 
-  CeedCallBackend(CeedElemRestrictionSetData(rstr, impl));
-  CeedCallBackend(CeedElemRestrictionSetELayout(rstr, layout));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Apply", CeedElemRestrictionApply_Ref));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnsigned", CeedElemRestrictionApplyUnsigned_Ref));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnoriented", CeedElemRestrictionApplyUnoriented_Ref));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyBlock", CeedElemRestrictionApplyBlock_Ref));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOffsets", CeedElemRestrictionGetOffsets_Ref));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOrientations", CeedElemRestrictionGetOrientations_Ref));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetCurlOrientations", CeedElemRestrictionGetCurlOrientations_Ref));
-  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Destroy", CeedElemRestrictionDestroy_Ref));
-
   // Set apply function based upon num_comp, block_size, and comp_stride
   CeedInt index = -1;
 
@@ -795,6 +787,16 @@ int CeedElemRestrictionCreate_Ref(CeedMemType mem_type, CeedCopyMode copy_mode,
       impl->Apply = CeedElemRestrictionApply_Ref_Core;
       break;
   }
+
+  // Register backend functions
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Apply", CeedElemRestrictionApply_Ref));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnsigned", CeedElemRestrictionApplyUnsigned_Ref));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyUnoriented", CeedElemRestrictionApplyUnoriented_Ref));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "ApplyBlock", CeedElemRestrictionApplyBlock_Ref));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOffsets", CeedElemRestrictionGetOffsets_Ref));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetOrientations", CeedElemRestrictionGetOrientations_Ref));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "GetCurlOrientations", CeedElemRestrictionGetCurlOrientations_Ref));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "ElemRestriction", rstr, "Destroy", CeedElemRestrictionDestroy_Ref));
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/backends/sycl-ref/ceed-sycl-ref-basis.sycl.cpp b/backends/sycl-ref/ceed-sycl-ref-basis.sycl.cpp
index 0308b3bd..4721f423 100644
--- a/backends/sycl-ref/ceed-sycl-ref-basis.sycl.cpp
+++ b/backends/sycl-ref/ceed-sycl-ref-basis.sycl.cpp
@@ -524,7 +524,6 @@ static int CeedBasisDestroy_Sycl(CeedBasis basis) {
   CeedCallSycl(ceed, sycl::free(impl->d_grad_1d, data->sycl_context));
 
   CeedCallBackend(CeedFree(&impl));
-
   return CEED_ERROR_SUCCESS;
 }
 
@@ -547,7 +546,6 @@ static int CeedBasisDestroyNonTensor_Sycl(CeedBasis basis) {
   CeedCallSycl(ceed, sycl::free(impl->d_grad, data->sycl_context));
 
   CeedCallBackend(CeedFree(&impl));
-
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/backends/sycl-ref/ceed-sycl-ref-operator.sycl.cpp b/backends/sycl-ref/ceed-sycl-ref-operator.sycl.cpp
index 1a8b9370..78c71681 100644
--- a/backends/sycl-ref/ceed-sycl-ref-operator.sycl.cpp
+++ b/backends/sycl-ref/ceed-sycl-ref-operator.sycl.cpp
@@ -492,15 +492,6 @@ static inline int CeedOperatorLinearAssembleQFunctionCore_Sycl(CeedOperator op,
   // Setup
   CeedCallBackend(CeedOperatorSetup_Sycl(op));
 
-  // Check for identity
-  bool is_identity_qf;
-  CeedCallBackend(CeedQFunctionIsIdentity(qf, &is_identity_qf));
-  if (is_identity_qf) {
-    // LCOV_EXCL_START
-    return CeedError(ceed, CEED_ERROR_BACKEND, "Assembling identity QFunctions not supported");
-    // LCOV_EXCL_STOP
-  }
-
   // Input Evecs and Restriction
   CeedCallBackend(CeedOperatorSetupInputs_Sycl(num_input_fields, qf_input_fields, op_input_fields, NULL, true, e_data, impl, request));
 
@@ -630,45 +621,10 @@ static int CeedOperatorLinearAssembleQFunctionUpdate_Sycl(CeedOperator op, CeedV
   return CeedOperatorLinearAssembleQFunctionCore_Sycl(op, false, &assembled, &rstr, request);
 }
 
-//------------------------------------------------------------------------------
-// Create point block restriction
-//------------------------------------------------------------------------------
-static int CreatePBRestriction(CeedElemRestriction rstr, CeedElemRestriction *point_block_rstr) {
-  Ceed           ceed;
-  CeedSize       l_size;
-  CeedInt        num_elem, num_comp, elem_size, comp_stride, *point_block_offsets;
-  const CeedInt *offsets;
-
-  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetOffsets(rstr, CEED_MEM_HOST, &offsets));
-
-  // Expand offsets
-  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
-  CeedCallBackend(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
-  CeedCallBackend(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
-  CeedInt shift = num_comp;
-
-  if (comp_stride != 1) shift *= num_comp;
-  CeedCallBackend(CeedCalloc(num_elem * elem_size, &point_block_offsets));
-  for (CeedInt i = 0; i < num_elem * elem_size; i++) {
-    point_block_offsets[i] = offsets[i] * shift;
-  }
-
-  // Create new restriction
-  CeedCallBackend(CeedElemRestrictionCreate(ceed, num_elem, elem_size, num_comp * num_comp, 1, l_size * num_comp, CEED_MEM_HOST, CEED_OWN_POINTER,
-                                            point_block_offsets, point_block_rstr));
-
-  // Cleanup
-  CeedCallBackend(CeedElemRestrictionRestoreOffsets(rstr, &offsets));
-  return CEED_ERROR_SUCCESS;
-}
-
 //------------------------------------------------------------------------------
 // Assemble diagonal setup
 //------------------------------------------------------------------------------
-static inline int CeedOperatorAssembleDiagonalSetup_Sycl(CeedOperator op, const bool is_point_block) {
+static inline int CeedOperatorAssembleDiagonalSetup_Sycl(CeedOperator op) {
   Ceed                ceed;
   Ceed_Sycl          *sycl_data;
   CeedInt             num_input_fields, num_output_fields, num_e_mode_in = 0, num_comp = 0, dim = 1, num_e_mode_out = 0;
@@ -949,7 +905,7 @@ static inline int CeedOperatorAssembleDiagonalCore_Sycl(CeedOperator op, CeedVec
 
   // Setup
   if (!impl->diag) {
-    CeedCallBackend(CeedOperatorAssembleDiagonalSetup_Sycl(op, is_point_block));
+    CeedCallBackend(CeedOperatorAssembleDiagonalSetup_Sycl(op));
   }
   CeedOperatorDiag_Sycl *diag = impl->diag;
 
@@ -957,9 +913,7 @@ static inline int CeedOperatorAssembleDiagonalCore_Sycl(CeedOperator op, CeedVec
 
   // Restriction
   if (is_point_block && !diag->point_block_diag_rstr) {
-    CeedElemRestriction point_block_diag_rstr;
-    CeedCallBackend(CreatePBRestriction(diag->diag_rstr, &point_block_diag_rstr));
-    diag->point_block_diag_rstr = point_block_diag_rstr;
+    CeedCallBackend(CeedOperatorCreateActivePointBlockRestriction(diag->diag_rstr, &diag->point_block_diag_rstr));
   }
   CeedElemRestriction diag_rstr = is_point_block ? diag->point_block_diag_rstr : diag->diag_rstr;
 
@@ -979,12 +933,7 @@ static inline int CeedOperatorAssembleDiagonalCore_Sycl(CeedOperator op, CeedVec
   CeedCallBackend(CeedElemRestrictionGetNumElements(diag_rstr, &num_elem));
 
   // Compute the diagonal of B^T D B
-  // Umesh: This needs to be reviewed later
-  // if (is_point_block) {
-  //  CeedCallBackend(CeedOperatorLinearPointBlockDiagonal_Sycl(sycl_data->sycl_queue, num_elem, diag, assembled_qf_array, elem_diag_array));
-  //} else {
   CeedCallBackend(CeedOperatorLinearDiagonal_Sycl(sycl_data->sycl_queue, is_point_block, num_elem, diag, assembled_qf_array, elem_diag_array));
-  // }
 
   // Wait for queue to complete and handle exceptions
   sycl_data->sycl_queue.wait_and_throw();
diff --git a/backends/sycl-ref/ceed-sycl-ref-qfunction-load.sycl.cpp b/backends/sycl-ref/ceed-sycl-ref-qfunction-load.sycl.cpp
index bcdc1a6e..26bf5d9e 100644
--- a/backends/sycl-ref/ceed-sycl-ref-qfunction-load.sycl.cpp
+++ b/backends/sycl-ref/ceed-sycl-ref-qfunction-load.sycl.cpp
@@ -28,13 +28,13 @@
 //------------------------------------------------------------------------------
 extern "C" int CeedQFunctionBuildKernel_Sycl(CeedQFunction qf) {
   Ceed                ceed;
-  Ceed_Sycl*          data;
-  char *              qfunction_name, *qfunction_source, *read_write_kernel_path, *read_write_kernel_source;
+  Ceed_Sycl          *data;
+  char               *qfunction_name, *qfunction_source, *read_write_kernel_path, *read_write_kernel_source;
   CeedInt             num_input_fields, num_output_fields;
   CeedQFunctionField *input_fields, *output_fields;
-  CeedQFunction_Sycl* impl;
+  CeedQFunction_Sycl *impl;
 
-  CeedCallBackend(CeedQFunctionGetData(qf, (void**)&impl));
+  CeedCallBackend(CeedQFunctionGetData(qf, (void **)&impl));
   // QFunction is built
   if (impl->QFunction) return CEED_ERROR_SUCCESS;
 
@@ -45,17 +45,17 @@ extern "C" int CeedQFunctionBuildKernel_Sycl(CeedQFunction qf) {
   CeedCallBackend(CeedQFunctionGetFields(qf, &num_input_fields, &input_fields, &num_output_fields, &output_fields));
 
   std::vector<CeedInt> input_sizes(num_input_fields);
-  CeedQFunctionField*  input_i = input_fields;
+  CeedQFunctionField  *input_i = input_fields;
 
-  for (auto& size_i : input_sizes) {
+  for (auto &size_i : input_sizes) {
     CeedCallBackend(CeedQFunctionFieldGetSize(*input_i, &size_i));
     ++input_i;
   }
 
   std::vector<CeedInt> output_sizes(num_output_fields);
-  CeedQFunctionField*  output_i = output_fields;
+  CeedQFunctionField  *output_i = output_fields;
 
-  for (auto& size_i : output_sizes) {
+  for (auto &size_i : output_sizes) {
     CeedCallBackend(CeedQFunctionFieldGetSize(*output_i, &size_i));
     ++output_i;
   }
diff --git a/backends/sycl-ref/ceed-sycl-restriction.sycl.cpp b/backends/sycl-ref/ceed-sycl-restriction.sycl.cpp
index 4ace9976..8aac0678 100644
--- a/backends/sycl-ref/ceed-sycl-restriction.sycl.cpp
+++ b/backends/sycl-ref/ceed-sycl-restriction.sycl.cpp
@@ -142,15 +142,15 @@ static int CeedElemRestrictionOffsetTranspose_Sycl(sycl::queue &sycl_queue, cons
 //------------------------------------------------------------------------------
 // Apply restriction
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionApply_Sycl(CeedElemRestriction r, CeedTransposeMode t_mode, CeedVector u, CeedVector v, CeedRequest *request) {
+static int CeedElemRestrictionApply_Sycl(CeedElemRestriction rstr, CeedTransposeMode t_mode, CeedVector u, CeedVector v, CeedRequest *request) {
   Ceed                      ceed;
   Ceed_Sycl                *data;
   const CeedScalar         *d_u;
   CeedScalar               *d_v;
   CeedElemRestriction_Sycl *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
   CeedCallBackend(CeedGetData(ceed, &data));
 
   // Get vectors
@@ -197,12 +197,12 @@ static int CeedElemRestrictionApply_Sycl(CeedElemRestriction r, CeedTransposeMod
 //------------------------------------------------------------------------------
 // Get offsets
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionGetOffsets_Sycl(CeedElemRestriction r, CeedMemType m_type, const CeedInt **offsets) {
+static int CeedElemRestrictionGetOffsets_Sycl(CeedElemRestriction rstr, CeedMemType m_type, const CeedInt **offsets) {
   Ceed                      ceed;
   CeedElemRestriction_Sycl *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
 
   switch (m_type) {
     case CEED_MEM_HOST:
@@ -218,13 +218,13 @@ static int CeedElemRestrictionGetOffsets_Sycl(CeedElemRestriction r, CeedMemType
 //------------------------------------------------------------------------------
 // Destroy restriction
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionDestroy_Sycl(CeedElemRestriction r) {
+static int CeedElemRestrictionDestroy_Sycl(CeedElemRestriction rstr) {
   Ceed                      ceed;
   Ceed_Sycl                *data;
   CeedElemRestriction_Sycl *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
   CeedCallBackend(CeedGetData(ceed, &data));
 
   // Wait for all work to finish before freeing memory
@@ -242,7 +242,7 @@ static int CeedElemRestrictionDestroy_Sycl(CeedElemRestriction r) {
 //------------------------------------------------------------------------------
 // Create transpose offsets and indices
 //------------------------------------------------------------------------------
-static int CeedElemRestrictionOffset_Sycl(const CeedElemRestriction r, const CeedInt *indices) {
+static int CeedElemRestrictionOffset_Sycl(const CeedElemRestriction rstr, const CeedInt *indices) {
   Ceed                      ceed;
   Ceed_Sycl                *data;
   bool                     *is_node;
@@ -250,12 +250,12 @@ static int CeedElemRestrictionOffset_Sycl(const CeedElemRestriction r, const Cee
   CeedInt                   num_elem, elem_size, num_comp, num_nodes = 0, *ind_to_offset, *l_vec_indices, *t_offsets, *t_indices;
   CeedElemRestriction_Sycl *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
-  CeedCallBackend(CeedElemRestrictionGetData(r, &impl));
-  CeedCallBackend(CeedElemRestrictionGetNumElements(r, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
-  CeedCallBackend(CeedElemRestrictionGetLVectorSize(r, &l_size));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(r, &num_comp));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetData(rstr, &impl));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  CeedCallBackend(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
 
   // Count num_nodes
   CeedCallBackend(CeedCalloc(l_size, &is_node));
@@ -330,7 +330,7 @@ static int CeedElemRestrictionOffset_Sycl(const CeedElemRestriction r, const Cee
 // Create restriction
 //------------------------------------------------------------------------------
 int CeedElemRestrictionCreate_Sycl(CeedMemType mem_type, CeedCopyMode copy_mode, const CeedInt *indices, const bool *orients,
-                                   const CeedInt8 *curl_orients, CeedElemRestriction r) {
+                                   const CeedInt8 *curl_orients, CeedElemRestriction rstr) {
   Ceed                      ceed;
   Ceed_Sycl                *data;
   bool                      is_strided;
@@ -338,32 +338,33 @@ int CeedElemRestrictionCreate_Sycl(CeedMemType mem_type, CeedCopyMode copy_mode,
   CeedRestrictionType       rstr_type;
   CeedElemRestriction_Sycl *impl;
 
-  CeedCallBackend(CeedElemRestrictionGetCeed(r, &ceed));
+  CeedCallBackend(CeedElemRestrictionGetCeed(rstr, &ceed));
   CeedCallBackend(CeedGetData(ceed, &data));
-  CeedCallBackend(CeedCalloc(1, &impl));
-  CeedCallBackend(CeedElemRestrictionGetNumElements(r, &num_elem));
-  CeedCallBackend(CeedElemRestrictionGetNumComponents(r, &num_comp));
-  CeedCallBackend(CeedElemRestrictionGetElementSize(r, &elem_size));
-  CeedInt size       = num_elem * elem_size;
-  CeedInt strides[3] = {1, size, elem_size};
-
-  CeedCallBackend(CeedElemRestrictionGetType(r, &rstr_type));
+  CeedCallBackend(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCallBackend(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
+  CeedCallBackend(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  const CeedInt size       = num_elem * elem_size;
+  CeedInt       strides[3] = {1, size, elem_size};
+  CeedInt       layout[3]  = {1, elem_size * num_elem, elem_size};
+
+  CeedCallBackend(CeedElemRestrictionGetType(rstr, &rstr_type));
   CeedCheck(rstr_type != CEED_RESTRICTION_ORIENTED && rstr_type != CEED_RESTRICTION_CURL_ORIENTED, ceed, CEED_ERROR_BACKEND,
             "Backend does not implement CeedElemRestrictionCreateOriented or CeedElemRestrictionCreateCurlOriented");
 
   // Stride data
-  CeedCallBackend(CeedElemRestrictionIsStrided(r, &is_strided));
+  CeedCallBackend(CeedElemRestrictionIsStrided(rstr, &is_strided));
   if (is_strided) {
     bool has_backend_strides;
 
-    CeedCallBackend(CeedElemRestrictionHasBackendStrides(r, &has_backend_strides));
+    CeedCallBackend(CeedElemRestrictionHasBackendStrides(rstr, &has_backend_strides));
     if (!has_backend_strides) {
-      CeedCallBackend(CeedElemRestrictionGetStrides(r, &strides));
+      CeedCallBackend(CeedElemRestrictionGetStrides(rstr, &strides));
     }
   } else {
-    CeedCallBackend(CeedElemRestrictionGetCompStride(r, &comp_stride));
+    CeedCallBackend(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
   }
 
+  CeedCallBackend(CeedCalloc(1, &impl));
   impl->h_ind           = NULL;
   impl->h_ind_allocated = NULL;
   impl->d_ind           = NULL;
@@ -378,9 +379,8 @@ int CeedElemRestrictionCreate_Sycl(CeedMemType mem_type, CeedCopyMode copy_mode,
   impl->strides[0]      = strides[0];
   impl->strides[1]      = strides[1];
   impl->strides[2]      = strides[2];
-  CeedCallBackend(CeedElemRestrictionSetData(r, impl));
-  CeedInt layout[3] = {1, elem_size * num_elem, elem_size};
-  CeedCallBackend(CeedElemRestrictionSetELayout(r, layout));
+  CeedCallBackend(CeedElemRestrictionSetData(rstr, impl));
+  CeedCallBackend(CeedElemRestrictionSetELayout(rstr, layout));
 
   // Set up device indices/offset arrays
   if (mem_type == CEED_MEM_HOST) {
@@ -409,7 +409,7 @@ int CeedElemRestrictionCreate_Sycl(CeedMemType mem_type, CeedCopyMode copy_mode,
       sycl::event copy_event = data->sycl_queue.copy<CeedInt>(indices, impl->d_ind, size, {e});
       // Wait for copy to finish and handle exceptions
       CeedCallSycl(ceed, copy_event.wait_and_throw());
-      CeedCallBackend(CeedElemRestrictionOffset_Sycl(r, indices));
+      CeedCallBackend(CeedElemRestrictionOffset_Sycl(rstr, indices));
     }
   } else if (mem_type == CEED_MEM_DEVICE) {
     switch (copy_mode) {
@@ -440,7 +440,7 @@ int CeedElemRestrictionCreate_Sycl(CeedMemType mem_type, CeedCopyMode copy_mode,
       sycl::event copy_event = data->sycl_queue.copy<CeedInt>(impl->d_ind, impl->h_ind_allocated, elem_size * num_elem, {e});
       CeedCallSycl(ceed, copy_event.wait_and_throw());
       impl->h_ind = impl->h_ind_allocated;
-      CeedCallBackend(CeedElemRestrictionOffset_Sycl(r, indices));
+      CeedCallBackend(CeedElemRestrictionOffset_Sycl(rstr, indices));
     }
   } else {
     // LCOV_EXCL_START
@@ -449,10 +449,10 @@ int CeedElemRestrictionCreate_Sycl(CeedMemType mem_type, CeedCopyMode copy_mode,
   }
 
   // Register backend functions
-  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", r, "Apply", CeedElemRestrictionApply_Sycl));
-  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", r, "ApplyUnsigned", CeedElemRestrictionApply_Sycl));
-  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", r, "ApplyUnoriented", CeedElemRestrictionApply_Sycl));
-  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", r, "GetOffsets", CeedElemRestrictionGetOffsets_Sycl));
-  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", r, "Destroy", CeedElemRestrictionDestroy_Sycl));
+  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", rstr, "Apply", CeedElemRestrictionApply_Sycl));
+  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", rstr, "ApplyUnsigned", CeedElemRestrictionApply_Sycl));
+  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", rstr, "ApplyUnoriented", CeedElemRestrictionApply_Sycl));
+  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", rstr, "GetOffsets", CeedElemRestrictionGetOffsets_Sycl));
+  CeedCallBackend(CeedSetBackendFunctionCpp(ceed, "ElemRestriction", rstr, "Destroy", CeedElemRestrictionDestroy_Sycl));
   return CEED_ERROR_SUCCESS;
 }
diff --git a/backends/sycl-ref/ceed-sycl-vector.sycl.cpp b/backends/sycl-ref/ceed-sycl-vector.sycl.cpp
index 92cb9830..0302fecc 100644
--- a/backends/sycl-ref/ceed-sycl-vector.sycl.cpp
+++ b/backends/sycl-ref/ceed-sycl-vector.sycl.cpp
@@ -202,10 +202,10 @@ static int CeedVectorSetArrayHost_Sycl(const CeedVector vec, const CeedCopyMode
       impl->h_array          = impl->h_array_owned;
       if (array) {
         CeedSize length;
+        size_t   bytes;
 
         CeedCallBackend(CeedVectorGetLength(vec, &length));
-        size_t bytes = length * sizeof(CeedScalar);
-
+        bytes = length * sizeof(CeedScalar);
         memcpy(impl->h_array, array, bytes);
       }
     } break;
@@ -221,7 +221,6 @@ static int CeedVectorSetArrayHost_Sycl(const CeedVector vec, const CeedCopyMode
       impl->h_array          = array;
       break;
   }
-
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/include/ceed-impl.h b/include/ceed-impl.h
index 1b777308..cc5b8e6a 100644
--- a/include/ceed-impl.h
+++ b/include/ceed-impl.h
@@ -314,9 +314,9 @@ struct CeedQFunctionAssemblyData_private {
 
 struct CeedOperatorAssemblyData_private {
   Ceed                 ceed;
-  CeedInt              num_active_bases;
-  CeedBasis           *active_bases;
-  CeedElemRestriction *active_elem_rstrs;
+  CeedInt              num_active_bases_in, num_active_bases_out;
+  CeedBasis           *active_bases_in, *active_bases_out;
+  CeedElemRestriction *active_elem_rstrs_in, *active_elem_rstrs_out;
   CeedInt             *num_eval_modes_in, *num_eval_modes_out;
   CeedEvalMode       **eval_modes_in, **eval_modes_out;
   CeedScalar         **assembled_bases_in, **assembled_bases_out;
diff --git a/include/ceed/backend.h b/include/ceed/backend.h
index 2fd483a2..64cc1970 100644
--- a/include/ceed/backend.h
+++ b/include/ceed/backend.h
@@ -219,7 +219,7 @@ CEED_EXTERN int CeedGetDelegate(Ceed ceed, Ceed *delegate);
 CEED_EXTERN int CeedSetDelegate(Ceed ceed, Ceed delegate);
 CEED_EXTERN int CeedGetObjectDelegate(Ceed ceed, Ceed *delegate, const char *obj_name);
 CEED_EXTERN int CeedSetObjectDelegate(Ceed ceed, Ceed delegate, const char *obj_name);
-CEED_EXTERN int CeedGetOperatorfallback_resource(Ceed ceed, const char **resource);
+CEED_EXTERN int CeedGetOperatorFallbackResource(Ceed ceed, const char **resource);
 CEED_EXTERN int CeedGetOperatorFallbackCeed(Ceed ceed, Ceed *fallback_ceed);
 CEED_EXTERN int CeedSetOperatorFallbackResource(Ceed ceed, const char *resource);
 CEED_EXTERN int CeedSetDeterministic(Ceed ceed, bool is_deterministic);
@@ -348,6 +348,8 @@ CEED_EXTERN int CeedQFunctionContextRestoreInt32Read(CeedQFunctionContext ctx, C
 CEED_EXTERN int CeedQFunctionContextGetDataDestroy(CeedQFunctionContext ctx, CeedMemType *f_mem_type, CeedQFunctionContextDataDestroyUser *f);
 CEED_EXTERN int CeedQFunctionContextReference(CeedQFunctionContext ctx);
 
+CEED_EXTERN int CeedOperatorGetBasisPointer(CeedBasis basis, CeedEvalMode eval_mode, const CeedScalar *identity, const CeedScalar **basis_ptr);
+CEED_EXTERN int CeedOperatorCreateActivePointBlockRestriction(CeedElemRestriction rstr, CeedElemRestriction *pointblock_rstr);
 CEED_EXTERN int CeedQFunctionAssemblyDataCreate(Ceed ceed, CeedQFunctionAssemblyData *data);
 CEED_EXTERN int CeedQFunctionAssemblyDataReference(CeedQFunctionAssemblyData data);
 CEED_EXTERN int CeedQFunctionAssemblyDataSetReuse(CeedQFunctionAssemblyData data, bool reuse_assembly_data);
@@ -360,19 +362,25 @@ CEED_EXTERN int CeedQFunctionAssemblyDataGetObjects(CeedQFunctionAssemblyData da
 CEED_EXTERN int CeedQFunctionAssemblyDataDestroy(CeedQFunctionAssemblyData *data);
 
 CEED_EXTERN int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssemblyData *data);
-CEED_EXTERN int CeedOperatorAssemblyDataGetEvalModes(CeedOperatorAssemblyData data, CeedInt *num_active_bases, CeedInt **num_eval_modes_in,
+CEED_EXTERN int CeedOperatorAssemblyDataGetEvalModes(CeedOperatorAssemblyData data, CeedInt *num_active_bases_in, CeedInt **num_eval_modes_in,
                                                      const CeedEvalMode ***eval_modes_in, CeedSize ***eval_mode_offsets_in,
-                                                     CeedInt **num_eval_modes_out, const CeedEvalMode ***eval_modes_out,
-                                                     CeedSize ***eval_mode_offsets_out, CeedSize *num_output_components);
-CEED_EXTERN int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num_active_bases, CeedBasis **active_bases,
-                                                 const CeedScalar ***assembled_bases_in, const CeedScalar ***assembled_bases_out);
-CEED_EXTERN int CeedOperatorAssemblyDataGetElemRestrictions(CeedOperatorAssemblyData data, CeedInt *num_active_elem_rstrs,
-                                                            CeedElemRestriction **active_elem_rstrs);
+                                                     CeedInt *num_active_bases_out, CeedInt **num_eval_modes_out,
+                                                     const CeedEvalMode ***eval_modes_out, CeedSize ***eval_mode_offsets_out,
+                                                     CeedSize *num_output_components);
+CEED_EXTERN int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num_active_bases_in, CeedBasis **active_bases_in,
+                                                 const CeedScalar ***assembled_bases_in, CeedInt *num_active_bases_out, CeedBasis **active_bases_out,
+                                                 const CeedScalar ***assembled_bases_out);
+CEED_EXTERN int CeedOperatorAssemblyDataGetElemRestrictions(CeedOperatorAssemblyData data, CeedInt *num_active_elem_rstrs_in,
+                                                            CeedElemRestriction **active_elem_rstrs_in, CeedInt *num_active_elem_rstrs_out,
+                                                            CeedElemRestriction **active_elem_rstrs_out);
 CEED_EXTERN int CeedOperatorAssemblyDataDestroy(CeedOperatorAssemblyData *data);
 
 CEED_EXTERN int CeedOperatorGetOperatorAssemblyData(CeedOperator op, CeedOperatorAssemblyData *data);
 CEED_EXTERN int CeedOperatorGetActiveBasis(CeedOperator op, CeedBasis *active_basis);
+CEED_EXTERN int CeedOperatorGetActiveBases(CeedOperator op, CeedBasis *active_input_basis, CeedBasis *active_output_basis);
 CEED_EXTERN int CeedOperatorGetActiveElemRestriction(CeedOperator op, CeedElemRestriction *active_rstr);
+CEED_EXTERN int CeedOperatorGetActiveElemRestrictions(CeedOperator op, CeedElemRestriction *active_input_rstr,
+                                                      CeedElemRestriction *active_output_rstr);
 CEED_EXTERN int CeedOperatorGetNumArgs(CeedOperator op, CeedInt *num_args);
 CEED_EXTERN int CeedOperatorIsSetupDone(CeedOperator op, bool *is_setup_done);
 CEED_EXTERN int CeedOperatorGetQFunction(CeedOperator op, CeedQFunction *qf);
diff --git a/include/ceed/ceed.h b/include/ceed/ceed.h
index 2a522f1f..da0b18de 100644
--- a/include/ceed/ceed.h
+++ b/include/ceed/ceed.h
@@ -385,7 +385,7 @@ CEED_EXTERN int CeedQFunctionContextDestroy(CeedQFunctionContext *ctx);
 CEED_EXTERN int CeedOperatorCreate(Ceed ceed, CeedQFunction qf, CeedQFunction dqf, CeedQFunction dqfT, CeedOperator *op);
 CEED_EXTERN int CeedCompositeOperatorCreate(Ceed ceed, CeedOperator *op);
 CEED_EXTERN int CeedOperatorReferenceCopy(CeedOperator op, CeedOperator *op_copy);
-CEED_EXTERN int CeedOperatorSetField(CeedOperator op, const char *field_name, CeedElemRestriction r, CeedBasis b, CeedVector v);
+CEED_EXTERN int CeedOperatorSetField(CeedOperator op, const char *field_name, CeedElemRestriction rstr, CeedBasis basis, CeedVector v);
 CEED_EXTERN int CeedOperatorGetFields(CeedOperator op, CeedInt *num_input_fields, CeedOperatorField **input_fields, CeedInt *num_output_fields,
                                       CeedOperatorField **output_fields);
 CEED_EXTERN int CeedCompositeOperatorAddSub(CeedOperator composite_op, CeedOperator sub_op);
diff --git a/include/ceed/jit-source/cuda/cuda-gen-templates.h b/include/ceed/jit-source/cuda/cuda-gen-templates.h
index d6634b4b..265f7d2b 100644
--- a/include/ceed/jit-source/cuda/cuda-gen-templates.h
+++ b/include/ceed/jit-source/cuda/cuda-gen-templates.h
@@ -29,7 +29,7 @@ inline __device__ void loadMatrix(SharedData_Cuda &data, const CeedScalar *__res
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int COMP_STRIDE, int P_1d>
 inline __device__ void readDofsOffset1d(SharedData_Cuda &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
-                                        const CeedScalar *__restrict__ d_u, CeedScalar *r_u) {
+                                        const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = indices[node + elem * P_1d];
@@ -42,7 +42,8 @@ inline __device__ void readDofsOffset1d(SharedData_Cuda &data, const CeedInt num
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void readDofsStrided1d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ d_u, CeedScalar *r_u) {
+inline __device__ void readDofsStrided1d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ d_u,
+                                         CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
@@ -56,7 +57,7 @@ inline __device__ void readDofsStrided1d(SharedData_Cuda &data, const CeedInt el
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int COMP_STRIDE, int P_1d>
 inline __device__ void writeDofsOffset1d(SharedData_Cuda &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
-                                         const CeedScalar *r_v, CeedScalar *d_v) {
+                                         const CeedScalar *__restrict__ r_v, CeedScalar *__restrict__ d_v) {
   if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = indices[node + elem * P_1d];
@@ -69,7 +70,8 @@ inline __device__ void writeDofsOffset1d(SharedData_Cuda &data, const CeedInt nu
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void writeDofsStrided1d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *r_v, CeedScalar *d_v) {
+inline __device__ void writeDofsStrided1d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ r_v,
+                                          CeedScalar *__restrict__ d_v) {
   if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
@@ -87,7 +89,7 @@ inline __device__ void writeDofsStrided1d(SharedData_Cuda &data, const CeedInt e
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int COMP_STRIDE, int P_1d>
 inline __device__ void readDofsOffset2d(SharedData_Cuda &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
-                                        const CeedScalar *__restrict__ d_u, CeedScalar *r_u) {
+                                        const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
     const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
     const CeedInt ind  = indices[node + elem * P_1d * P_1d];
@@ -100,7 +102,8 @@ inline __device__ void readDofsOffset2d(SharedData_Cuda &data, const CeedInt num
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void readDofsStrided2d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ d_u, CeedScalar *r_u) {
+inline __device__ void readDofsStrided2d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ d_u,
+                                         CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
     const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
@@ -114,7 +117,7 @@ inline __device__ void readDofsStrided2d(SharedData_Cuda &data, const CeedInt el
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int COMP_STRIDE, int P_1d>
 inline __device__ void writeDofsOffset2d(SharedData_Cuda &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
-                                         const CeedScalar *r_v, CeedScalar *d_v) {
+                                         const CeedScalar *__restrict__ r_v, CeedScalar *__restrict__ d_v) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
     const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
     const CeedInt ind  = indices[node + elem * P_1d * P_1d];
@@ -127,7 +130,8 @@ inline __device__ void writeDofsOffset2d(SharedData_Cuda &data, const CeedInt nu
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void writeDofsStrided2d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *r_v, CeedScalar *d_v) {
+inline __device__ void writeDofsStrided2d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ r_v,
+                                          CeedScalar *__restrict__ d_v) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
     const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
@@ -152,7 +156,7 @@ inline __device__ void writeDofsStrided2d(SharedData_Cuda &data, const CeedInt e
 //   - writeDofsStrided3d -> writeStrided3d ?
 template <int NUM_COMP, int COMP_STRIDE, int P_1d>
 inline __device__ void readDofsOffset3d(SharedData_Cuda &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
-                                        const CeedScalar *__restrict__ d_u, CeedScalar *r_u) {
+                                        const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d)
     for (CeedInt z = 0; z < P_1d; z++) {
       const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
@@ -166,7 +170,8 @@ inline __device__ void readDofsOffset3d(SharedData_Cuda &data, const CeedInt num
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void readDofsStrided3d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ d_u, CeedScalar *r_u) {
+inline __device__ void readDofsStrided3d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ d_u,
+                                         CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d)
     for (CeedInt z = 0; z < P_1d; z++) {
       const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
@@ -181,7 +186,7 @@ inline __device__ void readDofsStrided3d(SharedData_Cuda &data, const CeedInt el
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int COMP_STRIDE, int Q_1d>
 inline __device__ void readSliceQuadsOffset3d(SharedData_Cuda &data, const CeedInt nquads, const CeedInt elem, const CeedInt q,
-                                              const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ d_u, CeedScalar *r_u) {
+                                              const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < Q_1d && data.t_id_y < Q_1d) {
     const CeedInt node = data.t_id_x + data.t_id_y * Q_1d + q * Q_1d * Q_1d;
     const CeedInt ind  = indices[node + elem * Q_1d * Q_1d * Q_1d];
@@ -195,7 +200,7 @@ inline __device__ void readSliceQuadsOffset3d(SharedData_Cuda &data, const CeedI
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int Q_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
 inline __device__ void readSliceQuadsStrided3d(SharedData_Cuda &data, const CeedInt elem, const CeedInt q, const CeedScalar *__restrict__ d_u,
-                                               CeedScalar *r_u) {
+                                               CeedScalar *__restrict__ r_u) {
   if (data.t_id_x < Q_1d && data.t_id_y < Q_1d) {
     const CeedInt node = data.t_id_x + data.t_id_y * Q_1d + q * Q_1d * Q_1d;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
@@ -209,7 +214,7 @@ inline __device__ void readSliceQuadsStrided3d(SharedData_Cuda &data, const Ceed
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int COMP_STRIDE, int P_1d>
 inline __device__ void writeDofsOffset3d(SharedData_Cuda &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
-                                         const CeedScalar *r_v, CeedScalar *d_v) {
+                                         const CeedScalar *__restrict__ r_v, CeedScalar *__restrict__ d_v) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d)
     for (CeedInt z = 0; z < P_1d; z++) {
       const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
@@ -223,7 +228,8 @@ inline __device__ void writeDofsOffset3d(SharedData_Cuda &data, const CeedInt nu
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
 template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void writeDofsStrided3d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *r_v, CeedScalar *d_v) {
+inline __device__ void writeDofsStrided3d(SharedData_Cuda &data, const CeedInt elem, const CeedScalar *__restrict__ r_v,
+                                          CeedScalar *__restrict__ d_v) {
   if (data.t_id_x < P_1d && data.t_id_y < P_1d)
     for (CeedInt z = 0; z < P_1d; z++) {
       const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
diff --git a/include/ceed/jit-source/cuda/cuda-jit.h b/include/ceed/jit-source/cuda/cuda-jit.h
index 17bed668..c9bd35db 100644
--- a/include/ceed/jit-source/cuda/cuda-jit.h
+++ b/include/ceed/jit-source/cuda/cuda-jit.h
@@ -17,4 +17,4 @@
 
 #include "cuda-types.h"
 
-#endif  // CEED_CUDA_JIT_DEFS_H
+#endif  // CEED_CUDA_JIT_H
diff --git a/include/ceed/jit-source/cuda/cuda-ref-basis-tensor.h b/include/ceed/jit-source/cuda/cuda-ref-basis-tensor.h
index 26053c7a..176f4b2e 100644
--- a/include/ceed/jit-source/cuda/cuda-ref-basis-tensor.h
+++ b/include/ceed/jit-source/cuda/cuda-ref-basis-tensor.h
@@ -46,28 +46,27 @@ extern "C" __global__ void Interp(const CeedInt num_elem, const CeedInt transpos
     for (CeedInt comp = 0; comp < BASIS_NUM_COMP; comp++) {
       const CeedScalar *cur_u = u + elem * u_stride + comp * u_comp_stride;
       CeedScalar       *cur_v = v + elem * v_stride + comp * v_comp_stride;
+      CeedInt           pre   = u_size;
+      CeedInt           post  = 1;
 
       for (CeedInt k = i; k < u_size; k += blockDim.x) {
         s_buffer_1[k] = cur_u[k];
       }
-      CeedInt pre  = u_size;
-      CeedInt post = 1;
-
       for (CeedInt d = 0; d < BASIS_DIM; d++) {
         __syncthreads();
         // Update buffers used
         pre /= P;
-        const CeedScalar *in  = d % 2 ? s_buffer_2 : s_buffer_1;
-        CeedScalar       *out = d == BASIS_DIM - 1 ? cur_v : (d % 2 ? s_buffer_1 : s_buffer_2);
+        const CeedScalar *in       = d % 2 ? s_buffer_2 : s_buffer_1;
+        CeedScalar       *out      = d == BASIS_DIM - 1 ? cur_v : (d % 2 ? s_buffer_1 : s_buffer_2);
+        const CeedInt     writeLen = pre * post * Q;
 
         // Contract along middle index
-        const CeedInt writeLen = pre * post * Q;
         for (CeedInt k = i; k < writeLen; k += blockDim.x) {
-          const CeedInt c = k % post;
-          const CeedInt j = (k / post) % Q;
-          const CeedInt a = k / (post * Q);
+          const CeedInt c  = k % post;
+          const CeedInt j  = (k / post) % Q;
+          const CeedInt a  = k / (post * Q);
+          CeedScalar    vk = 0;
 
-          CeedScalar vk = 0;
           for (CeedInt b = 0; b < P; b++) vk += s_interp_1d[j * stride_0 + b * stride_1] * in[(a * P + b) * post + c];
           out[k] = vk;
         }
@@ -119,12 +118,12 @@ extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose,
           __syncthreads();
           // Update buffers used
           pre /= P;
-          const CeedScalar *op  = dim_1 == dim_2 ? s_grad_1d : s_interp_1d;
-          const CeedScalar *in  = dim_2 == 0 ? cur_u : (dim_2 % 2 ? s_buffer_2 : s_buffer_1);
-          CeedScalar       *out = dim_2 == BASIS_DIM - 1 ? cur_v : (dim_2 % 2 ? s_buffer_1 : s_buffer_2);
+          const CeedScalar *op       = dim_1 == dim_2 ? s_grad_1d : s_interp_1d;
+          const CeedScalar *in       = dim_2 == 0 ? cur_u : (dim_2 % 2 ? s_buffer_2 : s_buffer_1);
+          CeedScalar       *out      = dim_2 == BASIS_DIM - 1 ? cur_v : (dim_2 % 2 ? s_buffer_1 : s_buffer_2);
+          const CeedInt     writeLen = pre * post * Q;
 
           // Contract along middle index
-          const CeedInt writeLen = pre * post * Q;
           for (CeedInt k = i; k < writeLen; k += blockDim.x) {
             const CeedInt c   = k % post;
             const CeedInt j   = (k / post) % Q;
@@ -132,7 +131,6 @@ extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose,
             CeedScalar    v_k = 0;
 
             for (CeedInt b = 0; b < P; b++) v_k += op[j * stride_0 + b * stride_1] * in[(a * P + b) * post + c];
-
             if (transpose && dim_2 == BASIS_DIM - 1) out[k] += v_k;
             else out[k] = v_k;
           }
diff --git a/include/ceed/jit-source/cuda/cuda-ref-operator-assemble-diagonal.h b/include/ceed/jit-source/cuda/cuda-ref-operator-assemble-diagonal.h
index 7c6f8789..ab366e79 100644
--- a/include/ceed/jit-source/cuda/cuda-ref-operator-assemble-diagonal.h
+++ b/include/ceed/jit-source/cuda/cuda-ref-operator-assemble-diagonal.h
@@ -19,11 +19,11 @@ typedef CeedInt IndexType;
 #endif
 
 //------------------------------------------------------------------------------
-// Get Basis Emode Pointer
+// Get basis pointer
 //------------------------------------------------------------------------------
-extern "C" __device__ void CeedOperatorGetBasisPointer_Cuda(const CeedScalar **basis_ptr, CeedEvalMode e_mode, const CeedScalar *identity,
-                                                            const CeedScalar *interp, const CeedScalar *grad) {
-  switch (e_mode) {
+static __device__ __inline__ void GetBasisPointer(const CeedScalar **basis_ptr, CeedEvalMode eval_modes, const CeedScalar *identity,
+                                                  const CeedScalar *interp, const CeedScalar *grad, const CeedScalar *div, const CeedScalar *curl) {
+  switch (eval_modes) {
     case CEED_EVAL_NONE:
       *basis_ptr = identity;
       break;
@@ -33,52 +33,67 @@ extern "C" __device__ void CeedOperatorGetBasisPointer_Cuda(const CeedScalar **b
     case CEED_EVAL_GRAD:
       *basis_ptr = grad;
       break;
-    case CEED_EVAL_WEIGHT:
     case CEED_EVAL_DIV:
+      *basis_ptr = div;
+      break;
     case CEED_EVAL_CURL:
-      break;  // Caught by QF Assembly
+      *basis_ptr = curl;
+      break;
+    case CEED_EVAL_WEIGHT:
+      break;  // Caught by QF assembly
   }
 }
 
 //------------------------------------------------------------------------------
 // Core code for diagonal assembly
 //------------------------------------------------------------------------------
-__device__ void diagonalCore(const CeedInt num_elem, const bool is_point_block, const CeedScalar *identity, const CeedScalar *interp_in,
-                             const CeedScalar *grad_in, const CeedScalar *interp_out, const CeedScalar *grad_out, const CeedEvalMode *e_mode_in,
-                             const CeedEvalMode *e_mode_out, const CeedScalar *__restrict__ assembled_qf_array,
-                             CeedScalar *__restrict__ elem_diag_array) {
-  const int tid = threadIdx.x;  // running with P threads, tid is evec node
+static __device__ __inline__ void DiagonalCore(const CeedInt num_elem, const bool is_point_block, const CeedScalar *identity,
+                                               const CeedScalar *interp_in, const CeedScalar *grad_in, const CeedScalar *div_in,
+                                               const CeedScalar *curl_in, const CeedScalar *interp_out, const CeedScalar *grad_out,
+                                               const CeedScalar *div_out, const CeedScalar *curl_out, const CeedEvalMode *eval_modes_in,
+                                               const CeedEvalMode *eval_modes_out, const CeedScalar *__restrict__ assembled_qf_array,
+                                               CeedScalar *__restrict__ elem_diag_array) {
+  const int tid = threadIdx.x;  // Running with P threads
+
   if (tid >= NUM_NODES) return;
 
   // Compute the diagonal of B^T D B
   // Each element
   for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < num_elem; e += gridDim.x * blockDim.z) {
-    IndexType d_out = -1;
-
     // Each basis eval mode pair
-    for (IndexType e_out = 0; e_out < NUM_E_MODE_OUT; e_out++) {
-      const CeedScalar *b_t = NULL;
+    IndexType    d_out               = 0;
+    CeedEvalMode eval_modes_out_prev = CEED_EVAL_NONE;
 
-      if (e_mode_out[e_out] == CEED_EVAL_GRAD) d_out += 1;
-      CeedOperatorGetBasisPointer_Cuda(&b_t, e_mode_out[e_out], identity, interp_out, &grad_out[d_out * NUM_QPTS * NUM_NODES]);
-      IndexType d_in = -1;
+    for (IndexType e_out = 0; e_out < NUM_EVAL_MODES_OUT; e_out++) {
+      IndexType         d_in               = 0;
+      CeedEvalMode      eval_modes_in_prev = CEED_EVAL_NONE;
+      const CeedScalar *b_t                = NULL;
 
-      for (IndexType e_in = 0; e_in < NUM_E_MODE_IN; e_in++) {
+      GetBasisPointer(&b_t, eval_modes_out[e_out], identity, interp_out, grad_out, div_out, curl_out);
+      if (e_out == 0 || eval_modes_out[e_out] != eval_modes_out_prev) d_out = 0;
+      else b_t = &b_t[(++d_out) * NUM_QPTS * NUM_NODES];
+      eval_modes_out_prev = eval_modes_out[e_out];
+
+      for (IndexType e_in = 0; e_in < NUM_EVAL_MODES_IN; e_in++) {
         const CeedScalar *b = NULL;
 
-        if (e_mode_in[e_in] == CEED_EVAL_GRAD) d_in += 1;
-        CeedOperatorGetBasisPointer_Cuda(&b, e_mode_in[e_in], identity, interp_in, &grad_in[d_in * NUM_QPTS * NUM_NODES]);
+        GetBasisPointer(&b, eval_modes_in[e_in], identity, interp_in, grad_in, div_in, curl_in);
+        if (e_in == 0 || eval_modes_in[e_in] != eval_modes_in_prev) d_in = 0;
+        else b = &b[(++d_in) * NUM_QPTS * NUM_NODES];
+        eval_modes_in_prev = eval_modes_in[e_in];
+
         // Each component
         for (IndexType comp_out = 0; comp_out < NUM_COMP; comp_out++) {
           // Each qpoint/node pair
           if (is_point_block) {
-            // Point Block Diagonal
+            // Point block diagonal
             for (IndexType comp_in = 0; comp_in < NUM_COMP; comp_in++) {
               CeedScalar e_value = 0.;
 
               for (IndexType q = 0; q < NUM_QPTS; q++) {
                 const CeedScalar qf_value =
-                    assembled_qf_array[((((e_in * NUM_COMP + comp_in) * NUM_E_MODE_OUT + e_out) * NUM_COMP + comp_out) * num_elem + e) * NUM_QPTS +
+                    assembled_qf_array[((((e_in * NUM_COMP + comp_in) * NUM_EVAL_MODES_OUT + e_out) * NUM_COMP + comp_out) * num_elem + e) *
+                                           NUM_QPTS +
                                        q];
 
                 e_value += b_t[q * NUM_NODES + tid] * qf_value * b[q * NUM_NODES + tid];
@@ -86,12 +101,13 @@ __device__ void diagonalCore(const CeedInt num_elem, const bool is_point_block,
               elem_diag_array[((comp_out * NUM_COMP + comp_in) * num_elem + e) * NUM_NODES + tid] += e_value;
             }
           } else {
-            // Diagonal Only
+            // Diagonal only
             CeedScalar e_value = 0.;
 
             for (IndexType q = 0; q < NUM_QPTS; q++) {
               const CeedScalar qf_value =
-                  assembled_qf_array[((((e_in * NUM_COMP + comp_out) * NUM_E_MODE_OUT + e_out) * NUM_COMP + comp_out) * num_elem + e) * NUM_QPTS + q];
+                  assembled_qf_array[((((e_in * NUM_COMP + comp_out) * NUM_EVAL_MODES_OUT + e_out) * NUM_COMP + comp_out) * num_elem + e) * NUM_QPTS +
+                                     q];
 
               e_value += b_t[q * NUM_NODES + tid] * qf_value * b[q * NUM_NODES + tid];
             }
@@ -106,21 +122,25 @@ __device__ void diagonalCore(const CeedInt num_elem, const bool is_point_block,
 //------------------------------------------------------------------------------
 // Linear diagonal
 //------------------------------------------------------------------------------
-extern "C" __global__ void linearDiagonal(const CeedInt num_elem, const CeedScalar *identity, const CeedScalar *interp_in, const CeedScalar *grad_in,
-                                          const CeedScalar *interp_out, const CeedScalar *grad_out, const CeedEvalMode *e_mode_in,
-                                          const CeedEvalMode *e_mode_out, const CeedScalar *__restrict__ assembled_qf_array,
-                                          CeedScalar *__restrict__ elem_diag_array) {
-  diagonalCore(num_elem, false, identity, interp_in, grad_in, interp_out, grad_out, e_mode_in, e_mode_out, assembled_qf_array, elem_diag_array);
+extern "C" __global__ void LinearDiagonal(const CeedInt num_elem, const CeedScalar *identity, const CeedScalar *interp_in, const CeedScalar *grad_in,
+                                          const CeedScalar *div_in, const CeedScalar *curl_in, const CeedScalar *interp_out,
+                                          const CeedScalar *grad_out, const CeedScalar *div_out, const CeedScalar *curl_out,
+                                          const CeedEvalMode *eval_modes_in, const CeedEvalMode *eval_modes_out,
+                                          const CeedScalar *__restrict__ assembled_qf_array, CeedScalar *__restrict__ elem_diag_array) {
+  DiagonalCore(num_elem, false, identity, interp_in, grad_in, div_in, curl_in, interp_out, grad_out, div_out, curl_out, eval_modes_in, eval_modes_out,
+               assembled_qf_array, elem_diag_array);
 }
 
 //------------------------------------------------------------------------------
 // Linear point block diagonal
 //------------------------------------------------------------------------------
-extern "C" __global__ void linearPointBlockDiagonal(const CeedInt num_elem, const CeedScalar *identity, const CeedScalar *interp_in,
-                                                    const CeedScalar *grad_in, const CeedScalar *interp_out, const CeedScalar *grad_out,
-                                                    const CeedEvalMode *e_mode_in, const CeedEvalMode *e_mode_out,
+extern "C" __global__ void LinearPointBlockDiagonal(const CeedInt num_elem, const CeedScalar *identity, const CeedScalar *interp_in,
+                                                    const CeedScalar *grad_in, const CeedScalar *div_in, const CeedScalar *curl_in,
+                                                    const CeedScalar *interp_out, const CeedScalar *grad_out, const CeedScalar *div_out,
+                                                    const CeedScalar *curl_out, const CeedEvalMode *eval_modes_in, const CeedEvalMode *eval_modes_out,
                                                     const CeedScalar *__restrict__ assembled_qf_array, CeedScalar *__restrict__ elem_diag_array) {
-  diagonalCore(num_elem, true, identity, interp_in, grad_in, interp_out, grad_out, e_mode_in, e_mode_out, assembled_qf_array, elem_diag_array);
+  DiagonalCore(num_elem, true, identity, interp_in, grad_in, div_in, curl_in, interp_out, grad_out, div_out, curl_out, eval_modes_in, eval_modes_out,
+               assembled_qf_array, elem_diag_array);
 }
 
 //------------------------------------------------------------------------------
diff --git a/include/ceed/jit-source/cuda/cuda-ref-operator-assemble.h b/include/ceed/jit-source/cuda/cuda-ref-operator-assemble.h
index eeb256fe..60d641ed 100644
--- a/include/ceed/jit-source/cuda/cuda-ref-operator-assemble.h
+++ b/include/ceed/jit-source/cuda/cuda-ref-operator-assemble.h
@@ -19,108 +19,92 @@ typedef CeedInt IndexType;
 #endif
 
 //------------------------------------------------------------------------------
-// Matrix assembly kernel for low-order elements (2D thread block)
+// Matrix assembly kernel
 //------------------------------------------------------------------------------
 extern "C" __launch_bounds__(BLOCK_SIZE) __global__
-    void linearAssemble(const CeedScalar *B_in, const CeedScalar *B_out, const CeedScalar *__restrict__ qf_array,
-                        CeedScalar *__restrict__ values_array) {
-  // This kernel assumes B_in and B_out have the same number of quadrature points and basis points.
-  // TODO: expand to more general cases
-  const int i = threadIdx.x;  // The output row index of each B^TDB operation
-  const int l = threadIdx.y;  // The output column index of each B^TDB operation
-                              // such that we have (Bout^T)_ij D_jk Bin_kl = C_il
-
-  // Strides for final output ordering, determined by the reference (interface) implementation of the symbolic assembly, slowest --> fastest: element,
-  // comp_in, comp_out, node_row, node_col
-  const IndexType comp_out_stride = NUM_NODES * NUM_NODES;
-  const IndexType comp_in_stride  = comp_out_stride * NUM_COMP;
-  const IndexType e_stride        = comp_in_stride * NUM_COMP;
-  // Strides for QF array, slowest --> fastest:  e_mode_in, comp_in, e_mode_out, comp_out, elem, qpt
-  const IndexType q_e_stride          = NUM_QPTS;
-  const IndexType q_comp_out_stride   = NUM_ELEM * q_e_stride;
-  const IndexType q_e_mode_out_stride = q_comp_out_stride * NUM_COMP;
-  const IndexType q_comp_in_stride    = q_e_mode_out_stride * NUM_E_MODE_OUT;
-  const IndexType q_e_mode_in_stride  = q_comp_in_stride * NUM_COMP;
-
-  // Loop over each element (if necessary)
-  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < NUM_ELEM; e += gridDim.x * blockDim.z) {
-    for (IndexType comp_in = 0; comp_in < NUM_COMP; comp_in++) {
-      for (IndexType comp_out = 0; comp_out < NUM_COMP; comp_out++) {
-        CeedScalar result        = 0.0;
-        IndexType  qf_index_comp = q_comp_in_stride * comp_in + q_comp_out_stride * comp_out + q_e_stride * e;
-
-        for (IndexType e_mode_in = 0; e_mode_in < NUM_E_MODE_IN; e_mode_in++) {
-          IndexType b_in_index = e_mode_in * NUM_QPTS * NUM_NODES;
+    void LinearAssemble(const CeedInt num_elem, const CeedScalar *B_in, const CeedScalar *B_out, const bool *orients_in,
+                        const CeedInt8 *curl_orients_in, const bool *orients_out, const CeedInt8 *curl_orients_out,
+                        const CeedScalar *__restrict__ qf_array, CeedScalar *__restrict__ values_array) {
+  extern __shared__ CeedScalar s_CT[];
+  CeedScalar                  *s_C = s_CT + NUM_NODES_OUT * NUM_NODES_IN;
 
-          for (IndexType e_mode_out = 0; e_mode_out < NUM_E_MODE_OUT; e_mode_out++) {
-            IndexType b_out_index = e_mode_out * NUM_QPTS * NUM_NODES;
-            IndexType qf_index    = qf_index_comp + q_e_mode_out_stride * e_mode_out + q_e_mode_in_stride * e_mode_in;
-
-            // Perform the B^T D B operation for this 'chunk' of D (the qf_array)
-            for (IndexType j = 0; j < NUM_QPTS; j++) {
-              result += B_out[b_out_index + j * NUM_NODES + i] * qf_array[qf_index + j] * B_in[b_in_index + j * NUM_NODES + l];
-            }
-          }  // end of e_mode_out
-        }    // end of e_mode_in
-        IndexType val_index = comp_in_stride * comp_in + comp_out_stride * comp_out + e_stride * e + NUM_NODES * i + l;
-
-        values_array[val_index] = result;
-      }  // end of out component
-    }    // end of in component
-  }      // end of element loop
-}
-
-//------------------------------------------------------------------------------
-// Fallback kernel for larger orders (1D thread block)
-//------------------------------------------------------------------------------
-extern "C" __launch_bounds__(BLOCK_SIZE) __global__
-    void linearAssembleFallback(const CeedScalar *B_in, const CeedScalar *B_out, const CeedScalar *__restrict__ qf_array,
-                                CeedScalar *__restrict__ values_array) {
-  // This kernel assumes B_in and B_out have the same number of quadrature points and basis points.
-  // TODO: expand to more general cases
-  const int l = threadIdx.x;  // The output column index of each B^TDB operation
+  const int l = threadIdx.x;  // The output column index of each B^T D B operation
                               // such that we have (Bout^T)_ij D_jk Bin_kl = C_il
 
-  // Strides for final output ordering, determined by the reference (interface) implementation of the symbolic assembly, slowest --> fastest: element,
+  // Strides for final output ordering, determined by the reference (interface) implementation of the symbolic assembly, slowest --> fastest: e,
   // comp_in, comp_out, node_row, node_col
-  const IndexType comp_out_stride = NUM_NODES * NUM_NODES;
-  const IndexType comp_in_stride  = comp_out_stride * NUM_COMP;
-  const IndexType e_stride        = comp_in_stride * NUM_COMP;
-  // Strides for QF array, slowest --> fastest:  e_mode_in, comp_in, e_mode_out, comp_out, elem, qpt
-  const IndexType q_e_stride          = NUM_QPTS;
-  const IndexType q_comp_out_stride   = NUM_ELEM * q_e_stride;
-  const IndexType q_e_mode_out_stride = q_comp_out_stride * NUM_COMP;
-  const IndexType q_comp_in_stride    = q_e_mode_out_stride * NUM_E_MODE_OUT;
-  const IndexType q_e_mode_in_stride  = q_comp_in_stride * NUM_COMP;
+  const IndexType comp_out_stride = NUM_NODES_OUT * NUM_NODES_IN;
+  const IndexType comp_in_stride  = comp_out_stride * NUM_COMP_OUT;
+  const IndexType e_stride        = comp_in_stride * NUM_COMP_IN;
+
+  // Strides for QF array, slowest --> fastest: e_in, comp_in, e_out, comp_out, e, q
+  const IndexType q_e_stride             = NUM_QPTS;
+  const IndexType q_comp_out_stride      = num_elem * q_e_stride;
+  const IndexType q_eval_mode_out_stride = q_comp_out_stride * NUM_COMP_OUT;
+  const IndexType q_comp_in_stride       = q_eval_mode_out_stride * NUM_EVAL_MODES_OUT;
+  const IndexType q_eval_mode_in_stride  = q_comp_in_stride * NUM_COMP_IN;
 
   // Loop over each element (if necessary)
-  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < NUM_ELEM; e += gridDim.x * blockDim.z) {
-    for (IndexType comp_in = 0; comp_in < NUM_COMP; comp_in++) {
-      for (IndexType comp_out = 0; comp_out < NUM_COMP; comp_out++) {
-        for (IndexType i = 0; i < NUM_NODES; i++) {
+  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < num_elem; e += gridDim.x * blockDim.z) {
+    for (IndexType comp_in = 0; comp_in < NUM_COMP_IN; comp_in++) {
+      for (IndexType comp_out = 0; comp_out < NUM_COMP_OUT; comp_out++) {
+        for (IndexType i = threadIdx.y; i < NUM_NODES_OUT; i += BLOCK_SIZE_Y) {
           CeedScalar result        = 0.0;
           IndexType  qf_index_comp = q_comp_in_stride * comp_in + q_comp_out_stride * comp_out + q_e_stride * e;
 
-          for (IndexType e_mode_in = 0; e_mode_in < NUM_E_MODE_IN; e_mode_in++) {
-            IndexType b_in_index = e_mode_in * NUM_QPTS * NUM_NODES;
+          for (IndexType e_in = 0; e_in < NUM_EVAL_MODES_IN; e_in++) {
+            IndexType b_in_index = e_in * NUM_QPTS * NUM_NODES_IN;
 
-            for (IndexType e_mode_out = 0; e_mode_out < NUM_E_MODE_OUT; e_mode_out++) {
-              IndexType b_out_index = e_mode_out * NUM_QPTS * NUM_NODES;
-              IndexType qf_index    = qf_index_comp + q_e_mode_out_stride * e_mode_out + q_e_mode_in_stride * e_mode_in;
+            for (IndexType e_out = 0; e_out < NUM_EVAL_MODES_OUT; e_out++) {
+              IndexType b_out_index = e_out * NUM_QPTS * NUM_NODES_OUT;
+              IndexType qf_index    = qf_index_comp + q_eval_mode_out_stride * e_out + q_eval_mode_in_stride * e_in;
 
               // Perform the B^T D B operation for this 'chunk' of D (the qf_array)
               for (IndexType j = 0; j < NUM_QPTS; j++) {
-                result += B_out[b_out_index + j * NUM_NODES + i] * qf_array[qf_index + j] * B_in[b_in_index + j * NUM_NODES + l];
+                result += B_out[b_out_index + j * NUM_NODES_OUT + i] * qf_array[qf_index + j] * B_in[b_in_index + j * NUM_NODES_IN + l];
               }
-            }  // end of e_mode_out
-          }    // end of e_mode_in
-          IndexType val_index = comp_in_stride * comp_in + comp_out_stride * comp_out + e_stride * e + NUM_NODES * i + l;
-
-          values_array[val_index] = result;
+            }  // end of out eval mode
+          }    // end of in eval mode
+          if (orients_in) {
+            result *= orients_in[NUM_NODES_IN * e + l] ? -1.0 : 1.0;
+          }
+          if (orients_out) {
+            result *= orients_out[NUM_NODES_OUT * e + i] ? -1.0 : 1.0;
+          }
+          if (!curl_orients_in && !curl_orients_out) {
+            IndexType val_index = e_stride * e + comp_in_stride * comp_in + comp_out_stride * comp_out + NUM_NODES_IN * i + l;
+
+            values_array[val_index] = result;
+          } else if (curl_orients_in) {
+            s_C[NUM_NODES_IN * threadIdx.y + l] = result;
+            __syncthreads();
+            s_CT[NUM_NODES_IN * i + l] =
+                (l > 0 ? s_C[NUM_NODES_IN * threadIdx.y + l - 1] * curl_orients_in[3 * NUM_NODES_IN * e + 3 * l - 1] : 0.0) +
+                s_C[NUM_NODES_IN * threadIdx.y + l] * curl_orients_in[3 * NUM_NODES_IN * e + 3 * l + 1] +
+                (l < (NUM_NODES_IN - 1) ? s_C[NUM_NODES_IN * threadIdx.y + l + 1] * curl_orients_in[3 * NUM_NODES_IN * e + 3 * l + 3] : 0.0);
+          } else {
+            s_CT[NUM_NODES_IN * i + l] = result;
+          }
         }  // end of loop over element node index, i
-      }    // end of out component
-    }      // end of in component
-  }        // end of element loop
+        if (curl_orients_in || curl_orients_out) {
+          // Compute and store the final T^T (B^T D B T) using the fully computed C T product in shared memory
+          if (curl_orients_out) __syncthreads();
+          for (IndexType i = threadIdx.y; i < NUM_NODES_OUT; i += BLOCK_SIZE_Y) {
+            IndexType val_index = e_stride * e + comp_in_stride * comp_in + comp_out_stride * comp_out + NUM_NODES_IN * i + l;
+
+            if (curl_orients_out) {
+              values_array[val_index] =
+                  (i > 0 ? s_CT[NUM_NODES_IN * (i - 1) + l] * curl_orients_out[3 * NUM_NODES_OUT * e + 3 * i - 1] : 0.0) +
+                  s_CT[NUM_NODES_IN * i + l] * curl_orients_out[3 * NUM_NODES_OUT * e + 3 * i + 1] +
+                  (i < (NUM_NODES_OUT - 1) ? s_CT[NUM_NODES_IN * (i + 1) + l] * curl_orients_out[3 * NUM_NODES_OUT * e + 3 * i + 3] : 0.0);
+            } else {
+              values_array[val_index] = s_CT[NUM_NODES_IN * i + l];
+            }
+          }
+        }
+      }  // end of out component
+    }    // end of in component
+  }      // end of element loop
 }
 
 //------------------------------------------------------------------------------
diff --git a/include/ceed/jit-source/cuda/cuda-ref-qfunction.h b/include/ceed/jit-source/cuda/cuda-ref-qfunction.h
index 42ae1d54..c750a4f9 100644
--- a/include/ceed/jit-source/cuda/cuda-ref-qfunction.h
+++ b/include/ceed/jit-source/cuda/cuda-ref-qfunction.h
@@ -16,7 +16,7 @@
 // Read from quadrature points
 //------------------------------------------------------------------------------
 template <int SIZE>
-inline __device__ void readQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar* d_u, CeedScalar* r_u) {
+inline __device__ void readQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar *d_u, CeedScalar *r_u) {
   for (CeedInt comp = 0; comp < SIZE; comp++) {
     r_u[comp] = d_u[quad + num_qpts * comp];
   }
@@ -26,7 +26,7 @@ inline __device__ void readQuads(const CeedInt quad, const CeedInt num_qpts, con
 // Write at quadrature points
 //------------------------------------------------------------------------------
 template <int SIZE>
-inline __device__ void writeQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar* r_v, CeedScalar* d_v) {
+inline __device__ void writeQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar *r_v, CeedScalar *d_v) {
   for (CeedInt comp = 0; comp < SIZE; comp++) {
     d_v[quad + num_qpts * comp] = r_v[comp];
   }
diff --git a/include/ceed/jit-source/cuda/cuda-ref-restriction.h b/include/ceed/jit-source/cuda/cuda-ref-restriction.h
index 1df6f049..48bbd206 100644
--- a/include/ceed/jit-source/cuda/cuda-ref-restriction.h
+++ b/include/ceed/jit-source/cuda/cuda-ref-restriction.h
@@ -28,38 +28,107 @@ extern "C" __global__ void StridedNoTranspose(const CeedInt num_elem, const Ceed
 }
 
 //------------------------------------------------------------------------------
-// E-vector -> L-vector, strided
+// L-vector -> E-vector, standard (with offsets)
 //------------------------------------------------------------------------------
-extern "C" __global__ void StridedTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
+extern "C" __global__ void OffsetNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ u,
+                                             CeedScalar *__restrict__ v) {
   for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt ind      = indices[node];
     const CeedInt loc_node = node % RSTR_ELEM_SIZE;
     const CeedInt elem     = node / RSTR_ELEM_SIZE;
 
     for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
-      v[loc_node * RSTR_STRIDE_NODES + comp * RSTR_STRIDE_COMP + elem * RSTR_STRIDE_ELEM] +=
-          u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE];
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = u[ind + comp * RSTR_COMP_STRIDE];
     }
   }
 }
 
 //------------------------------------------------------------------------------
-// L-vector -> E-vector, offsets provided
+// L-vector -> E-vector, oriented
 //------------------------------------------------------------------------------
-extern "C" __global__ void OffsetNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ u,
-                                             CeedScalar *__restrict__ v) {
+extern "C" __global__ void OrientedNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const bool *__restrict__ orients,
+                                               const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
   for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
     const CeedInt ind      = indices[node];
+    const bool    orient   = orients[node];
     const CeedInt loc_node = node % RSTR_ELEM_SIZE;
     const CeedInt elem     = node / RSTR_ELEM_SIZE;
 
     for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
-      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = u[ind + comp * RSTR_COMP_STRIDE];
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = u[ind + comp * RSTR_COMP_STRIDE] * (orient ? -1.0 : 1.0);
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// L-vector -> E-vector, curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                   const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                   CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind_dl         = loc_node > 0 ? indices[node - 1] : 0;
+    const CeedInt  ind_d          = indices[node];
+    const CeedInt  ind_du         = loc_node < (RSTR_ELEM_SIZE - 1) ? indices[node + 1] : 0;
+    const CeedInt8 curl_orient_dl = curl_orients[3 * node + 0];
+    const CeedInt8 curl_orient_d  = curl_orients[3 * node + 1];
+    const CeedInt8 curl_orient_du = curl_orients[3 * node + 2];
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[ind_dl + comp * RSTR_COMP_STRIDE] * curl_orient_dl : 0.0;
+      value += u[ind_d + comp * RSTR_COMP_STRIDE] * curl_orient_d;
+      value += loc_node < (RSTR_ELEM_SIZE - 1) ? u[ind_du + comp * RSTR_COMP_STRIDE] * curl_orient_du : 0.0;
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = value;
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// L-vector -> E-vector, unsigned curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedUnsignedNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                           const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                           CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind_dl         = loc_node > 0 ? indices[node - 1] : 0;
+    const CeedInt  ind_d          = indices[node];
+    const CeedInt  ind_du         = loc_node < (RSTR_ELEM_SIZE - 1) ? indices[node + 1] : 0;
+    const CeedInt8 curl_orient_dl = abs(curl_orients[3 * node + 0]);
+    const CeedInt8 curl_orient_d  = abs(curl_orients[3 * node + 1]);
+    const CeedInt8 curl_orient_du = abs(curl_orients[3 * node + 2]);
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[ind_dl + comp * RSTR_COMP_STRIDE] * curl_orient_dl : 0.0;
+      value += u[ind_d + comp * RSTR_COMP_STRIDE] * curl_orient_d;
+      value += loc_node < (RSTR_ELEM_SIZE - 1) ? u[ind_du + comp * RSTR_COMP_STRIDE] * curl_orient_du : 0.0;
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = value;
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, strided
+//------------------------------------------------------------------------------
+extern "C" __global__ void StridedTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      v[loc_node * RSTR_STRIDE_NODES + comp * RSTR_STRIDE_COMP + elem * RSTR_STRIDE_ELEM] +=
+          u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE];
     }
   }
 }
 
 //------------------------------------------------------------------------------
-// E-vector -> L-vector, offsets provided
+// E-vector -> L-vector, standard (with offsets)
 //------------------------------------------------------------------------------
 extern "C" __global__ void OffsetTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ u,
                                            CeedScalar *__restrict__ v) {
@@ -87,8 +156,8 @@ extern "C" __global__ void OffsetTransposeDet(const CeedInt *__restrict__ l_vec_
 
     for (CeedInt j = range_1; j < range_N; j++) {
       const CeedInt t_ind    = t_indices[j];
-      CeedInt       loc_node = t_ind % RSTR_ELEM_SIZE;
-      CeedInt       elem     = t_ind / RSTR_ELEM_SIZE;
+      const CeedInt loc_node = t_ind % RSTR_ELEM_SIZE;
+      const CeedInt elem     = t_ind / RSTR_ELEM_SIZE;
 
       for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
         value[comp] += u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE];
@@ -99,6 +168,74 @@ extern "C" __global__ void OffsetTransposeDet(const CeedInt *__restrict__ l_vec_
   }
 }
 
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void OrientedTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const bool *__restrict__ orients,
+                                             const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt ind      = indices[node];
+    const bool    orient   = orients[node];
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      atomicAdd(v + ind + comp * RSTR_COMP_STRIDE,
+                u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * (orient ? -1.0 : 1.0));
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                 const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                 CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind            = indices[node];
+    const CeedInt8 curl_orient_du = loc_node > 0 ? curl_orients[3 * node - 1] : 0.0;
+    const CeedInt8 curl_orient_d  = curl_orients[3 * node + 1];
+    const CeedInt8 curl_orient_dl = loc_node < (RSTR_ELEM_SIZE - 1) ? curl_orients[3 * node + 3] : 0.0;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[loc_node - 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_du : 0.0;
+      value += u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_d;
+      value +=
+          loc_node < (RSTR_ELEM_SIZE - 1) ? u[loc_node + 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_dl : 0.0;
+      atomicAdd(v + ind + comp * RSTR_COMP_STRIDE, value);
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, unsigned curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedUnsignedTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                         const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                         CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind            = indices[node];
+    const CeedInt8 curl_orient_du = loc_node > 0 ? abs(curl_orients[3 * node - 1]) : 0.0;
+    const CeedInt8 curl_orient_d  = abs(curl_orients[3 * node + 1]);
+    const CeedInt8 curl_orient_dl = loc_node < (RSTR_ELEM_SIZE - 1) ? abs(curl_orients[3 * node + 3]) : 0.0;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[loc_node - 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_du : 0.0;
+      value += u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_d;
+      value +=
+          loc_node < (RSTR_ELEM_SIZE - 1) ? u[loc_node + 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_dl : 0.0;
+      atomicAdd(v + ind + comp * RSTR_COMP_STRIDE, value);
+    }
+  }
+}
+
 //------------------------------------------------------------------------------
 
 #endif  // CEED_CUDA_REF_RESTRICTION_H
diff --git a/include/ceed/jit-source/cuda/cuda-types.h b/include/ceed/jit-source/cuda/cuda-types.h
index 9736e59c..76f7f2a9 100644
--- a/include/ceed/jit-source/cuda/cuda-types.h
+++ b/include/ceed/jit-source/cuda/cuda-types.h
@@ -15,13 +15,13 @@
 #define CEED_CUDA_NUMBER_FIELDS 16
 
 typedef struct {
-  const CeedScalar* inputs[CEED_CUDA_NUMBER_FIELDS];
-  CeedScalar*       outputs[CEED_CUDA_NUMBER_FIELDS];
+  const CeedScalar *inputs[CEED_CUDA_NUMBER_FIELDS];
+  CeedScalar       *outputs[CEED_CUDA_NUMBER_FIELDS];
 } Fields_Cuda;
 
 typedef struct {
-  CeedInt* inputs[CEED_CUDA_NUMBER_FIELDS];
-  CeedInt* outputs[CEED_CUDA_NUMBER_FIELDS];
+  CeedInt *inputs[CEED_CUDA_NUMBER_FIELDS];
+  CeedInt *outputs[CEED_CUDA_NUMBER_FIELDS];
 } FieldsInt_Cuda;
 
 typedef struct {
@@ -29,7 +29,7 @@ typedef struct {
   CeedInt     t_id_y;
   CeedInt     t_id_z;
   CeedInt     t_id;
-  CeedScalar* slice;
+  CeedScalar *slice;
 } SharedData_Cuda;
 
 #endif  // CEED_CUDA_TYPES_H
diff --git a/include/ceed/jit-source/hip/hip-gen-templates.h b/include/ceed/jit-source/hip/hip-gen-templates.h
index f0f019db..884f346d 100644
--- a/include/ceed/jit-source/hip/hip-gen-templates.h
+++ b/include/ceed/jit-source/hip/hip-gen-templates.h
@@ -16,7 +16,7 @@
 // Load matrices for basis actions
 //------------------------------------------------------------------------------
 template <int P, int Q>
-inline __device__ void loadMatrix(SharedData_Hip& data, const CeedScalar* d_B, CeedScalar* B) {
+inline __device__ void loadMatrix(SharedData_Hip &data, const CeedScalar *__restrict__ d_B, CeedScalar *B) {
   for (CeedInt i = data.t_id; i < P * Q; i += blockDim.x * blockDim.y * blockDim.z) B[i] = d_B[i];
 }
 
@@ -27,50 +27,55 @@ inline __device__ void loadMatrix(SharedData_Hip& data, const CeedScalar* d_B, C
 //------------------------------------------------------------------------------
 // L-vector -> E-vector, offsets provided
 //------------------------------------------------------------------------------
-template <int NCOMP, int COMPSTRIDE, int P1d>
-inline __device__ void readDofsOffset1d(SharedData_Hip& data, const CeedInt nnodes, const CeedInt elem, const CeedInt* indices, const CeedScalar* d_u,
-                                        CeedScalar* r_u) {
-  if (data.t_id_x < P1d) {
+template <int NUM_COMP, int COMP_STRIDE, int P_1d>
+inline __device__ void readDofsOffset1d(SharedData_Hip &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
+                                        const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
-    const CeedInt ind  = indices[node + elem * P1d];
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[comp] = d_u[ind + COMPSTRIDE * comp];
+    const CeedInt ind  = indices[node + elem * P_1d];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[comp] = d_u[ind + COMP_STRIDE * comp];
   }
 }
 
 //------------------------------------------------------------------------------
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
-template <int NCOMP, int P1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void readDofsStrided1d(SharedData_Hip& data, const CeedInt elem, const CeedScalar* d_u, CeedScalar* r_u) {
-  if (data.t_id_x < P1d) {
+template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
+inline __device__ void readDofsStrided1d(SharedData_Hip &data, const CeedInt elem, const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[comp] = d_u[ind + comp * STRIDES_COMP];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[comp] = d_u[ind + comp * STRIDES_COMP];
   }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> L-vector, offsets provided
 //------------------------------------------------------------------------------
-template <int NCOMP, int COMPSTRIDE, int P1d>
-inline __device__ void writeDofsOffset1d(SharedData_Hip& data, const CeedInt nnodes, const CeedInt elem, const CeedInt* indices,
-                                         const CeedScalar* r_v, CeedScalar* d_v) {
-  if (data.t_id_x < P1d) {
+template <int NUM_COMP, int COMP_STRIDE, int P_1d>
+inline __device__ void writeDofsOffset1d(SharedData_Hip &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
+                                         const CeedScalar *__restrict__ r_v, CeedScalar *__restrict__ d_v) {
+  if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
-    const CeedInt ind  = indices[node + elem * P1d];
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) atomicAdd(&d_v[ind + COMPSTRIDE * comp], r_v[comp]);
+    const CeedInt ind  = indices[node + elem * P_1d];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) atomicAdd(&d_v[ind + COMP_STRIDE * comp], r_v[comp]);
   }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
-template <int NCOMP, int P1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void writeDofsStrided1d(SharedData_Hip& data, const CeedInt elem, const CeedScalar* r_v, CeedScalar* d_v) {
-  if (data.t_id_x < P1d) {
+template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
+inline __device__ void writeDofsStrided1d(SharedData_Hip &data, const CeedInt elem, const CeedScalar *__restrict__ r_v,
+                                          CeedScalar *__restrict__ d_v) {
+  if (data.t_id_x < P_1d) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) d_v[ind + comp * STRIDES_COMP] += r_v[comp];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) d_v[ind + comp * STRIDES_COMP] += r_v[comp];
   }
 }
 
@@ -81,50 +86,55 @@ inline __device__ void writeDofsStrided1d(SharedData_Hip& data, const CeedInt el
 //------------------------------------------------------------------------------
 // L-vector -> E-vector, offsets provided
 //------------------------------------------------------------------------------
-template <int NCOMP, int COMPSTRIDE, int P1d>
-inline __device__ void readDofsOffset2d(SharedData_Hip& data, const CeedInt nnodes, const CeedInt elem, const CeedInt* indices, const CeedScalar* d_u,
-                                        CeedScalar* r_u) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d) {
-    const CeedInt node = data.t_id_x + data.t_id_y * P1d;
-    const CeedInt ind  = indices[node + elem * P1d * P1d];
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[comp] = d_u[ind + COMPSTRIDE * comp];
+template <int NUM_COMP, int COMP_STRIDE, int P_1d>
+inline __device__ void readDofsOffset2d(SharedData_Hip &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
+                                        const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
+    const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
+    const CeedInt ind  = indices[node + elem * P_1d * P_1d];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[comp] = d_u[ind + COMP_STRIDE * comp];
   }
 }
 
 //------------------------------------------------------------------------------
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
-template <int NCOMP, int P1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void readDofsStrided2d(SharedData_Hip& data, const CeedInt elem, const CeedScalar* d_u, CeedScalar* r_u) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d) {
-    const CeedInt node = data.t_id_x + data.t_id_y * P1d;
+template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
+inline __device__ void readDofsStrided2d(SharedData_Hip &data, const CeedInt elem, const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
+    const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[comp] = d_u[ind + comp * STRIDES_COMP];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[comp] = d_u[ind + comp * STRIDES_COMP];
   }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> L-vector, offsets provided
 //------------------------------------------------------------------------------
-template <int NCOMP, int COMPSTRIDE, int P1d>
-inline __device__ void writeDofsOffset2d(SharedData_Hip& data, const CeedInt nnodes, const CeedInt elem, const CeedInt* indices,
-                                         const CeedScalar* r_v, CeedScalar* d_v) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d) {
-    const CeedInt node = data.t_id_x + data.t_id_y * P1d;
-    const CeedInt ind  = indices[node + elem * P1d * P1d];
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) atomicAdd(&d_v[ind + COMPSTRIDE * comp], r_v[comp]);
+template <int NUM_COMP, int COMP_STRIDE, int P_1d>
+inline __device__ void writeDofsOffset2d(SharedData_Hip &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
+                                         const CeedScalar *__restrict__ r_v, CeedScalar *__restrict__ d_v) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
+    const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
+    const CeedInt ind  = indices[node + elem * P_1d * P_1d];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) atomicAdd(&d_v[ind + COMP_STRIDE * comp], r_v[comp]);
   }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
-template <int NCOMP, int P1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void writeDofsStrided2d(SharedData_Hip& data, const CeedInt elem, const CeedScalar* r_v, CeedScalar* d_v) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d) {
-    const CeedInt node = data.t_id_x + data.t_id_y * P1d;
+template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
+inline __device__ void writeDofsStrided2d(SharedData_Hip &data, const CeedInt elem, const CeedScalar *__restrict__ r_v,
+                                          CeedScalar *__restrict__ d_v) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d) {
+    const CeedInt node = data.t_id_x + data.t_id_y * P_1d;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) d_v[ind + comp * STRIDES_COMP] += r_v[comp];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) d_v[ind + comp * STRIDES_COMP] += r_v[comp];
   }
 }
 
@@ -135,104 +145,118 @@ inline __device__ void writeDofsStrided2d(SharedData_Hip& data, const CeedInt el
 //------------------------------------------------------------------------------
 // L-vector -> E-vector, offsets provided
 //------------------------------------------------------------------------------
-template <int NCOMP, int COMPSTRIDE, int P1d>
-inline __device__ void readDofsOffset3d(SharedData_Hip& data, const CeedInt nnodes, const CeedInt elem, const CeedInt* indices, const CeedScalar* d_u,
-                                        CeedScalar* r_u) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d)
-    for (CeedInt z = 0; z < P1d; ++z) {
-      const CeedInt node = data.t_id_x + data.t_id_y * P1d + z * P1d * P1d;
-      const CeedInt ind  = indices[node + elem * P1d * P1d * P1d];
-      for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[z + comp * P1d] = d_u[ind + COMPSTRIDE * comp];
+// TODO: remove "Dofs" and "Quads" in the following function names?
+//   - readDofsOffset3d -> readOffset3d ?
+//   - readDofsStrided3d -> readStrided3d ?
+//   - readSliceQuadsOffset3d -> readSliceOffset3d ?
+//   - readSliceQuadsStrided3d -> readSliceStrided3d ?
+//   - writeDofsOffset3d -> writeOffset3d ?
+//   - writeDofsStrided3d -> writeStrided3d ?
+template <int NUM_COMP, int COMP_STRIDE, int P_1d>
+inline __device__ void readDofsOffset3d(SharedData_Hip &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
+                                        const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d)
+    for (CeedInt z = 0; z < P_1d; z++) {
+      const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
+      const CeedInt ind  = indices[node + elem * P_1d * P_1d * P_1d];
+
+      for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[z + comp * P_1d] = d_u[ind + COMP_STRIDE * comp];
     }
 }
 
 //------------------------------------------------------------------------------
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
-template <int NCOMP, int P1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void readDofsStrided3d(SharedData_Hip& data, const CeedInt elem, const CeedScalar* d_u, CeedScalar* r_u) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d)
-    for (CeedInt z = 0; z < P1d; ++z) {
-      const CeedInt node = data.t_id_x + data.t_id_y * P1d + z * P1d * P1d;
+template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
+inline __device__ void readDofsStrided3d(SharedData_Hip &data, const CeedInt elem, const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d)
+    for (CeedInt z = 0; z < P_1d; z++) {
+      const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
       const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
-      for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[z + comp * P1d] = d_u[ind + comp * STRIDES_COMP];
+
+      for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[z + comp * P_1d] = d_u[ind + comp * STRIDES_COMP];
     }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> Q-vector, offests provided
 //------------------------------------------------------------------------------
-template <int NCOMP, int COMPSTRIDE, int Q1d>
-inline __device__ void readSliceQuadsOffset3d(SharedData_Hip& data, const CeedInt nquads, const CeedInt elem, const CeedInt q, const CeedInt* indices,
-                                              const CeedScalar* d_u, CeedScalar* r_u) {
-  if (data.t_id_x < Q1d && data.t_id_y < Q1d) {
-    const CeedInt node = data.t_id_x + data.t_id_y * Q1d + q * Q1d * Q1d;
-    const CeedInt ind  = indices[node + elem * Q1d * Q1d * Q1d];
-    ;
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[comp] = d_u[ind + COMPSTRIDE * comp];
+template <int NUM_COMP, int COMP_STRIDE, int Q_1d>
+inline __device__ void readSliceQuadsOffset3d(SharedData_Hip &data, const CeedInt nquads, const CeedInt elem, const CeedInt q,
+                                              const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ d_u, CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < Q_1d && data.t_id_y < Q_1d) {
+    const CeedInt node = data.t_id_x + data.t_id_y * Q_1d + q * Q_1d * Q_1d;
+    const CeedInt ind  = indices[node + elem * Q_1d * Q_1d * Q_1d];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[comp] = d_u[ind + COMP_STRIDE * comp];
   }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> Q-vector, strided
 //------------------------------------------------------------------------------
-template <int NCOMP, int Q1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void readSliceQuadsStrided3d(SharedData_Hip& data, const CeedInt elem, const CeedInt q, const CeedScalar* d_u, CeedScalar* r_u) {
-  if (data.t_id_x < Q1d && data.t_id_y < Q1d) {
-    const CeedInt node = data.t_id_x + data.t_id_y * Q1d + q * Q1d * Q1d;
+template <int NUM_COMP, int Q_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
+inline __device__ void readSliceQuadsStrided3d(SharedData_Hip &data, const CeedInt elem, const CeedInt q, const CeedScalar *__restrict__ d_u,
+                                               CeedScalar *__restrict__ r_u) {
+  if (data.t_id_x < Q_1d && data.t_id_y < Q_1d) {
+    const CeedInt node = data.t_id_x + data.t_id_y * Q_1d + q * Q_1d * Q_1d;
     const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) r_u[comp] = d_u[ind + comp * STRIDES_COMP];
+
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) r_u[comp] = d_u[ind + comp * STRIDES_COMP];
   }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> L-vector, offsets provided
 //------------------------------------------------------------------------------
-template <int NCOMP, int COMPSTRIDE, int P1d>
-inline __device__ void writeDofsOffset3d(SharedData_Hip& data, const CeedInt nnodes, const CeedInt elem, const CeedInt* indices,
-                                         const CeedScalar* r_v, CeedScalar* d_v) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d)
-    for (CeedInt z = 0; z < P1d; ++z) {
-      const CeedInt node = data.t_id_x + data.t_id_y * P1d + z * P1d * P1d;
-      const CeedInt ind  = indices[node + elem * P1d * P1d * P1d];
-      for (CeedInt comp = 0; comp < NCOMP; ++comp) atomicAdd(&d_v[ind + COMPSTRIDE * comp], r_v[z + comp * P1d]);
+template <int NUM_COMP, int COMP_STRIDE, int P_1d>
+inline __device__ void writeDofsOffset3d(SharedData_Hip &data, const CeedInt num_nodes, const CeedInt elem, const CeedInt *__restrict__ indices,
+                                         const CeedScalar *__restrict__ r_v, CeedScalar *__restrict__ d_v) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d)
+    for (CeedInt z = 0; z < P_1d; z++) {
+      const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
+      const CeedInt ind  = indices[node + elem * P_1d * P_1d * P_1d];
+
+      for (CeedInt comp = 0; comp < NUM_COMP; comp++) atomicAdd(&d_v[ind + COMP_STRIDE * comp], r_v[z + comp * P_1d]);
     }
 }
 
 //------------------------------------------------------------------------------
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
-template <int NCOMP, int P1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
-inline __device__ void writeDofsStrided3d(SharedData_Hip& data, const CeedInt elem, const CeedScalar* r_v, CeedScalar* d_v) {
-  if (data.t_id_x < P1d && data.t_id_y < P1d)
-    for (CeedInt z = 0; z < P1d; ++z) {
-      const CeedInt node = data.t_id_x + data.t_id_y * P1d + z * P1d * P1d;
+template <int NUM_COMP, int P_1d, int STRIDES_NODE, int STRIDES_COMP, int STRIDES_ELEM>
+inline __device__ void writeDofsStrided3d(SharedData_Hip &data, const CeedInt elem, const CeedScalar *__restrict__ r_v,
+                                          CeedScalar *__restrict__ d_v) {
+  if (data.t_id_x < P_1d && data.t_id_y < P_1d)
+    for (CeedInt z = 0; z < P_1d; z++) {
+      const CeedInt node = data.t_id_x + data.t_id_y * P_1d + z * P_1d * P_1d;
       const CeedInt ind  = node * STRIDES_NODE + elem * STRIDES_ELEM;
-      for (CeedInt comp = 0; comp < NCOMP; ++comp) d_v[ind + comp * STRIDES_COMP] += r_v[z + comp * P1d];
+
+      for (CeedInt comp = 0; comp < NUM_COMP; comp++) d_v[ind + comp * STRIDES_COMP] += r_v[z + comp * P_1d];
     }
 }
 
 //------------------------------------------------------------------------------
 // 3D collocated derivatives computation
 //------------------------------------------------------------------------------
-template <int NCOMP, int Q1d>
-inline __device__ void gradCollo3d(SharedData_Hip& data, const CeedInt q, const CeedScalar* __restrict__ r_U, const CeedScalar* c_G,
-                                   CeedScalar* __restrict__ r_V) {
-  if (data.t_id_x < Q1d && data.t_id_y < Q1d) {
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) {
-      data.slice[data.t_id_x + data.t_id_y * T_1D] = r_U[q + comp * Q1d];
+template <int NUM_COMP, int Q_1d>
+inline __device__ void gradCollo3d(SharedData_Hip &data, const CeedInt q, const CeedScalar *__restrict__ r_U, const CeedScalar *c_G,
+                                   CeedScalar *__restrict__ r_V) {
+  if (data.t_id_x < Q_1d && data.t_id_y < Q_1d) {
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
+      data.slice[data.t_id_x + data.t_id_y * T_1D] = r_U[q + comp * Q_1d];
       __syncthreads();
       // X derivative
-      r_V[comp + 0 * NCOMP] = 0.0;
-      for (CeedInt i = 0; i < Q1d; ++i)
-        r_V[comp + 0 * NCOMP] += c_G[i + data.t_id_x * Q1d] * data.slice[i + data.t_id_y * T_1D];  // Contract x direction (X derivative)
+      r_V[comp + 0 * NUM_COMP] = 0.0;
+      for (CeedInt i = 0; i < Q_1d; i++)
+        r_V[comp + 0 * NUM_COMP] += c_G[i + data.t_id_x * Q_1d] * data.slice[i + data.t_id_y * T_1D];  // Contract x direction (X derivative)
       // Y derivative
-      r_V[comp + 1 * NCOMP] = 0.0;
-      for (CeedInt i = 0; i < Q1d; ++i)
-        r_V[comp + 1 * NCOMP] += c_G[i + data.t_id_y * Q1d] * data.slice[data.t_id_x + i * T_1D];  // Contract y direction (Y derivative)
+      r_V[comp + 1 * NUM_COMP] = 0.0;
+      for (CeedInt i = 0; i < Q_1d; i++)
+        r_V[comp + 1 * NUM_COMP] += c_G[i + data.t_id_y * Q_1d] * data.slice[data.t_id_x + i * T_1D];  // Contract y direction (Y derivative)
       // Z derivative
-      r_V[comp + 2 * NCOMP] = 0.0;
-      for (CeedInt i = 0; i < Q1d; ++i) r_V[comp + 2 * NCOMP] += c_G[i + q * Q1d] * r_U[i + comp * Q1d];  // Contract z direction (Z derivative)
+      r_V[comp + 2 * NUM_COMP] = 0.0;
+      for (CeedInt i = 0; i < Q_1d; i++) r_V[comp + 2 * NUM_COMP] += c_G[i + q * Q_1d] * r_U[i + comp * Q_1d];  // Contract z direction (Z derivative)
       __syncthreads();
     }
   }
@@ -241,26 +265,26 @@ inline __device__ void gradCollo3d(SharedData_Hip& data, const CeedInt q, const
 //------------------------------------------------------------------------------
 // 3D collocated derivatives transpose
 //------------------------------------------------------------------------------
-template <int NCOMP, int Q1d>
-inline __device__ void gradColloTranspose3d(SharedData_Hip& data, const CeedInt q, const CeedScalar* __restrict__ r_U, const CeedScalar* c_G,
-                                            CeedScalar* __restrict__ r_V) {
-  if (data.t_id_x < Q1d && data.t_id_y < Q1d) {
-    for (CeedInt comp = 0; comp < NCOMP; ++comp) {
+template <int NUM_COMP, int Q_1d>
+inline __device__ void gradColloTranspose3d(SharedData_Hip &data, const CeedInt q, const CeedScalar *__restrict__ r_U, const CeedScalar *c_G,
+                                            CeedScalar *__restrict__ r_V) {
+  if (data.t_id_x < Q_1d && data.t_id_y < Q_1d) {
+    for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
       // X derivative
-      data.slice[data.t_id_x + data.t_id_y * T_1D] = r_U[comp + 0 * NCOMP];
+      data.slice[data.t_id_x + data.t_id_y * T_1D] = r_U[comp + 0 * NUM_COMP];
       __syncthreads();
-      for (CeedInt i = 0; i < Q1d; ++i)
-        r_V[q + comp * Q1d] += c_G[data.t_id_x + i * Q1d] * data.slice[i + data.t_id_y * T_1D];  // Contract x direction (X derivative)
+      for (CeedInt i = 0; i < Q_1d; i++)
+        r_V[q + comp * Q_1d] += c_G[data.t_id_x + i * Q_1d] * data.slice[i + data.t_id_y * T_1D];  // Contract x direction (X derivative)
       __syncthreads();
       // Y derivative
-      data.slice[data.t_id_x + data.t_id_y * T_1D] = r_U[comp + 1 * NCOMP];
+      data.slice[data.t_id_x + data.t_id_y * T_1D] = r_U[comp + 1 * NUM_COMP];
       __syncthreads();
-      for (CeedInt i = 0; i < Q1d; ++i)
-        r_V[q + comp * Q1d] += c_G[data.t_id_y + i * Q1d] * data.slice[data.t_id_x + i * T_1D];  // Contract y direction (Y derivative)
+      for (CeedInt i = 0; i < Q_1d; i++)
+        r_V[q + comp * Q_1d] += c_G[data.t_id_y + i * Q_1d] * data.slice[data.t_id_x + i * T_1D];  // Contract y direction (Y derivative)
       __syncthreads();
       // Z derivative
-      for (CeedInt i = 0; i < Q1d; ++i)
-        r_V[i + comp * Q1d] += c_G[i + q * Q1d] * r_U[comp + 2 * NCOMP];  // PARTIAL contract z direction (Z derivative)
+      for (CeedInt i = 0; i < Q_1d; i++)
+        r_V[i + comp * Q_1d] += c_G[i + q * Q_1d] * r_U[comp + 2 * NUM_COMP];  // PARTIAL contract z direction (Z derivative)
     }
   }
 }
diff --git a/include/ceed/jit-source/hip/hip-ref-basis-tensor.h b/include/ceed/jit-source/hip/hip-ref-basis-tensor.h
index 0f6958ae..6abcb5de 100644
--- a/include/ceed/jit-source/hip/hip-ref-basis-tensor.h
+++ b/include/ceed/jit-source/hip/hip-ref-basis-tensor.h
@@ -46,31 +46,30 @@ extern "C" __global__ void Interp(const CeedInt num_elem, const CeedInt transpos
     for (CeedInt comp = 0; comp < BASIS_NUM_COMP; comp++) {
       const CeedScalar *cur_u = u + elem * u_stride + comp * u_comp_stride;
       CeedScalar       *cur_v = v + elem * v_stride + comp * v_comp_stride;
+      CeedInt           pre   = u_size;
+      CeedInt           post  = 1;
+
       for (CeedInt k = i; k < u_size; k += blockDim.x) {
         s_buffer_1[k] = cur_u[k];
       }
-      CeedInt pre  = u_size;
-      CeedInt post = 1;
       for (CeedInt d = 0; d < BASIS_DIM; d++) {
         __syncthreads();
         // Update buffers used
         pre /= P;
-        const CeedScalar *in  = d % 2 ? s_buffer_2 : s_buffer_1;
-        CeedScalar       *out = d == BASIS_DIM - 1 ? cur_v : (d % 2 ? s_buffer_1 : s_buffer_2);
+        const CeedScalar *in       = d % 2 ? s_buffer_2 : s_buffer_1;
+        CeedScalar       *out      = d == BASIS_DIM - 1 ? cur_v : (d % 2 ? s_buffer_1 : s_buffer_2);
+        const CeedInt     writeLen = pre * post * Q;
 
         // Contract along middle index
-        const CeedInt writeLen = pre * post * Q;
         for (CeedInt k = i; k < writeLen; k += blockDim.x) {
-          const CeedInt c = k % post;
-          const CeedInt j = (k / post) % Q;
-          const CeedInt a = k / (post * Q);
+          const CeedInt c  = k % post;
+          const CeedInt j  = (k / post) % Q;
+          const CeedInt a  = k / (post * Q);
+          CeedScalar    vk = 0;
 
-          CeedScalar vk = 0;
           for (CeedInt b = 0; b < P; b++) vk += s_interp_1d[j * stride_0 + b * stride_1] * in[(a * P + b) * post + c];
-
           out[k] = vk;
         }
-
         post *= Q;
       }
     }
@@ -114,27 +113,27 @@ extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose,
         CeedInt           post  = 1;
         const CeedScalar *cur_u = u + elem * u_stride + dim_1 * u_dim_stride + comp * u_comp_stride;
         CeedScalar       *cur_v = v + elem * v_stride + dim_1 * v_dim_stride + comp * v_comp_stride;
+
         for (CeedInt dim_2 = 0; dim_2 < BASIS_DIM; dim_2++) {
           __syncthreads();
           // Update buffers used
           pre /= P;
-          const CeedScalar *op  = dim_1 == dim_2 ? s_grad_1d : s_interp_1d;
-          const CeedScalar *in  = dim_2 == 0 ? cur_u : (dim_2 % 2 ? s_buffer_2 : s_buffer_1);
-          CeedScalar       *out = dim_2 == BASIS_DIM - 1 ? cur_v : (dim_2 % 2 ? s_buffer_1 : s_buffer_2);
+          const CeedScalar *op       = dim_1 == dim_2 ? s_grad_1d : s_interp_1d;
+          const CeedScalar *in       = dim_2 == 0 ? cur_u : (dim_2 % 2 ? s_buffer_2 : s_buffer_1);
+          CeedScalar       *out      = dim_2 == BASIS_DIM - 1 ? cur_v : (dim_2 % 2 ? s_buffer_1 : s_buffer_2);
+          const CeedInt     writeLen = pre * post * Q;
 
           // Contract along middle index
-          const CeedInt writeLen = pre * post * Q;
           for (CeedInt k = i; k < writeLen; k += blockDim.x) {
             const CeedInt c   = k % post;
             const CeedInt j   = (k / post) % Q;
             const CeedInt a   = k / (post * Q);
             CeedScalar    v_k = 0;
-            for (CeedInt b = 0; b < P; b++) v_k += op[j * stride_0 + b * stride_1] * in[(a * P + b) * post + c];
 
+            for (CeedInt b = 0; b < P; b++) v_k += op[j * stride_0 + b * stride_1] * in[(a * P + b) * post + c];
             if (transpose && dim_2 == BASIS_DIM - 1) out[k] += v_k;
             else out[k] = v_k;
           }
-
           post *= Q;
         }
       }
@@ -147,8 +146,10 @@ extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose,
 //------------------------------------------------------------------------------
 __device__ void Weight1d(const CeedInt num_elem, const CeedScalar *q_weight_1d, CeedScalar *w) {
   const CeedInt i = threadIdx.x;
+
   if (i < BASIS_Q_1D) {
     const size_t elem = blockIdx.x;
+
     if (elem < num_elem) w[elem * BASIS_Q_1D + i] = q_weight_1d[i];
   }
 }
@@ -159,11 +160,14 @@ __device__ void Weight1d(const CeedInt num_elem, const CeedScalar *q_weight_1d,
 __device__ void Weight2d(const CeedInt num_elem, const CeedScalar *q_weight_1d, CeedScalar *w) {
   const CeedInt i = threadIdx.x;
   const CeedInt j = threadIdx.y;
+
   if (i < BASIS_Q_1D && j < BASIS_Q_1D) {
     const size_t elem = blockIdx.x;
+
     if (elem < num_elem) {
       const size_t ind = (elem * BASIS_Q_1D + j) * BASIS_Q_1D + i;
-      w[ind]           = q_weight_1d[i] * q_weight_1d[j];
+
+      w[ind] = q_weight_1d[i] * q_weight_1d[j];
     }
   }
 }
@@ -174,12 +178,15 @@ __device__ void Weight2d(const CeedInt num_elem, const CeedScalar *q_weight_1d,
 __device__ void Weight3d(const CeedInt num_elem, const CeedScalar *q_weight_1d, CeedScalar *w) {
   const CeedInt i = threadIdx.x;
   const CeedInt j = threadIdx.y;
+
   if (i < BASIS_Q_1D && j < BASIS_Q_1D) {
     const size_t elem = blockIdx.x;
+
     if (elem < num_elem) {
       for (CeedInt k = 0; k < BASIS_Q_1D; k++) {
         const size_t ind = ((elem * BASIS_Q_1D + k) * BASIS_Q_1D + j) * BASIS_Q_1D + i;
-        w[ind]           = q_weight_1d[i] * q_weight_1d[j] * q_weight_1d[k];
+
+        w[ind] = q_weight_1d[i] * q_weight_1d[j] * q_weight_1d[k];
       }
     }
   }
diff --git a/include/ceed/jit-source/hip/hip-ref-operator-assemble-diagonal.h b/include/ceed/jit-source/hip/hip-ref-operator-assemble-diagonal.h
index 8270c73d..fcd8df29 100644
--- a/include/ceed/jit-source/hip/hip-ref-operator-assemble-diagonal.h
+++ b/include/ceed/jit-source/hip/hip-ref-operator-assemble-diagonal.h
@@ -12,80 +12,106 @@
 
 #include <ceed.h>
 
-#if CEEDSIZE
+#if USE_CEEDSIZE
 typedef CeedSize IndexType;
 #else
 typedef CeedInt IndexType;
 #endif
 
 //------------------------------------------------------------------------------
-// Get Basis Emode Pointer
+// Get basis pointer
 //------------------------------------------------------------------------------
-extern "C" __device__ void CeedOperatorGetBasisPointer_Hip(const CeedScalar **basisptr, CeedEvalMode emode, const CeedScalar *identity,
-                                                           const CeedScalar *interp, const CeedScalar *grad) {
-  switch (emode) {
+static __device__ __inline__ void GetBasisPointer(const CeedScalar **basis_ptr, CeedEvalMode eval_modes, const CeedScalar *identity,
+                                                  const CeedScalar *interp, const CeedScalar *grad, const CeedScalar *div, const CeedScalar *curl) {
+  switch (eval_modes) {
     case CEED_EVAL_NONE:
-      *basisptr = identity;
+      *basis_ptr = identity;
       break;
     case CEED_EVAL_INTERP:
-      *basisptr = interp;
+      *basis_ptr = interp;
       break;
     case CEED_EVAL_GRAD:
-      *basisptr = grad;
+      *basis_ptr = grad;
       break;
-    case CEED_EVAL_WEIGHT:
     case CEED_EVAL_DIV:
+      *basis_ptr = div;
+      break;
     case CEED_EVAL_CURL:
-      break;  // Caught by QF Assembly
+      *basis_ptr = curl;
+      break;
+    case CEED_EVAL_WEIGHT:
+      break;  // Caught by QF assembly
   }
 }
 
 //------------------------------------------------------------------------------
 // Core code for diagonal assembly
 //------------------------------------------------------------------------------
-__device__ void diagonalCore(const CeedInt nelem, const bool pointBlock, const CeedScalar *identity, const CeedScalar *interpin,
-                             const CeedScalar *gradin, const CeedScalar *interpout, const CeedScalar *gradout, const CeedEvalMode *emodein,
-                             const CeedEvalMode *emodeout, const CeedScalar *__restrict__ assembledqfarray, CeedScalar *__restrict__ elemdiagarray) {
-  const int tid = threadIdx.x;  // running with P threads, tid is evec node
-  if (tid >= NNODES) return;
+static __device__ __inline__ void DiagonalCore(const CeedInt num_elem, const bool is_point_block, const CeedScalar *identity,
+                                               const CeedScalar *interp_in, const CeedScalar *grad_in, const CeedScalar *div_in,
+                                               const CeedScalar *curl_in, const CeedScalar *interp_out, const CeedScalar *grad_out,
+                                               const CeedScalar *div_out, const CeedScalar *curl_out, const CeedEvalMode *eval_modes_in,
+                                               const CeedEvalMode *eval_modes_out, const CeedScalar *__restrict__ assembled_qf_array,
+                                               CeedScalar *__restrict__ elem_diag_array) {
+  const int tid = threadIdx.x;  // Running with P threads
+
+  if (tid >= NUM_NODES) return;
 
   // Compute the diagonal of B^T D B
   // Each element
-  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < nelem; e += gridDim.x * blockDim.z) {
-    IndexType dout = -1;
+  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < num_elem; e += gridDim.x * blockDim.z) {
     // Each basis eval mode pair
-    for (IndexType eout = 0; eout < NUMEMODEOUT; eout++) {
-      const CeedScalar *bt = NULL;
-      if (emodeout[eout] == CEED_EVAL_GRAD) dout += 1;
-      CeedOperatorGetBasisPointer_Hip(&bt, emodeout[eout], identity, interpout, &gradout[dout * NQPTS * NNODES]);
-      IndexType din = -1;
-      for (IndexType ein = 0; ein < NUMEMODEIN; ein++) {
+    IndexType    d_out               = 0;
+    CeedEvalMode eval_modes_out_prev = CEED_EVAL_NONE;
+
+    for (IndexType e_out = 0; e_out < NUM_EVAL_MODES_OUT; e_out++) {
+      IndexType         d_in               = 0;
+      CeedEvalMode      eval_modes_in_prev = CEED_EVAL_NONE;
+      const CeedScalar *b_t                = NULL;
+
+      GetBasisPointer(&b_t, eval_modes_out[e_out], identity, interp_out, grad_out, div_out, curl_out);
+      if (e_out == 0 || eval_modes_out[e_out] != eval_modes_out_prev) d_out = 0;
+      else b_t = &b_t[(++d_out) * NUM_QPTS * NUM_NODES];
+      eval_modes_out_prev = eval_modes_out[e_out];
+
+      for (IndexType e_in = 0; e_in < NUM_EVAL_MODES_IN; e_in++) {
         const CeedScalar *b = NULL;
-        if (emodein[ein] == CEED_EVAL_GRAD) din += 1;
-        CeedOperatorGetBasisPointer_Hip(&b, emodein[ein], identity, interpin, &gradin[din * NQPTS * NNODES]);
+
+        GetBasisPointer(&b, eval_modes_in[e_in], identity, interp_in, grad_in, div_in, curl_in);
+        if (e_in == 0 || eval_modes_in[e_in] != eval_modes_in_prev) d_in = 0;
+        else b = &b[(++d_in) * NUM_QPTS * NUM_NODES];
+        eval_modes_in_prev = eval_modes_in[e_in];
+
         // Each component
-        for (IndexType compOut = 0; compOut < NCOMP; compOut++) {
+        for (IndexType comp_out = 0; comp_out < NUM_COMP; comp_out++) {
           // Each qpoint/node pair
-          if (pointBlock) {
-            // Point Block Diagonal
-            for (IndexType compIn = 0; compIn < NCOMP; compIn++) {
-              CeedScalar evalue = 0.;
-              for (IndexType q = 0; q < NQPTS; q++) {
-                const CeedScalar qfvalue =
-                    assembledqfarray[((((ein * NCOMP + compIn) * NUMEMODEOUT + eout) * NCOMP + compOut) * nelem + e) * NQPTS + q];
-                evalue += bt[q * NNODES + tid] * qfvalue * b[q * NNODES + tid];
+          if (is_point_block) {
+            // Point block diagonal
+            for (IndexType comp_in = 0; comp_in < NUM_COMP; comp_in++) {
+              CeedScalar e_value = 0.;
+
+              for (IndexType q = 0; q < NUM_QPTS; q++) {
+                const CeedScalar qf_value =
+                    assembled_qf_array[((((e_in * NUM_COMP + comp_in) * NUM_EVAL_MODES_OUT + e_out) * NUM_COMP + comp_out) * num_elem + e) *
+                                           NUM_QPTS +
+                                       q];
+
+                e_value += b_t[q * NUM_NODES + tid] * qf_value * b[q * NUM_NODES + tid];
               }
-              elemdiagarray[((compOut * NCOMP + compIn) * nelem + e) * NNODES + tid] += evalue;
+              elem_diag_array[((comp_out * NUM_COMP + comp_in) * num_elem + e) * NUM_NODES + tid] += e_value;
             }
           } else {
-            // Diagonal Only
-            CeedScalar evalue = 0.;
-            for (IndexType q = 0; q < NQPTS; q++) {
-              const CeedScalar qfvalue =
-                  assembledqfarray[((((ein * NCOMP + compOut) * NUMEMODEOUT + eout) * NCOMP + compOut) * nelem + e) * NQPTS + q];
-              evalue += bt[q * NNODES + tid] * qfvalue * b[q * NNODES + tid];
+            // Diagonal only
+            CeedScalar e_value = 0.;
+
+            for (IndexType q = 0; q < NUM_QPTS; q++) {
+              const CeedScalar qf_value =
+                  assembled_qf_array[((((e_in * NUM_COMP + comp_out) * NUM_EVAL_MODES_OUT + e_out) * NUM_COMP + comp_out) * num_elem + e) * NUM_QPTS +
+                                     q];
+
+              e_value += b_t[q * NUM_NODES + tid] * qf_value * b[q * NUM_NODES + tid];
             }
-            elemdiagarray[(compOut * nelem + e) * NNODES + tid] += evalue;
+            elem_diag_array[(comp_out * num_elem + e) * NUM_NODES + tid] += e_value;
           }
         }
       }
@@ -96,21 +122,25 @@ __device__ void diagonalCore(const CeedInt nelem, const bool pointBlock, const C
 //------------------------------------------------------------------------------
 // Linear diagonal
 //------------------------------------------------------------------------------
-extern "C" __global__ void linearDiagonal(const CeedInt nelem, const CeedScalar *identity, const CeedScalar *interpin, const CeedScalar *gradin,
-                                          const CeedScalar *interpout, const CeedScalar *gradout, const CeedEvalMode *emodein,
-                                          const CeedEvalMode *emodeout, const CeedScalar *__restrict__ assembledqfarray,
-                                          CeedScalar *__restrict__ elemdiagarray) {
-  diagonalCore(nelem, false, identity, interpin, gradin, interpout, gradout, emodein, emodeout, assembledqfarray, elemdiagarray);
+extern "C" __global__ void LinearDiagonal(const CeedInt num_elem, const CeedScalar *identity, const CeedScalar *interp_in, const CeedScalar *grad_in,
+                                          const CeedScalar *div_in, const CeedScalar *curl_in, const CeedScalar *interp_out,
+                                          const CeedScalar *grad_out, const CeedScalar *div_out, const CeedScalar *curl_out,
+                                          const CeedEvalMode *eval_modes_in, const CeedEvalMode *eval_modes_out,
+                                          const CeedScalar *__restrict__ assembled_qf_array, CeedScalar *__restrict__ elem_diag_array) {
+  DiagonalCore(num_elem, false, identity, interp_in, grad_in, div_in, curl_in, interp_out, grad_out, div_out, curl_out, eval_modes_in, eval_modes_out,
+               assembled_qf_array, elem_diag_array);
 }
 
 //------------------------------------------------------------------------------
 // Linear point block diagonal
 //------------------------------------------------------------------------------
-extern "C" __global__ void linearPointBlockDiagonal(const CeedInt nelem, const CeedScalar *identity, const CeedScalar *interpin,
-                                                    const CeedScalar *gradin, const CeedScalar *interpout, const CeedScalar *gradout,
-                                                    const CeedEvalMode *emodein, const CeedEvalMode *emodeout,
-                                                    const CeedScalar *__restrict__ assembledqfarray, CeedScalar *__restrict__ elemdiagarray) {
-  diagonalCore(nelem, true, identity, interpin, gradin, interpout, gradout, emodein, emodeout, assembledqfarray, elemdiagarray);
+extern "C" __global__ void LinearPointBlockDiagonal(const CeedInt num_elem, const CeedScalar *identity, const CeedScalar *interp_in,
+                                                    const CeedScalar *grad_in, const CeedScalar *div_in, const CeedScalar *curl_in,
+                                                    const CeedScalar *interp_out, const CeedScalar *grad_out, const CeedScalar *div_out,
+                                                    const CeedScalar *curl_out, const CeedEvalMode *eval_modes_in, const CeedEvalMode *eval_modes_out,
+                                                    const CeedScalar *__restrict__ assembled_qf_array, CeedScalar *__restrict__ elem_diag_array) {
+  DiagonalCore(num_elem, true, identity, interp_in, grad_in, div_in, curl_in, interp_out, grad_out, div_out, curl_out, eval_modes_in, eval_modes_out,
+               assembled_qf_array, elem_diag_array);
 }
 
 //------------------------------------------------------------------------------
diff --git a/include/ceed/jit-source/hip/hip-ref-operator-assemble.h b/include/ceed/jit-source/hip/hip-ref-operator-assemble.h
index 005fa6f7..a0c21f9d 100644
--- a/include/ceed/jit-source/hip/hip-ref-operator-assemble.h
+++ b/include/ceed/jit-source/hip/hip-ref-operator-assemble.h
@@ -12,107 +12,99 @@
 
 #include <ceed.h>
 
-#if CEEDSIZE
+#if USE_CEEDSIZE
 typedef CeedSize IndexType;
 #else
 typedef CeedInt IndexType;
 #endif
 
 //------------------------------------------------------------------------------
-// Matrix assembly kernel for low-order elements (2D thread block)
+// Matrix assembly kernel
 //------------------------------------------------------------------------------
 extern "C" __launch_bounds__(BLOCK_SIZE) __global__
-    void linearAssemble(const CeedScalar *B_in, const CeedScalar *B_out, const CeedScalar *__restrict__ qf_array,
-                        CeedScalar *__restrict__ values_array) {
-  // This kernel assumes B_in and B_out have the same number of quadrature points and basis points.
-  // TODO: expand to more general cases
-  const int i = threadIdx.x;  // The output row index of each B^TDB operation
-  const int l = threadIdx.y;  // The output column index of each B^TDB operation
+    void LinearAssemble(const CeedInt num_elem, const CeedScalar *B_in, const CeedScalar *B_out, const bool *orients_in,
+                        const CeedInt8 *curl_orients_in, const bool *orients_out, const CeedInt8 *curl_orients_out,
+                        const CeedScalar *__restrict__ qf_array, CeedScalar *__restrict__ values_array) {
+  extern __shared__ CeedScalar s_CT[];
+  CeedScalar                  *s_C = s_CT + NUM_NODES_OUT * NUM_NODES_IN;
+
+  const int l = threadIdx.x;  // The output column index of each B^T D B operation
                               // such that we have (Bout^T)_ij D_jk Bin_kl = C_il
 
-  // Strides for final output ordering, determined by the reference (interface) implementation of the symbolic assembly, slowest --> fastest: element,
+  // Strides for final output ordering, determined by the reference (interface) implementation of the symbolic assembly, slowest --> fastest: e,
   // comp_in, comp_out, node_row, node_col
-  const IndexType comp_out_stride = NNODES * NNODES;
-  const IndexType comp_in_stride  = comp_out_stride * NCOMP;
-  const IndexType e_stride        = comp_in_stride * NCOMP;
-  // Strides for QF array, slowest --> fastest:  emode_in, comp_in, emode_out, comp_out, elem, qpt
-  const IndexType qe_stride         = NQPTS;
-  const IndexType qcomp_out_stride  = NELEM * qe_stride;
-  const IndexType qemode_out_stride = qcomp_out_stride * NCOMP;
-  const IndexType qcomp_in_stride   = qemode_out_stride * NUMEMODEOUT;
-  const IndexType qemode_in_stride  = qcomp_in_stride * NCOMP;
+  const IndexType comp_out_stride = NUM_NODES_OUT * NUM_NODES_IN;
+  const IndexType comp_in_stride  = comp_out_stride * NUM_COMP_OUT;
+  const IndexType e_stride        = comp_in_stride * NUM_COMP_IN;
+
+  // Strides for QF array, slowest --> fastest: e_in, comp_in, e_out, comp_out, e, q
+  const IndexType q_e_stride             = NUM_QPTS;
+  const IndexType q_comp_out_stride      = num_elem * q_e_stride;
+  const IndexType q_eval_mode_out_stride = q_comp_out_stride * NUM_COMP_OUT;
+  const IndexType q_comp_in_stride       = q_eval_mode_out_stride * NUM_EVAL_MODES_OUT;
+  const IndexType q_eval_mode_in_stride  = q_comp_in_stride * NUM_COMP_IN;
 
   // Loop over each element (if necessary)
-  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < NELEM; e += gridDim.x * blockDim.z) {
-    for (IndexType comp_in = 0; comp_in < NCOMP; comp_in++) {
-      for (IndexType comp_out = 0; comp_out < NCOMP; comp_out++) {
-        CeedScalar result        = 0.0;
-        IndexType  qf_index_comp = qcomp_in_stride * comp_in + qcomp_out_stride * comp_out + qe_stride * e;
-        for (IndexType emode_in = 0; emode_in < NUMEMODEIN; emode_in++) {
-          IndexType b_in_index = emode_in * NQPTS * NNODES;
-          for (IndexType emode_out = 0; emode_out < NUMEMODEOUT; emode_out++) {
-            IndexType b_out_index = emode_out * NQPTS * NNODES;
-            IndexType qf_index    = qf_index_comp + qemode_out_stride * emode_out + qemode_in_stride * emode_in;
-            // Perform the B^T D B operation for this 'chunk' of D (the qf_array)
-            for (IndexType j = 0; j < NQPTS; j++) {
-              result += B_out[b_out_index + j * NNODES + i] * qf_array[qf_index + j] * B_in[b_in_index + j * NNODES + l];
-            }
-          }  // end of emode_out
-        }    // end of emode_in
-        IndexType val_index     = comp_in_stride * comp_in + comp_out_stride * comp_out + e_stride * e + NNODES * i + l;
-        values_array[val_index] = result;
-      }  // end of out component
-    }    // end of in component
-  }      // end of element loop
-}
+  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < num_elem; e += gridDim.x * blockDim.z) {
+    for (IndexType comp_in = 0; comp_in < NUM_COMP_IN; comp_in++) {
+      for (IndexType comp_out = 0; comp_out < NUM_COMP_OUT; comp_out++) {
+        for (IndexType i = threadIdx.y; i < NUM_NODES_OUT; i += BLOCK_SIZE_Y) {
+          CeedScalar result        = 0.0;
+          IndexType  qf_index_comp = q_comp_in_stride * comp_in + q_comp_out_stride * comp_out + q_e_stride * e;
 
-//------------------------------------------------------------------------------
-// Fallback kernel for larger orders (1D thread block)
-//------------------------------------------------------------------------------
-extern "C" __launch_bounds__(BLOCK_SIZE) __global__
-    void linearAssembleFallback(const CeedScalar *B_in, const CeedScalar *B_out, const CeedScalar *__restrict__ qf_array,
-                                CeedScalar *__restrict__ values_array) {
-  // This kernel assumes B_in and B_out have the same number of quadrature points and basis points.
-  // TODO: expand to more general cases
-  const int l = threadIdx.x;  // The output column index of each B^TDB operation
-                              // such that we have (Bout^T)_ij D_jk Bin_kl = C_il
+          for (IndexType e_in = 0; e_in < NUM_EVAL_MODES_IN; e_in++) {
+            IndexType b_in_index = e_in * NUM_QPTS * NUM_NODES_IN;
 
-  // Strides for final output ordering, determined by the reference (interface) implementation of the symbolic assembly, slowest --> fastest: element,
-  // comp_in, comp_out, node_row, node_col
-  const IndexType comp_out_stride = NNODES * NNODES;
-  const IndexType comp_in_stride  = comp_out_stride * NCOMP;
-  const IndexType e_stride        = comp_in_stride * NCOMP;
-  // Strides for QF array, slowest --> fastest:  emode_in, comp_in, emode_out, comp_out, elem, qpt
-  const IndexType qe_stride         = NQPTS;
-  const IndexType qcomp_out_stride  = NELEM * qe_stride;
-  const IndexType qemode_out_stride = qcomp_out_stride * NCOMP;
-  const IndexType qcomp_in_stride   = qemode_out_stride * NUMEMODEOUT;
-  const IndexType qemode_in_stride  = qcomp_in_stride * NCOMP;
+            for (IndexType e_out = 0; e_out < NUM_EVAL_MODES_OUT; e_out++) {
+              IndexType b_out_index = e_out * NUM_QPTS * NUM_NODES_OUT;
+              IndexType qf_index    = qf_index_comp + q_eval_mode_out_stride * e_out + q_eval_mode_in_stride * e_in;
 
-  // Loop over each element (if necessary)
-  for (IndexType e = blockIdx.x * blockDim.z + threadIdx.z; e < NELEM; e += gridDim.x * blockDim.z) {
-    for (IndexType comp_in = 0; comp_in < NCOMP; comp_in++) {
-      for (IndexType comp_out = 0; comp_out < NCOMP; comp_out++) {
-        for (IndexType i = 0; i < NNODES; i++) {
-          CeedScalar result        = 0.0;
-          IndexType  qf_index_comp = qcomp_in_stride * comp_in + qcomp_out_stride * comp_out + qe_stride * e;
-          for (IndexType emode_in = 0; emode_in < NUMEMODEIN; emode_in++) {
-            IndexType b_in_index = emode_in * NQPTS * NNODES;
-            for (IndexType emode_out = 0; emode_out < NUMEMODEOUT; emode_out++) {
-              IndexType b_out_index = emode_out * NQPTS * NNODES;
-              IndexType qf_index    = qf_index_comp + qemode_out_stride * emode_out + qemode_in_stride * emode_in;
               // Perform the B^T D B operation for this 'chunk' of D (the qf_array)
-              for (IndexType j = 0; j < NQPTS; j++) {
-                result += B_out[b_out_index + j * NNODES + i] * qf_array[qf_index + j] * B_in[b_in_index + j * NNODES + l];
+              for (IndexType j = 0; j < NUM_QPTS; j++) {
+                result += B_out[b_out_index + j * NUM_NODES_OUT + i] * qf_array[qf_index + j] * B_in[b_in_index + j * NUM_NODES_IN + l];
               }
-            }  // end of emode_out
-          }    // end of emode_in
-          IndexType val_index     = comp_in_stride * comp_in + comp_out_stride * comp_out + e_stride * e + NNODES * i + l;
-          values_array[val_index] = result;
+            }  // end of out eval mode
+          }    // end of in eval mode
+          if (orients_in) {
+            result *= orients_in[NUM_NODES_IN * e + l] ? -1.0 : 1.0;
+          }
+          if (orients_out) {
+            result *= orients_out[NUM_NODES_OUT * e + i] ? -1.0 : 1.0;
+          }
+          if (!curl_orients_in && !curl_orients_out) {
+            IndexType val_index = e_stride * e + comp_in_stride * comp_in + comp_out_stride * comp_out + NUM_NODES_IN * i + l;
+
+            values_array[val_index] = result;
+          } else if (curl_orients_in) {
+            s_C[NUM_NODES_IN * threadIdx.y + l] = result;
+            __syncthreads();
+            s_CT[NUM_NODES_IN * i + l] =
+                (l > 0 ? s_C[NUM_NODES_IN * threadIdx.y + l - 1] * curl_orients_in[3 * NUM_NODES_IN * e + 3 * l - 1] : 0.0) +
+                s_C[NUM_NODES_IN * threadIdx.y + l] * curl_orients_in[3 * NUM_NODES_IN * e + 3 * l + 1] +
+                (l < (NUM_NODES_IN - 1) ? s_C[NUM_NODES_IN * threadIdx.y + l + 1] * curl_orients_in[3 * NUM_NODES_IN * e + 3 * l + 3] : 0.0);
+          } else {
+            s_CT[NUM_NODES_IN * i + l] = result;
+          }
         }  // end of loop over element node index, i
-      }    // end of out component
-    }      // end of in component
-  }        // end of element loop
+        if (curl_orients_in || curl_orients_out) {
+          // Compute and store the final T^T (B^T D B T) using the fully computed C T product in shared memory
+          if (curl_orients_out) __syncthreads();
+          for (IndexType i = threadIdx.y; i < NUM_NODES_OUT; i += BLOCK_SIZE_Y) {
+            IndexType val_index = e_stride * e + comp_in_stride * comp_in + comp_out_stride * comp_out + NUM_NODES_IN * i + l;
+
+            if (curl_orients_out) {
+              values_array[val_index] =
+                  (i > 0 ? s_CT[NUM_NODES_IN * (i - 1) + l] * curl_orients_out[3 * NUM_NODES_OUT * e + 3 * i - 1] : 0.0) +
+                  s_CT[NUM_NODES_IN * i + l] * curl_orients_out[3 * NUM_NODES_OUT * e + 3 * i + 1] +
+                  (i < (NUM_NODES_OUT - 1) ? s_CT[NUM_NODES_IN * (i + 1) + l] * curl_orients_out[3 * NUM_NODES_OUT * e + 3 * i + 3] : 0.0);
+            } else {
+              values_array[val_index] = s_CT[NUM_NODES_IN * i + l];
+            }
+          }
+        }
+      }  // end of out component
+    }    // end of in component
+  }      // end of element loop
 }
 
 //------------------------------------------------------------------------------
diff --git a/include/ceed/jit-source/hip/hip-ref-qfunction.h b/include/ceed/jit-source/hip/hip-ref-qfunction.h
index e3492b42..35107e4e 100644
--- a/include/ceed/jit-source/hip/hip-ref-qfunction.h
+++ b/include/ceed/jit-source/hip/hip-ref-qfunction.h
@@ -16,7 +16,7 @@
 // Read from quadrature points
 //------------------------------------------------------------------------------
 template <int SIZE>
-inline __device__ void readQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar* d_u, CeedScalar* r_u) {
+inline __device__ void readQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar *d_u, CeedScalar *r_u) {
   for (CeedInt comp = 0; comp < SIZE; comp++) {
     r_u[comp] = d_u[quad + num_qpts * comp];
   }
@@ -26,7 +26,7 @@ inline __device__ void readQuads(const CeedInt quad, const CeedInt num_qpts, con
 // Write at quadrature points
 //------------------------------------------------------------------------------
 template <int SIZE>
-inline __device__ void writeQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar* r_v, CeedScalar* d_v) {
+inline __device__ void writeQuads(const CeedInt quad, const CeedInt num_qpts, const CeedScalar *r_v, CeedScalar *d_v) {
   for (CeedInt comp = 0; comp < SIZE; comp++) {
     d_v[quad + num_qpts * comp] = r_v[comp];
   }
diff --git a/include/ceed/jit-source/hip/hip-ref-restriction.h b/include/ceed/jit-source/hip/hip-ref-restriction.h
index c34aa980..cdd3b770 100644
--- a/include/ceed/jit-source/hip/hip-ref-restriction.h
+++ b/include/ceed/jit-source/hip/hip-ref-restriction.h
@@ -16,86 +16,223 @@
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
 extern "C" __global__ void StridedNoTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
-  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RESTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
-    const CeedInt loc_node = node % RESTR_ELEM_SIZE;
-    const CeedInt elem     = node / RESTR_ELEM_SIZE;
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
 
-    for (CeedInt comp = 0; comp < RESTR_NUM_COMP; comp++) {
-      v[loc_node + comp * RESTR_ELEM_SIZE * RESTR_NUM_ELEM + elem * RESTR_ELEM_SIZE] =
-          u[loc_node * RESTR_STRIDE_NODES + comp * RESTR_STRIDE_COMP + elem * RESTR_STRIDE_ELEM];
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] =
+          u[loc_node * RSTR_STRIDE_NODES + comp * RSTR_STRIDE_COMP + elem * RSTR_STRIDE_ELEM];
     }
   }
 }
 
 //------------------------------------------------------------------------------
-// E-vector -> L-vector, strided
+// L-vector -> E-vector, standard (with offsets)
 //------------------------------------------------------------------------------
-extern "C" __global__ void StridedTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
-  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RESTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
-    const CeedInt loc_node = node % RESTR_ELEM_SIZE;
-    const CeedInt elem     = node / RESTR_ELEM_SIZE;
+extern "C" __global__ void OffsetNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ u,
+                                             CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt ind      = indices[node];
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
 
-    for (CeedInt comp = 0; comp < RESTR_NUM_COMP; comp++) {
-      v[loc_node * RESTR_STRIDE_NODES + comp * RESTR_STRIDE_COMP + elem * RESTR_STRIDE_ELEM] +=
-          u[loc_node + comp * RESTR_ELEM_SIZE * RESTR_NUM_ELEM + elem * RESTR_ELEM_SIZE];
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = u[ind + comp * RSTR_COMP_STRIDE];
     }
   }
 }
 
 //------------------------------------------------------------------------------
-// L-vector -> E-vector, offsets provided
+// L-vector -> E-vector, oriented
 //------------------------------------------------------------------------------
-extern "C" __global__ void OffsetNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ u,
-                                             CeedScalar *__restrict__ v) {
-  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RESTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+extern "C" __global__ void OrientedNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const bool *__restrict__ orients,
+                                               const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
     const CeedInt ind      = indices[node];
-    const CeedInt loc_node = node % RESTR_ELEM_SIZE;
-    const CeedInt elem     = node / RESTR_ELEM_SIZE;
+    const bool    orient   = orients[node];
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = u[ind + comp * RSTR_COMP_STRIDE] * (orient ? -1.0 : 1.0);
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// L-vector -> E-vector, curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                   const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                   CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind_dl         = loc_node > 0 ? indices[node - 1] : 0;
+    const CeedInt  ind_d          = indices[node];
+    const CeedInt  ind_du         = loc_node < (RSTR_ELEM_SIZE - 1) ? indices[node + 1] : 0;
+    const CeedInt8 curl_orient_dl = curl_orients[3 * node + 0];
+    const CeedInt8 curl_orient_d  = curl_orients[3 * node + 1];
+    const CeedInt8 curl_orient_du = curl_orients[3 * node + 2];
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[ind_dl + comp * RSTR_COMP_STRIDE] * curl_orient_dl : 0.0;
+      value += u[ind_d + comp * RSTR_COMP_STRIDE] * curl_orient_d;
+      value += loc_node < (RSTR_ELEM_SIZE - 1) ? u[ind_du + comp * RSTR_COMP_STRIDE] * curl_orient_du : 0.0;
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = value;
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// L-vector -> E-vector, unsigned curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedUnsignedNoTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                           const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                           CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind_dl         = loc_node > 0 ? indices[node - 1] : 0;
+    const CeedInt  ind_d          = indices[node];
+    const CeedInt  ind_du         = loc_node < (RSTR_ELEM_SIZE - 1) ? indices[node + 1] : 0;
+    const CeedInt8 curl_orient_dl = abs(curl_orients[3 * node + 0]);
+    const CeedInt8 curl_orient_d  = abs(curl_orients[3 * node + 1]);
+    const CeedInt8 curl_orient_du = abs(curl_orients[3 * node + 2]);
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[ind_dl + comp * RSTR_COMP_STRIDE] * curl_orient_dl : 0.0;
+      value += u[ind_d + comp * RSTR_COMP_STRIDE] * curl_orient_d;
+      value += loc_node < (RSTR_ELEM_SIZE - 1) ? u[ind_du + comp * RSTR_COMP_STRIDE] * curl_orient_du : 0.0;
+      v[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] = value;
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, strided
+//------------------------------------------------------------------------------
+extern "C" __global__ void StridedTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
 
-    for (CeedInt comp = 0; comp < RESTR_NUM_COMP; comp++) {
-      v[loc_node + comp * RESTR_ELEM_SIZE * RESTR_NUM_ELEM + elem * RESTR_ELEM_SIZE] = u[ind + comp * RESTR_COMP_STRIDE];
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      v[loc_node * RSTR_STRIDE_NODES + comp * RSTR_STRIDE_COMP + elem * RSTR_STRIDE_ELEM] +=
+          u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE];
     }
   }
 }
 
 //------------------------------------------------------------------------------
-// E-vector -> L-vector, offsets provided
+// E-vector -> L-vector, standard (with offsets)
 //------------------------------------------------------------------------------
 extern "C" __global__ void OffsetTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const CeedScalar *__restrict__ u,
                                            CeedScalar *__restrict__ v) {
-  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RESTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
     const CeedInt ind      = indices[node];
-    const CeedInt loc_node = node % RESTR_ELEM_SIZE;
-    const CeedInt elem     = node / RESTR_ELEM_SIZE;
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
 
-    for (CeedInt comp = 0; comp < RESTR_NUM_COMP; comp++) {
-      atomicAdd(v + ind + comp * RESTR_COMP_STRIDE, u[loc_node + comp * RESTR_ELEM_SIZE * RESTR_NUM_ELEM + elem * RESTR_ELEM_SIZE]);
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      atomicAdd(v + ind + comp * RSTR_COMP_STRIDE, u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE]);
     }
   }
 }
 
 extern "C" __global__ void OffsetTransposeDet(const CeedInt *__restrict__ l_vec_indices, const CeedInt *__restrict__ t_indices,
                                               const CeedInt *__restrict__ t_offsets, const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
-  CeedScalar value[RESTR_NUM_COMP];
+  CeedScalar value[RSTR_NUM_COMP];
 
-  for (CeedInt i = blockIdx.x * blockDim.x + threadIdx.x; i < RESTR_NUM_NODES; i += blockDim.x * gridDim.x) {
+  for (CeedInt i = blockIdx.x * blockDim.x + threadIdx.x; i < RSTR_NUM_NODES; i += blockDim.x * gridDim.x) {
     const CeedInt ind     = l_vec_indices[i];
     const CeedInt range_1 = t_offsets[i];
     const CeedInt range_N = t_offsets[i + 1];
 
-    for (CeedInt comp = 0; comp < RESTR_NUM_COMP; comp++) value[comp] = 0.0;
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) value[comp] = 0.0;
 
     for (CeedInt j = range_1; j < range_N; j++) {
       const CeedInt t_ind    = t_indices[j];
-      CeedInt       loc_node = t_ind % RESTR_ELEM_SIZE;
-      CeedInt       elem     = t_ind / RESTR_ELEM_SIZE;
+      const CeedInt loc_node = t_ind % RSTR_ELEM_SIZE;
+      const CeedInt elem     = t_ind / RSTR_ELEM_SIZE;
 
-      for (CeedInt comp = 0; comp < RESTR_NUM_COMP; comp++) {
-        value[comp] += u[loc_node + comp * RESTR_ELEM_SIZE * RESTR_NUM_ELEM + elem * RESTR_ELEM_SIZE];
+      for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+        value[comp] += u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE];
       }
     }
 
-    for (CeedInt comp = 0; comp < RESTR_NUM_COMP; comp++) v[ind + comp * RESTR_COMP_STRIDE] += value[comp];
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) v[ind + comp * RSTR_COMP_STRIDE] += value[comp];
+  }
+}
+
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void OrientedTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices, const bool *__restrict__ orients,
+                                             const CeedScalar *__restrict__ u, CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt ind      = indices[node];
+    const bool    orient   = orients[node];
+    const CeedInt loc_node = node % RSTR_ELEM_SIZE;
+    const CeedInt elem     = node / RSTR_ELEM_SIZE;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      atomicAdd(v + ind + comp * RSTR_COMP_STRIDE,
+                u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * (orient ? -1.0 : 1.0));
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                 const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                 CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind            = indices[node];
+    const CeedInt8 curl_orient_du = loc_node > 0 ? curl_orients[3 * node - 1] : 0.0;
+    const CeedInt8 curl_orient_d  = curl_orients[3 * node + 1];
+    const CeedInt8 curl_orient_dl = loc_node < (RSTR_ELEM_SIZE - 1) ? curl_orients[3 * node + 3] : 0.0;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[loc_node - 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_du : 0.0;
+      value += u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_d;
+      value +=
+          loc_node < (RSTR_ELEM_SIZE - 1) ? u[loc_node + 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_dl : 0.0;
+      atomicAdd(v + ind + comp * RSTR_COMP_STRIDE, value);
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// E-vector -> L-vector, unsigned curl-oriented
+//------------------------------------------------------------------------------
+extern "C" __global__ void CurlOrientedUnsignedTranspose(const CeedInt num_elem, const CeedInt *__restrict__ indices,
+                                                         const CeedInt8 *__restrict__ curl_orients, const CeedScalar *__restrict__ u,
+                                                         CeedScalar *__restrict__ v) {
+  for (CeedInt node = blockIdx.x * blockDim.x + threadIdx.x; node < num_elem * RSTR_ELEM_SIZE; node += blockDim.x * gridDim.x) {
+    const CeedInt  loc_node       = node % RSTR_ELEM_SIZE;
+    const CeedInt  elem           = node / RSTR_ELEM_SIZE;
+    const CeedInt  ind            = indices[node];
+    const CeedInt8 curl_orient_du = loc_node > 0 ? abs(curl_orients[3 * node - 1]) : 0.0;
+    const CeedInt8 curl_orient_d  = abs(curl_orients[3 * node + 1]);
+    const CeedInt8 curl_orient_dl = loc_node < (RSTR_ELEM_SIZE - 1) ? abs(curl_orients[3 * node + 3]) : 0.0;
+
+    for (CeedInt comp = 0; comp < RSTR_NUM_COMP; comp++) {
+      CeedScalar value = 0.0;
+      value += loc_node > 0 ? u[loc_node - 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_du : 0.0;
+      value += u[loc_node + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_d;
+      value +=
+          loc_node < (RSTR_ELEM_SIZE - 1) ? u[loc_node + 1 + comp * RSTR_ELEM_SIZE * RSTR_NUM_ELEM + elem * RSTR_ELEM_SIZE] * curl_orient_dl : 0.0;
+      atomicAdd(v + ind + comp * RSTR_COMP_STRIDE, value);
+    }
   }
 }
 
diff --git a/include/ceed/jit-source/hip/hip-shared-basis-read-write-templates.h b/include/ceed/jit-source/hip/hip-shared-basis-read-write-templates.h
index 9109b3f2..e556cbf3 100644
--- a/include/ceed/jit-source/hip/hip-shared-basis-read-write-templates.h
+++ b/include/ceed/jit-source/hip/hip-shared-basis-read-write-templates.h
@@ -18,6 +18,7 @@
 template <int SIZE>
 inline __device__ void loadMatrix(const CeedScalar *d_B, CeedScalar *B) {
   CeedInt tid = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.y * blockDim.x;
+
   for (CeedInt i = tid; i < SIZE; i += blockDim.x * blockDim.y * blockDim.z) B[i] = d_B[i];
 }
 
@@ -34,6 +35,7 @@ inline __device__ void ReadElementStrided1d(SharedData_Hip &data, const CeedInt
   if (data.t_id_x < P_1D) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = node * strides_node + elem * strides_elem;
+
     for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
       r_u[comp] = d_u[ind + comp * strides_comp];
     }
@@ -49,6 +51,7 @@ inline __device__ void WriteElementStrided1d(SharedData_Hip &data, const CeedInt
   if (data.t_id_x < P_1D) {
     const CeedInt node = data.t_id_x;
     const CeedInt ind  = node * strides_node + elem * strides_elem;
+
     for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
       d_v[ind + comp * strides_comp] = r_v[comp];
     }
@@ -68,6 +71,7 @@ inline __device__ void ReadElementStrided2d(SharedData_Hip &data, const CeedInt
   if (data.t_id_x < P_1D && data.t_id_y < P_1D) {
     const CeedInt node = data.t_id_x + data.t_id_y * P_1D;
     const CeedInt ind  = node * strides_node + elem * strides_elem;
+
     for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
       r_u[comp] = d_u[ind + comp * strides_comp];
     }
@@ -83,6 +87,7 @@ inline __device__ void WriteElementStrided2d(SharedData_Hip &data, const CeedInt
   if (data.t_id_x < P_1D && data.t_id_y < P_1D) {
     const CeedInt node = data.t_id_x + data.t_id_y * P_1D;
     const CeedInt ind  = node * strides_node + elem * strides_elem;
+
     for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
       d_v[ind + comp * strides_comp] = r_v[comp];
     }
@@ -103,6 +108,7 @@ inline __device__ void ReadElementStrided3d(SharedData_Hip &data, const CeedInt
     for (CeedInt z = 0; z < P_1D; z++) {
       const CeedInt node = data.t_id_x + data.t_id_y * P_1D + z * P_1D * P_1D;
       const CeedInt ind  = node * strides_node + elem * strides_elem;
+
       for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
         r_u[z + comp * P_1D] = d_u[ind + comp * strides_comp];
       }
@@ -120,6 +126,7 @@ inline __device__ void WriteElementStrided3d(SharedData_Hip &data, const CeedInt
     for (CeedInt z = 0; z < P_1D; z++) {
       const CeedInt node = data.t_id_x + data.t_id_y * P_1D + z * P_1D * P_1D;
       const CeedInt ind  = node * strides_node + elem * strides_elem;
+
       for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
         d_v[ind + comp * strides_comp] = r_v[z + comp * P_1D];
       }
diff --git a/include/ceed/jit-source/hip/hip-types.h b/include/ceed/jit-source/hip/hip-types.h
index 1ef7714c..e05cfc15 100644
--- a/include/ceed/jit-source/hip/hip-types.h
+++ b/include/ceed/jit-source/hip/hip-types.h
@@ -15,13 +15,13 @@
 #define CEED_HIP_NUMBER_FIELDS 16
 
 typedef struct {
-  const CeedScalar* inputs[CEED_HIP_NUMBER_FIELDS];
-  CeedScalar*       outputs[CEED_HIP_NUMBER_FIELDS];
+  const CeedScalar *inputs[CEED_HIP_NUMBER_FIELDS];
+  CeedScalar       *outputs[CEED_HIP_NUMBER_FIELDS];
 } Fields_Hip;
 
 typedef struct {
-  CeedInt* inputs[CEED_HIP_NUMBER_FIELDS];
-  CeedInt* outputs[CEED_HIP_NUMBER_FIELDS];
+  CeedInt *inputs[CEED_HIP_NUMBER_FIELDS];
+  CeedInt *outputs[CEED_HIP_NUMBER_FIELDS];
 } FieldsInt_Hip;
 
 typedef struct {
@@ -29,7 +29,7 @@ typedef struct {
   CeedInt     t_id_y;
   CeedInt     t_id_z;
   CeedInt     t_id;
-  CeedScalar* slice;
+  CeedScalar *slice;
 } SharedData_Hip;
 
 #endif  // CEED_HIP_TYPES_H
diff --git a/include/ceed/jit-source/magma/grad-1d.h b/include/ceed/jit-source/magma/grad-1d.h
index cadbd38a..5eea0ee2 100644
--- a/include/ceed/jit-source/magma/grad-1d.h
+++ b/include/ceed/jit-source/magma/grad-1d.h
@@ -11,7 +11,7 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 // grad basis action (1D)
 template <typename T, int DIM_, int NCOMP_, int P_, int Q_>
-static __device__ __inline__ void magma_grad_1d_device(const T* sT, magma_trans_t transT, T* sU[NCOMP_], T* sV[NCOMP_], const int tx) {
+static __device__ __inline__ void magma_grad_1d_device(const T *sT, magma_trans_t transT, T *sU[NCOMP_], T *sV[NCOMP_], const int tx) {
   // Assumptions
   // 1. 1D threads of size max(P_,Q_)
   // 2. sU[i] is 1xP_: in shared memory
@@ -35,8 +35,8 @@ static __device__ __inline__ void magma_grad_1d_device(const T* sT, magma_trans_
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __global__
-    void magma_gradn_1d_kernel(const CeedScalar* dTinterp, const CeedScalar* dTgrad, const CeedScalar* dU, const int estrdU, const int cstrdU,
-                               const int dstrdU, CeedScalar* dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
+    void magma_gradn_1d_kernel(const CeedScalar *dTinterp, const CeedScalar *dTgrad, const CeedScalar *dU, const int estrdU, const int cstrdU,
+                               const int dstrdU, CeedScalar *dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
   const int     tx      = threadIdx.x;
@@ -46,16 +46,16 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __g
 
   if (elem_id >= nelem) return;
 
-  CeedScalar* sU[NCOMP];
-  CeedScalar* sV[NCOMP];
+  CeedScalar *sU[NCOMP];
+  CeedScalar *sV[NCOMP];
 
   // shift global memory pointers by elem stride
   dU += elem_id * estrdU;
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT = (CeedScalar*)(shared_data);
-  CeedScalar* sW = sT + P * Q;
+  CeedScalar *sT = (CeedScalar *)(shared_data);
+  CeedScalar *sW = sT + P * Q;
   sU[0]          = sW + ty * NCOMP * (P + Q);
   sV[0]          = sU[0] + (NCOMP * 1 * P);
   for (int icomp = 1; icomp < NCOMP; icomp++) {
@@ -81,8 +81,8 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __g
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __global__
-    void magma_gradt_1d_kernel(const CeedScalar* dTinterp, const CeedScalar* dTgrad, const CeedScalar* dU, const int estrdU, const int cstrdU,
-                               const int dstrdU, CeedScalar* dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
+    void magma_gradt_1d_kernel(const CeedScalar *dTinterp, const CeedScalar *dTgrad, const CeedScalar *dU, const int estrdU, const int cstrdU,
+                               const int dstrdU, CeedScalar *dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
   const int     tx      = threadIdx.x;
@@ -92,16 +92,16 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __g
 
   if (elem_id >= nelem) return;
 
-  CeedScalar* sU[NCOMP];
-  CeedScalar* sV[NCOMP];
+  CeedScalar *sU[NCOMP];
+  CeedScalar *sV[NCOMP];
 
   // shift global memory pointers by elem stride
   dU += elem_id * estrdU;
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT = (CeedScalar*)(shared_data);
-  CeedScalar* sW = sT + Q * P;
+  CeedScalar *sT = (CeedScalar *)(shared_data);
+  CeedScalar *sW = sT + Q * P;
   sU[0]          = sW + ty * NCOMP * (Q + P);
   sV[0]          = sU[0] + (NCOMP * 1 * Q);
   for (int icomp = 1; icomp < NCOMP; icomp++) {
diff --git a/include/ceed/jit-source/magma/grad-3d.h b/include/ceed/jit-source/magma/grad-3d.h
index 5198a27a..072c1da2 100644
--- a/include/ceed/jit-source/magma/grad-3d.h
+++ b/include/ceed/jit-source/magma/grad-3d.h
@@ -20,8 +20,8 @@
 // iDIM_V -- which dim index of rV is accessed (0, 1, or 2 for notrans, always 0 for trans)
 // the scalar beta is used to specify whether to accumulate to rV, or overwrite it
 template <typename T, int DIM_U, int DIM_V, int NCOMP_, int P_, int Q_, int rUsize, int rVsize, int iDIM_, int iDIM_U, int iDIM_V>
-static __device__ __inline__ void magma_grad_3d_device(const T* sTinterp, const T* sTgrad, T rU[DIM_U][NCOMP_][rUsize], T rV[DIM_V][NCOMP_][rVsize],
-                                                       T beta, const int tx, T rTmp, T* swork) {
+static __device__ __inline__ void magma_grad_3d_device(const T *sTinterp, const T *sTgrad, T rU[DIM_U][NCOMP_][rUsize], T rV[DIM_V][NCOMP_][rVsize],
+                                                       T beta, const int tx, T rTmp, T *swork) {
   // Assumptions
   // 0. This device routine applies grad for one dim only (iDIM_), so it should be thrice for 3D
   // 1. 1D threads of size max(P_,Q_)^2
@@ -34,15 +34,15 @@ static __device__ __inline__ void magma_grad_3d_device(const T* sTinterp, const
   // 6. Each thread computes one row of the output of each product
   // 7. Sync is recommended before and after the call
 
-  T* sW1 = swork;
-  T* sW2 = sW1 + P_ * P_ * Q_;
+  T *sW1 = swork;
+  T *sW2 = sW1 + P_ * P_ * Q_;
   for (int icomp = 0; icomp < NCOMP_; icomp++) {
     // Batch P_^2 of (1xP_) matrices [reg] times (P_xQ_) matrix [shmem] => Batch P_^2 of (1xQ_) matrices [shmem]
     if (tx < (P_ * P_)) {
       const int batchid = tx;
       const int sld     = 1;
-      const T*  sT      = (iDIM_ == 0) ? sTgrad : sTinterp;
-      T*        sTmp    = sW1 + batchid * (1 * Q_);
+      const T  *sT      = (iDIM_ == 0) ? sTgrad : sTinterp;
+      T        *sTmp    = sW1 + batchid * (1 * Q_);
       for (int j = 0; j < Q_; j++) {
         rTmp = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -58,9 +58,9 @@ static __device__ __inline__ void magma_grad_3d_device(const T* sTinterp, const
       const int batchid = tx / Q_;
       const int tx_     = tx % Q_;
       const int sld     = Q_;
-      const T*  sT      = (iDIM_ == 1) ? sTgrad : sTinterp;
-      T*        sTmp    = sW1 + batchid * (Q_ * P_);  // sTmp is input
-      T*        sTmp2   = sW2 + batchid * (Q_ * Q_);  // sTmp2 is output
+      const T  *sT      = (iDIM_ == 1) ? sTgrad : sTinterp;
+      T        *sTmp    = sW1 + batchid * (Q_ * P_);  // sTmp is input
+      T        *sTmp2   = sW2 + batchid * (Q_ * Q_);  // sTmp2 is output
       for (int j = 0; j < Q_; j++) {
         rTmp = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -76,8 +76,8 @@ static __device__ __inline__ void magma_grad_3d_device(const T* sTinterp, const
       // No need to declare batchid = (tx  / Q_^2) = always zero
       // No need to declare tx_     = (tx_ % Q_^2) = always tx
       const int sld  = Q_ * Q_;
-      const T*  sT   = (iDIM_ == 2) ? sTgrad : sTinterp;
-      T*        sTmp = sW2;  // sTmp is input
+      const T  *sT   = (iDIM_ == 2) ? sTgrad : sTinterp;
+      T        *sTmp = sW2;  // sTmp is input
       for (int j = 0; j < Q_; j++) {
         rTmp = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -92,9 +92,9 @@ static __device__ __inline__ void magma_grad_3d_device(const T* sTinterp, const
 }
 
 //////////////////////////////////////////////////////////////////////////////////////////
-extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
-    void magma_gradn_3d_kernel(const CeedScalar* dinterp1d, const CeedScalar* dgrad1d, const CeedScalar* dU, const int estrdU, const int cstrdU,
-                               const int dstrdU, CeedScalar* dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
+extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ *MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
+    void magma_gradn_3d_kernel(const CeedScalar *dinterp1d, const CeedScalar *dgrad1d, const CeedScalar *dU, const int estrdU, const int cstrdU,
+                               const int dstrdU, CeedScalar *dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
   const int     tx      = threadIdx.x;
@@ -113,9 +113,9 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sTinterp = (CeedScalar*)(shared_data);
-  CeedScalar* sTgrad   = sTinterp + P * Q;
-  CeedScalar* sTmp     = sTgrad + P * Q;
+  CeedScalar *sTinterp = (CeedScalar *)(shared_data);
+  CeedScalar *sTgrad   = sTinterp + P * Q;
+  CeedScalar *sTmp     = sTgrad + P * Q;
   sTmp += ty * (max(P * P * P, (P * P * Q) + (P * Q * Q)));
 
   // read T
@@ -152,9 +152,9 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3
 }
 
 //////////////////////////////////////////////////////////////////////////////////////////
-extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
-    void magma_gradt_3d_kernel(const CeedScalar* dinterp1d, const CeedScalar* dgrad1d, const CeedScalar* dU, const int estrdU, const int cstrdU,
-                               const int dstrdU, CeedScalar* dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
+extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ *MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
+    void magma_gradt_3d_kernel(const CeedScalar *dinterp1d, const CeedScalar *dgrad1d, const CeedScalar *dU, const int estrdU, const int cstrdU,
+                               const int dstrdU, CeedScalar *dV, const int estrdV, const int cstrdV, const int dstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
   const int     tx      = threadIdx.x;
@@ -173,9 +173,9 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sTinterp = (CeedScalar*)(shared_data);
-  CeedScalar* sTgrad   = sTinterp + Q * P;
-  CeedScalar* sTmp     = sTgrad + Q * P;
+  CeedScalar *sTinterp = (CeedScalar *)(shared_data);
+  CeedScalar *sTgrad   = sTinterp + Q * P;
+  CeedScalar *sTmp     = sTgrad + Q * P;
   sTmp += ty * (max(Q * Q * Q, (Q * Q * P) + (Q * P * P)));
 
   // read T
diff --git a/include/ceed/jit-source/magma/grad-nontensor.h b/include/ceed/jit-source/magma/grad-nontensor.h
index 9a0a370c..164f2c75 100644
--- a/include/ceed/jit-source/magma/grad-nontensor.h
+++ b/include/ceed/jit-source/magma/grad-nontensor.h
@@ -10,9 +10,9 @@
 
 ////////////////////////////////////////////////////////////////////////////////
 // Different A's and C's, same B
-extern "C" __global__ __launch_bounds__(Q* MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void magma_grad_nontensor_n(magma_trans_t transA, magma_trans_t transB,
-                                                                                                       int n, CeedScalar const* dA, int ldda,
-                                                                                                       CeedScalar const* dB, int lddb, CeedScalar* dC,
+extern "C" __global__ __launch_bounds__(Q *MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void magma_grad_nontensor_n(magma_trans_t transA, magma_trans_t transB,
+                                                                                                       int n, CeedScalar const *dA, int ldda,
+                                                                                                       CeedScalar const *dB, int lddb, CeedScalar *dC,
                                                                                                        int lddc) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data);
 
@@ -31,8 +31,8 @@ extern "C" __global__ __launch_bounds__(Q* MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void
   // A is P x Q
   const int   slda = P;
   const int   sldb = P;
-  CeedScalar* sA   = (CeedScalar*)(shared_data);
-  CeedScalar* sB   = sA + Q * P;
+  CeedScalar *sA   = (CeedScalar *)(shared_data);
+  CeedScalar *sB   = sA + Q * P;
   sB += ty * sldb * NB_GRAD_N;
 
   // read B once for all C's
@@ -68,9 +68,9 @@ extern "C" __global__ __launch_bounds__(Q* MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void
 
 ////////////////////////////////////////////////////////////////////////////////
 // Different A's and B's, same C
-extern "C" __global__ __launch_bounds__(P* MAGMA_NONTENSOR_BASIS_NTCOL(P)) void magma_grad_nontensor_t(magma_trans_t transA, magma_trans_t transB,
-                                                                                                       int n, CeedScalar const* dA, int ldda,
-                                                                                                       CeedScalar const* dB, int lddb, CeedScalar* dC,
+extern "C" __global__ __launch_bounds__(P *MAGMA_NONTENSOR_BASIS_NTCOL(P)) void magma_grad_nontensor_t(magma_trans_t transA, magma_trans_t transB,
+                                                                                                       int n, CeedScalar const *dA, int ldda,
+                                                                                                       CeedScalar const *dB, int lddb, CeedScalar *dC,
                                                                                                        int lddc) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data);
 
@@ -89,7 +89,7 @@ extern "C" __global__ __launch_bounds__(P* MAGMA_NONTENSOR_BASIS_NTCOL(P)) void
 
   // A is P x Q
   const int   sldb = Q;
-  CeedScalar* sB   = (CeedScalar*)(shared_data);
+  CeedScalar *sB   = (CeedScalar *)(shared_data);
   sB += ty * sldb * NB_GRAD_T;
 
   // init rC
diff --git a/include/ceed/jit-source/magma/interp-1d.h b/include/ceed/jit-source/magma/interp-1d.h
index 218b1459..3ca89e3c 100644
--- a/include/ceed/jit-source/magma/interp-1d.h
+++ b/include/ceed/jit-source/magma/interp-1d.h
@@ -11,7 +11,7 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 // interp basis action (1D)
 template <typename T, int DIM_, int NCOMP_, int P_, int Q_>
-static __device__ __inline__ void magma_interp_1d_device(const T* sT, magma_trans_t transT, T* sU[NCOMP_], T* sV[NCOMP_], const int tx) {
+static __device__ __inline__ void magma_interp_1d_device(const T *sT, magma_trans_t transT, T *sU[NCOMP_], T *sV[NCOMP_], const int tx) {
   // Assumptions
   // 1. 1D threads of size max(P_,Q_)
   // 2. sU[i] is 1xP_: in shared memory
@@ -35,7 +35,7 @@ static __device__ __inline__ void magma_interp_1d_device(const T* sT, magma_tran
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __global__
-    void magma_interpn_1d_kernel(const CeedScalar* dT, const CeedScalar* dU, const int estrdU, const int cstrdU, CeedScalar* dV, const int estrdV,
+    void magma_interpn_1d_kernel(const CeedScalar *dT, const CeedScalar *dU, const int estrdU, const int cstrdU, CeedScalar *dV, const int estrdV,
                                  const int cstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
@@ -46,16 +46,16 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __g
 
   if (elem_id >= nelem) return;
 
-  CeedScalar* sU[NCOMP];
-  CeedScalar* sV[NCOMP];
+  CeedScalar *sU[NCOMP];
+  CeedScalar *sV[NCOMP];
 
   // shift global memory pointers by elem stride
   dU += elem_id * estrdU;
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT = (CeedScalar*)(shared_data);
-  CeedScalar* sW = sT + P * Q;
+  CeedScalar *sT = (CeedScalar *)(shared_data);
+  CeedScalar *sW = sT + P * Q;
   sU[0]          = sW + ty * NCOMP * (P + Q);
   sV[0]          = sU[0] + (NCOMP * 1 * P);
   for (int icomp = 1; icomp < NCOMP; icomp++) {
@@ -81,7 +81,7 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __g
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __global__
-    void magma_interpt_1d_kernel(const CeedScalar* dT, const CeedScalar* dU, const int estrdU, const int cstrdU, CeedScalar* dV, const int estrdV,
+    void magma_interpt_1d_kernel(const CeedScalar *dT, const CeedScalar *dU, const int estrdU, const int cstrdU, CeedScalar *dV, const int estrdV,
                                  const int cstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
@@ -92,16 +92,16 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_1D)) __g
 
   if (elem_id >= nelem) return;
 
-  CeedScalar* sU[NCOMP];
-  CeedScalar* sV[NCOMP];
+  CeedScalar *sU[NCOMP];
+  CeedScalar *sV[NCOMP];
 
   // shift global memory pointers by elem stride
   dU += elem_id * estrdU;
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT = (CeedScalar*)(shared_data);
-  CeedScalar* sW = sT + Q * P;
+  CeedScalar *sT = (CeedScalar *)(shared_data);
+  CeedScalar *sW = sT + Q * P;
   sU[0]          = sW + ty * NCOMP * (Q + P);
   sV[0]          = sU[0] + (NCOMP * 1 * Q);
   for (int icomp = 1; icomp < NCOMP; icomp++) {
diff --git a/include/ceed/jit-source/magma/interp-2d.h b/include/ceed/jit-source/magma/interp-2d.h
index 07947793..901128ba 100644
--- a/include/ceed/jit-source/magma/interp-2d.h
+++ b/include/ceed/jit-source/magma/interp-2d.h
@@ -12,8 +12,8 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 // interp basis action (2D)
 template <typename T, int DIM_U, int DIM_V, int NCOMP_, int P_, int Q_, int rUsize, int rVsize>
-static __device__ __inline__ void magma_interp_2d_device(const T* sT, magma_trans_t transT, T rU[DIM_U][NCOMP_][rUsize], T rV[DIM_V][NCOMP_][rVsize],
-                                                         const int tx, T rTmp, T* swork) {
+static __device__ __inline__ void magma_interp_2d_device(const T *sT, magma_trans_t transT, T rU[DIM_U][NCOMP_][rUsize], T rV[DIM_V][NCOMP_][rVsize],
+                                                         const int tx, T rTmp, T *swork) {
   // Assumptions
   // 1. 1D threads of size max(P_,Q_)
   // 2. input:  rU[DIM_U x NCOMP_ x rUsize] in registers (per thread)
@@ -30,7 +30,7 @@ static __device__ __inline__ void magma_interp_2d_device(const T* sT, magma_tran
     if (tx < P_) {
       const int batchid = tx;
       const int sld     = 1;
-      T*        sTmp    = swork + batchid * (1 * Q_);
+      T        *sTmp    = swork + batchid * (1 * Q_);
       for (int j = 0; j < Q_; j++) {
         rTmp = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -45,7 +45,7 @@ static __device__ __inline__ void magma_interp_2d_device(const T* sT, magma_tran
     if (tx < Q_) {
       const int batchid = 0;
       const int sld     = Q_;
-      T*        sTmp    = swork + batchid * (Q_ * P_);
+      T        *sTmp    = swork + batchid * (Q_ * P_);
       for (int j = 0; j < Q_; j++) {
         rTmp = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -60,7 +60,7 @@ static __device__ __inline__ void magma_interp_2d_device(const T* sT, magma_tran
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_2D)) __global__
-    void magma_interpn_2d_kernel(const CeedScalar* dT, const CeedScalar* dU, const int estrdU, const int cstrdU, CeedScalar* dV, const int estrdV,
+    void magma_interpn_2d_kernel(const CeedScalar *dT, const CeedScalar *dU, const int estrdU, const int cstrdU, CeedScalar *dV, const int estrdV,
                                  const int cstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
@@ -80,8 +80,8 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_2D)) __g
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT   = (CeedScalar*)(shared_data);
-  CeedScalar* sTmp = sT + P * Q;
+  CeedScalar *sT   = (CeedScalar *)(shared_data);
+  CeedScalar *sTmp = sT + P * Q;
   sTmp += ty * (P * MAXPQ);
 
   // read T
@@ -102,7 +102,7 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_2D)) __g
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_2D)) __global__
-    void magma_interpt_2d_kernel(const CeedScalar* dT, const CeedScalar* dU, const int estrdU, const int cstrdU, CeedScalar* dV, const int estrdV,
+    void magma_interpt_2d_kernel(const CeedScalar *dT, const CeedScalar *dU, const int estrdU, const int cstrdU, CeedScalar *dV, const int estrdV,
                                  const int cstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
@@ -122,8 +122,8 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ, MAGMA_MAXTHREADS_2D)) __g
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT   = (CeedScalar*)(shared_data);
-  CeedScalar* sTmp = sT + Q * P;
+  CeedScalar *sT   = (CeedScalar *)(shared_data);
+  CeedScalar *sTmp = sT + Q * P;
   sTmp += ty * (Q * MAXPQ);
 
   // read T
diff --git a/include/ceed/jit-source/magma/interp-3d.h b/include/ceed/jit-source/magma/interp-3d.h
index 5fceaf8a..a886910a 100644
--- a/include/ceed/jit-source/magma/interp-3d.h
+++ b/include/ceed/jit-source/magma/interp-3d.h
@@ -12,8 +12,8 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 // interp basis action (3D)
 template <typename T, int DIM_U, int DIM_V, int NCOMP_, int P_, int Q_, int rUsize, int rVsize>
-static __device__ __inline__ void magma_interp_3d_device(const T* sT, magma_trans_t transT, T rU[DIM_U][NCOMP_][rUsize], T rV[DIM_V][NCOMP_][rVsize],
-                                                         const int tx, T rTmp[Q_], T* swork) {
+static __device__ __inline__ void magma_interp_3d_device(const T *sT, magma_trans_t transT, T rU[DIM_U][NCOMP_][rUsize], T rV[DIM_V][NCOMP_][rVsize],
+                                                         const int tx, T rTmp[Q_], T *swork) {
   // Assumptions
   // 1. 1D threads of size max(P_,Q_)^2
   // 2. input:  rU[DIM_U x NCOMP_ x rUsize] in registers (per thread)
@@ -30,7 +30,7 @@ static __device__ __inline__ void magma_interp_3d_device(const T* sT, magma_tran
     if (tx < (P_ * P_)) {
       const int batchid = tx;
       const int sld     = 1;
-      T*        sTmp    = swork + batchid * (1 * Q_);
+      T        *sTmp    = swork + batchid * (1 * Q_);
       for (int j = 0; j < Q_; j++) {
         rTmp[0] = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -46,7 +46,7 @@ static __device__ __inline__ void magma_interp_3d_device(const T* sT, magma_tran
       const int batchid = tx / Q_;
       const int tx_     = tx % Q_;
       const int sld     = Q_;
-      T*        sTmp    = swork + batchid * (Q_ * P_);  // sTmp is input
+      T        *sTmp    = swork + batchid * (Q_ * P_);  // sTmp is input
       for (int j = 0; j < Q_; j++) {
         rTmp[j] = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -61,7 +61,7 @@ static __device__ __inline__ void magma_interp_3d_device(const T* sT, magma_tran
       const int batchid = tx / Q_;
       const int tx_     = tx % Q_;
       const int sld     = Q_;
-      T*        sTmp    = swork + batchid * (Q_ * Q_);
+      T        *sTmp    = swork + batchid * (Q_ * Q_);
       for (int j = 0; j < Q_; j++) {
         sTmp(tx_, j, sld) = rTmp[j];
       }
@@ -73,7 +73,7 @@ static __device__ __inline__ void magma_interp_3d_device(const T* sT, magma_tran
       // No need to declare batchid = (tx  / Q_^2) = always zero
       // No need to declare tx_     = (tx_ % Q_^2) = always tx
       const int sld  = Q_ * Q_;
-      T*        sTmp = swork;
+      T        *sTmp = swork;
       for (int j = 0; j < Q_; j++) {
         rTmp[0] = 0.0;
         for (int i = 0; i < P_; i++) {
@@ -87,8 +87,8 @@ static __device__ __inline__ void magma_interp_3d_device(const T* sT, magma_tran
 }
 
 //////////////////////////////////////////////////////////////////////////////////////////
-extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
-    void magma_interpn_3d_kernel(const CeedScalar* dT, const CeedScalar* dU, const int estrdU, const int cstrdU, CeedScalar* dV, const int estrdV,
+extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ *MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
+    void magma_interpn_3d_kernel(const CeedScalar *dT, const CeedScalar *dU, const int estrdU, const int cstrdU, CeedScalar *dV, const int estrdV,
                                  const int cstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
@@ -108,8 +108,8 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT   = (CeedScalar*)(shared_data);
-  CeedScalar* sTmp = sT + P * Q;
+  CeedScalar *sT   = (CeedScalar *)(shared_data);
+  CeedScalar *sTmp = sT + P * Q;
   sTmp += ty * (max(P * P * MAXPQ, P * Q * Q));
 
   // read T
@@ -129,8 +129,8 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3
 }
 
 //////////////////////////////////////////////////////////////////////////////////////////
-extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
-    void magma_interpt_3d_kernel(const CeedScalar* dT, const CeedScalar* dU, const int estrdU, const int cstrdU, CeedScalar* dV, const int estrdV,
+extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ *MAXPQ, MAGMA_MAXTHREADS_3D)) __global__
+    void magma_interpt_3d_kernel(const CeedScalar *dT, const CeedScalar *dU, const int estrdU, const int cstrdU, CeedScalar *dV, const int estrdV,
                                  const int cstrdV, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
@@ -150,8 +150,8 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(MAXPQ* MAXPQ, MAGMA_MAXTHREADS_3
   dV += elem_id * estrdV;
 
   // assign shared memory pointers
-  CeedScalar* sT   = (CeedScalar*)(shared_data);
-  CeedScalar* sTmp = sT + Q * P;
+  CeedScalar *sT   = (CeedScalar *)(shared_data);
+  CeedScalar *sTmp = sT + Q * P;
   sTmp += ty * (max(Q * Q * MAXPQ, Q * P * P));
 
   // read T
diff --git a/include/ceed/jit-source/magma/interp-nontensor.h b/include/ceed/jit-source/magma/interp-nontensor.h
index 1b540b0a..e715986a 100644
--- a/include/ceed/jit-source/magma/interp-nontensor.h
+++ b/include/ceed/jit-source/magma/interp-nontensor.h
@@ -9,9 +9,9 @@
 #define CEED_MAGMA_INTERP_NONTENSOR_H
 
 ////////////////////////////////////////////////////////////////////////////////
-extern "C" __global__ __launch_bounds__(Q* MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void magma_interp_nontensor_n(
-    magma_trans_t transA, magma_trans_t transB, int n, const CeedScalar alpha, CeedScalar const* dA, int ldda, CeedScalar const* dB, int lddb,
-    const CeedScalar beta, CeedScalar* dC, int lddc) {
+extern "C" __global__ __launch_bounds__(Q *MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void magma_interp_nontensor_n(
+    magma_trans_t transA, magma_trans_t transB, int n, const CeedScalar alpha, CeedScalar const *dA, int ldda, CeedScalar const *dB, int lddb,
+    const CeedScalar beta, CeedScalar *dC, int lddc) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data);
 
   const int tx      = threadIdx.x;
@@ -28,8 +28,8 @@ extern "C" __global__ __launch_bounds__(Q* MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void
 
   const int   slda = P;
   const int   sldb = P;
-  CeedScalar* sA   = (CeedScalar*)(shared_data);
-  CeedScalar* sB   = sA;
+  CeedScalar *sA   = (CeedScalar *)(shared_data);
+  CeedScalar *sB   = sA;
   sB += ty * sldb * NB_INTERP_N;
 
   // read A using all threads
@@ -53,9 +53,9 @@ extern "C" __global__ __launch_bounds__(Q* MAGMA_NONTENSOR_BASIS_NTCOL(Q)) void
 }
 
 ////////////////////////////////////////////////////////////////////////////////
-extern "C" __global__ __launch_bounds__(P* MAGMA_NONTENSOR_BASIS_NTCOL(P)) void magma_interp_nontensor_t(
-    magma_trans_t transA, magma_trans_t transB, int n, const CeedScalar alpha, CeedScalar const* dA, int ldda, CeedScalar const* dB, int lddb,
-    const CeedScalar beta, CeedScalar* dC, int lddc) {
+extern "C" __global__ __launch_bounds__(P *MAGMA_NONTENSOR_BASIS_NTCOL(P)) void magma_interp_nontensor_t(
+    magma_trans_t transA, magma_trans_t transB, int n, const CeedScalar alpha, CeedScalar const *dA, int ldda, CeedScalar const *dB, int lddb,
+    const CeedScalar beta, CeedScalar *dC, int lddc) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data);
 
   const int tx      = threadIdx.x;
@@ -71,7 +71,7 @@ extern "C" __global__ __launch_bounds__(P* MAGMA_NONTENSOR_BASIS_NTCOL(P)) void
 
   // A is P x Q
   const int   sldb = Q;
-  CeedScalar* sB   = (CeedScalar*)(shared_data);
+  CeedScalar *sB   = (CeedScalar *)(shared_data);
   sB += ty * sldb * NB_INTERP_T;
 
   // init rC
diff --git a/include/ceed/jit-source/magma/magma_common_nontensor.h b/include/ceed/jit-source/magma/magma_common_nontensor.h
index 04f6cfd0..edfd805d 100644
--- a/include/ceed/jit-source/magma/magma_common_nontensor.h
+++ b/include/ceed/jit-source/magma/magma_common_nontensor.h
@@ -32,7 +32,7 @@
 // 1D thread config. with (Mx1) threads
 // no sync at the end of the function
 template <typename T, int P_, int NB_, int Q_>
-static __device__ __inline__ void read_C_g2r_1D_nosync(const int tx, const int n, T* dC, int lddc, const T& beta, T rC[NB_]) {
+static __device__ __inline__ void read_C_g2r_1D_nosync(const int tx, const int n, T *dC, int lddc, const T &beta, T rC[NB_]) {
   if (n != NB_) {
 #pragma unroll
     for (int j = 0; j < NB_; j++) {
@@ -52,7 +52,7 @@ static __device__ __inline__ void read_C_g2r_1D_nosync(const int tx, const int n
 // 1D thread config. with (Mx1) threads
 // no sync at the end of the function
 template <typename T, int P_, int NB_, int Q_>
-static __device__ __inline__ void write_C_r2g_1D_nosync(const int tx, const int n, T rC[NB_], T* dC, int lddc) {
+static __device__ __inline__ void write_C_r2g_1D_nosync(const int tx, const int n, T rC[NB_], T *dC, int lddc) {
   if (n != NB_) {
 #pragma unroll
     for (int j = 0; j < NB_; j++) {
@@ -74,7 +74,7 @@ static __device__ __inline__ void write_C_r2g_1D_nosync(const int tx, const int
 // 1D thread config. with (Mx1) threads
 // no sync at the end of the function
 template <typename T, int P_, int NB_, int Q_>
-static __device__ __inline__ void read_A_notrans_g2r_1D_nosync(const int tx, const T* dA, int ldda, T* sA, int slda, T rA[Q_]) {
+static __device__ __inline__ void read_A_notrans_g2r_1D_nosync(const int tx, const T *dA, int ldda, T *sA, int slda, T rA[Q_]) {
 #pragma unroll
   for (int j = 0; j < Q_; j++) {
     rA[j] = dA(tx, j);
@@ -87,7 +87,7 @@ static __device__ __inline__ void read_A_notrans_g2r_1D_nosync(const int tx, con
 // 1D thread config. with (Mx1) threads
 // no sync at the end of the function
 template <typename T, int P_, int NB_, int Q_>
-static __device__ __inline__ void read_A_trans_g2r_1D_nosync(const int tx, const int ty, const T* dA, int ldda, T* sA, int slda, T rA[Q_]) {
+static __device__ __inline__ void read_A_trans_g2r_1D_nosync(const int tx, const int ty, const T *dA, int ldda, T *sA, int slda, T rA[Q_]) {
   int       ix  = 0;
   const int nTH = P_ * MAGMA_NONTENSOR_BASIS_NTCOL(P_);
   const int tid = ty * blockDim.x + tx;
@@ -114,7 +114,7 @@ static __device__ __inline__ void read_A_trans_g2r_1D_nosync(const int tx, const
 // 1D thread config. with (Mx1) threads
 // no sync at the end of the function
 template <typename T, int P_, int NB_, int Q_>
-static __device__ __inline__ void read_B_g2s_1D_nosync(const int tx, int n, const T* dB, int lddb, T* sB, int sldb) {
+static __device__ __inline__ void read_B_g2s_1D_nosync(const int tx, int n, const T *dB, int lddb, T *sB, int sldb) {
   if (n != NB_) {
     for (int i = 0; i < (Q_ * n) - P_; i += P_) {
       sB[i + tx] = dB[i + tx];
@@ -140,7 +140,7 @@ static __device__ __inline__ void read_B_g2s_1D_nosync(const int tx, int n, cons
 // C in registers -- one row per thread
 // no sync at the end of the function
 template <typename T, int P_, int NB_, int Q_>
-static __device__ __inline__ void mul_rAsBrC_1D_nosync(const int tx, const T& alpha, T rA[Q_], T* sB, int sldb, T rC[NB_]) {
+static __device__ __inline__ void mul_rAsBrC_1D_nosync(const int tx, const T &alpha, T rA[Q_], T *sB, int sldb, T rC[NB_]) {
   T rB[Q_] = {0};
 #pragma unroll
   for (int i = 0; i < NB_; i++) {
diff --git a/include/ceed/jit-source/magma/magma_common_tensor.h b/include/ceed/jit-source/magma/magma_common_tensor.h
index 9810d74c..48ad0fa1 100644
--- a/include/ceed/jit-source/magma/magma_common_tensor.h
+++ b/include/ceed/jit-source/magma/magma_common_tensor.h
@@ -23,7 +23,7 @@
 // the devptr is assumed to point directly to the element
 // must sync after call
 template <typename T, int LENGTH, int NCOMP_>
-__device__ __inline__ void read_1d(const T* devptr, const int compstride, T* sBuffer[NCOMP_], const int tx) {
+__device__ __inline__ void read_1d(const T *devptr, const int compstride, T *sBuffer[NCOMP_], const int tx) {
   if (tx < LENGTH) {
     for (int icomp = 0; icomp < NCOMP_; icomp++) {
       sBuffer[icomp][tx] = devptr[icomp * compstride + tx];
@@ -35,7 +35,7 @@ __device__ __inline__ void read_1d(const T* devptr, const int compstride, T* sBu
 // write V of a 1D element into global memory from sV[][] --  for all components
 // the devptr is assumed to point directly to the element
 template <typename T, int LENGTH, int NCOMP_>
-__device__ __inline__ void write_1d(T* sBuffer[NCOMP_], T* devptr, const int compstride, const int tx) {
+__device__ __inline__ void write_1d(T *sBuffer[NCOMP_], T *devptr, const int compstride, const int tx) {
   if (tx < LENGTH) {
     for (int icomp = 0; icomp < NCOMP_; icomp++) {
       devptr[icomp * compstride + tx] = sBuffer[icomp][tx];
@@ -51,7 +51,7 @@ __device__ __inline__ void write_1d(T* sBuffer[NCOMP_], T* devptr, const int com
 // rUsize can be different from P_ (e.g. MAXP_Q)
 // sTmp is a shared memory workspace of size P_^2
 template <typename T, int P_, int DIMU, int NCOMP_, int rUsize, int iDIM>
-__device__ __inline__ void readU_2d(const T* dU, const int compstride, T rU[DIMU][NCOMP_][rUsize], T* sTmp, const int tx) {
+__device__ __inline__ void readU_2d(const T *dU, const int compstride, T rU[DIMU][NCOMP_][rUsize], T *sTmp, const int tx) {
   // read U as a batch P_ of (1xP_) vectors
   // vec 0  : [u0, u1, u2, ... u_(P_-1)] -- contiguous in memory
   // vec 1  : [u0, u1, u2, ... u_(P_-1)] -- contiguous in memory
@@ -87,7 +87,7 @@ __device__ __inline__ void readU_2d(const T* dU, const int compstride, T rU[DIMU
 // iDIM specifies which dimension is being read into in rV
 // rVsize can be different from P_ (e.g. MAXP_Q)
 template <typename T, int Q_, int DIMV, int NCOMP_, int rVsize, int iDIM>
-__device__ __inline__ void readV_2d(const T* dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
+__device__ __inline__ void readV_2d(const T *dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
   if (tx < Q_) {
     for (int icomp = 0; icomp < NCOMP_; icomp++) {
       for (int j = 0; j < Q_; j++) {
@@ -105,7 +105,7 @@ __device__ __inline__ void readV_2d(const T* dV, const int compstride, T rV[DIMV
 // idim specifies which dimension is being written to in dV
 // rVsize can be different from P_ (e.g. MAXP_Q)
 template <typename T, int Q_, int DIMV, int NCOMP_, int rVsize, int iDIM>
-__device__ __inline__ void writeV_2d(T* dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
+__device__ __inline__ void writeV_2d(T *dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
   if (tx < Q_) {
     for (int icomp = 0; icomp < NCOMP_; icomp++) {
       for (int j = 0; j < Q_; j++) {
@@ -123,7 +123,7 @@ __device__ __inline__ void writeV_2d(T* dV, const int compstride, T rV[DIMV][NCO
 // rUsize can be different from P_ (e.g. MAXP_Q)
 // sTmp is a shared memory workspace of size P_^3
 template <typename T, int P_, int DIMU, int NCOMP_, int rUsize, int iDIM>
-__device__ __inline__ void readU_3d(const T* dU, const int compstride, T rU[DIMU][NCOMP_][rUsize], T* sTmp, const int tx) {
+__device__ __inline__ void readU_3d(const T *dU, const int compstride, T rU[DIMU][NCOMP_][rUsize], T *sTmp, const int tx) {
   // read U as a batch P_^2 of (1xP_) vectors
   // vec 0    : [u0, u1, u2, ... u_(P_-1)] -- contiguous in memory
   // vec 1    : [u0, u1, u2, ... u_(P_-1)] -- contiguous in memory
@@ -159,7 +159,7 @@ __device__ __inline__ void readU_3d(const T* dU, const int compstride, T rU[DIMU
 // iDIM specifies which dimension is being read into in rV
 // rVsize can be different from P_ (e.g. MAXP_Q)
 template <typename T, int Q_, int DIMV, int NCOMP_, int rVsize, int iDIM>
-__device__ __inline__ void readV_3d(const T* dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
+__device__ __inline__ void readV_3d(const T *dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
   if (tx < Q_ * Q_) {
     for (int icomp = 0; icomp < NCOMP_; icomp++) {
       for (int j = 0; j < Q_; j++) {
@@ -177,7 +177,7 @@ __device__ __inline__ void readV_3d(const T* dV, const int compstride, T rV[DIMV
 // idim specifies which dimension is being written to in dV
 // rVsize can be different from P_ (e.g. MAXP_Q)
 template <typename T, int Q_, int DIMV, int NCOMP_, int rVsize, int iDIM>
-__device__ __inline__ void writeV_3d(T* dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
+__device__ __inline__ void writeV_3d(T *dV, const int compstride, T rV[DIMV][NCOMP_][rVsize], const int tx) {
   if (tx < (Q_ * Q_)) {
     for (int icomp = 0; icomp < NCOMP_; icomp++) {
       for (int j = 0; j < Q_; j++) {
@@ -191,7 +191,7 @@ __device__ __inline__ void writeV_3d(T* dV, const int compstride, T rV[DIMV][NCO
 // reads T into shared memory
 // must sync after call
 template <int B, int J>
-__device__ __inline__ void dread_T_gm2sm(const int tx, const magma_trans_t transT, const CeedScalar* dT, CeedScalar* sT) {
+__device__ __inline__ void dread_T_gm2sm(const int tx, const magma_trans_t transT, const CeedScalar *dT, CeedScalar *sT) {
   if (transT == MagmaNoTrans) {
     // T is B x J
     if (tx < B) {
@@ -214,7 +214,7 @@ __device__ __inline__ void dread_T_gm2sm(const int tx, const magma_trans_t trans
 // reads a slice of U from shared/global memory into registers
 // the correct pointer U must be precomputed
 template <int B>
-__device__ __inline__ void dread_U_gsm2reg(const int C, const int tx_, const CeedScalar* U, CeedScalar rU[B]) {
+__device__ __inline__ void dread_U_gsm2reg(const int C, const int tx_, const CeedScalar *U, CeedScalar rU[B]) {
   for (int i = 0; i < B; i++) {
     rU[i] = U[i * C + tx_];
   }
@@ -224,7 +224,7 @@ __device__ __inline__ void dread_U_gsm2reg(const int C, const int tx_, const Cee
 // reads a slice of V from shared/global memory into registers with scaling
 // the correct pointer V must be precomputed
 template <int J>
-__device__ __inline__ void dread_V_gsm2reg(const int C, const int tx_, const CeedScalar* V, CeedScalar rV[J]) {
+__device__ __inline__ void dread_V_gsm2reg(const int C, const int tx_, const CeedScalar *V, CeedScalar rV[J]) {
   for (int i = 0; i < J; i++) {
     rV[i] = V[i * C + tx_];
   }
@@ -234,7 +234,7 @@ __device__ __inline__ void dread_V_gsm2reg(const int C, const int tx_, const Cee
 // writes a slice of V from reg to shared/global memory
 // the correct pointer V must be precomputed
 template <int J>
-__device__ __inline__ void dwrite_V_reg2gsm(const int C, const int tx_, CeedScalar rV[J], CeedScalar* V) {
+__device__ __inline__ void dwrite_V_reg2gsm(const int C, const int tx_, CeedScalar rV[J], CeedScalar *V) {
   for (int i = 0; i < J; i++) {
     V[i * C + tx_] = rV[i];
   }
@@ -243,7 +243,7 @@ __device__ __inline__ void dwrite_V_reg2gsm(const int C, const int tx_, CeedScal
 //////////////////////////////////////////////////////////////////////////////////////////
 // multiply a slice of U times T to produce a slice of V
 template <int B, int J>
-__device__ __inline__ void dgemm_slice(CeedScalar alpha, CeedScalar* sT, CeedScalar rU[B], CeedScalar beta, CeedScalar rV[J]) {
+__device__ __inline__ void dgemm_slice(CeedScalar alpha, CeedScalar *sT, CeedScalar rU[B], CeedScalar beta, CeedScalar rV[J]) {
   CeedScalar rTmp;
   for (int j = 0; j < J; j++) {
     rTmp = 0.0;
@@ -257,8 +257,8 @@ __device__ __inline__ void dgemm_slice(CeedScalar alpha, CeedScalar* sT, CeedSca
 
 //////////////////////////////////////////////////////////////////////////////////////////
 template <int B, int J>
-__device__ __inline__ void dgemm_ceed_device(const int tx, const int A, const int C, magma_trans_t transT, CeedScalar* sT, const CeedScalar alpha,
-                                             const CeedScalar beta, const CeedScalar* dU, CeedScalar* dV, CeedScalar rU[B], CeedScalar rV[J]) {
+__device__ __inline__ void dgemm_ceed_device(const int tx, const int A, const int C, magma_trans_t transT, CeedScalar *sT, const CeedScalar alpha,
+                                             const CeedScalar beta, const CeedScalar *dU, CeedScalar *dV, CeedScalar rU[B], CeedScalar rV[J]) {
   const int tx_      = tx % C;
   const int slice_id = tx / C;
 
@@ -268,7 +268,7 @@ __device__ __inline__ void dgemm_ceed_device(const int tx, const int A, const in
 
   // read V if beta is non-zero
   if (beta != 0.0) {
-    dread_V_gsm2reg<J>(C, tx_, (const CeedScalar*)dV, rV);
+    dread_V_gsm2reg<J>(C, tx_, (const CeedScalar *)dV, rV);
   }
 
   // read U
diff --git a/include/ceed/jit-source/magma/weight-1d.h b/include/ceed/jit-source/magma/weight-1d.h
index 93a671ed..e4a7abe1 100644
--- a/include/ceed/jit-source/magma/weight-1d.h
+++ b/include/ceed/jit-source/magma/weight-1d.h
@@ -8,7 +8,7 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 // weight basis action -- 1D
 template <typename T, int Q_>
-__device__ __inline__ void magma_weight_1d_device(const T* sTweight, T* sV, const int tx) {
+__device__ __inline__ void magma_weight_1d_device(const T *sTweight, T *sV, const int tx) {
   // Assumptions
   // 1. 1D thread configuration of size Q_
   // 2. The output sV is in shared memory -- size 1xQ_
@@ -19,7 +19,7 @@ __device__ __inline__ void magma_weight_1d_device(const T* sTweight, T* sV, cons
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(Q, MAGMA_MAXTHREADS_1D)) __global__
-    void magma_weight_1d_kernel(const CeedScalar* dqweight1d, CeedScalar* dV, const int v_stride, const int nelem) {
+    void magma_weight_1d_kernel(const CeedScalar *dqweight1d, CeedScalar *dV, const int v_stride, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
   const int tx      = threadIdx.x;
@@ -32,8 +32,8 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(Q, MAGMA_MAXTHREADS_1D)) __globa
   dV += elem_id * v_stride;
 
   // shared memory pointers
-  CeedScalar* sTweight = (CeedScalar*)shared_data;
-  CeedScalar* sV       = sTweight + Q;
+  CeedScalar *sTweight = (CeedScalar *)shared_data;
+  CeedScalar *sV       = sTweight + Q;
   sV += ty * Q;
 
   // read dqweight_1d
diff --git a/include/ceed/jit-source/magma/weight-2d.h b/include/ceed/jit-source/magma/weight-2d.h
index 23c56d15..7ad62d73 100644
--- a/include/ceed/jit-source/magma/weight-2d.h
+++ b/include/ceed/jit-source/magma/weight-2d.h
@@ -8,7 +8,7 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 // weight basis action -- 2D
 template <typename T, int DIM_, int NCOMP_, int Q_, int iDIM, int iCOMP>
-__device__ __inline__ void magma_weight_2d_device(const T* sTweight, T rV[DIM_][NCOMP_][Q_], const int tx) {
+__device__ __inline__ void magma_weight_2d_device(const T *sTweight, T rV[DIM_][NCOMP_][Q_], const int tx) {
   // Assumptions
   // 1. 1D thread configuration of size Q_
   // 2. rV[][][] matches the storage used in other actions (interp, grad, ... etc)
@@ -27,7 +27,7 @@ __device__ __inline__ void magma_weight_2d_device(const T* sTweight, T rV[DIM_][
 
 //////////////////////////////////////////////////////////////////////////////////////////
 extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(Q, MAGMA_MAXTHREADS_2D)) __global__
-    void magma_weight_2d_kernel(const CeedScalar* dqweight1d, CeedScalar* dV, const int v_stride, const int nelem) {
+    void magma_weight_2d_kernel(const CeedScalar *dqweight1d, CeedScalar *dV, const int v_stride, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
   const int tx      = threadIdx.x;
@@ -41,7 +41,7 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(Q, MAGMA_MAXTHREADS_2D)) __globa
   dV += elem_id * v_stride;
 
   // shared memory pointers
-  CeedScalar* sTweight = (CeedScalar*)shared_data;
+  CeedScalar *sTweight = (CeedScalar *)shared_data;
 
   // read dqweight_1d
   if (ty == 0 && tx < Q) {
diff --git a/include/ceed/jit-source/magma/weight-3d.h b/include/ceed/jit-source/magma/weight-3d.h
index 98ac14d2..07fc2286 100644
--- a/include/ceed/jit-source/magma/weight-3d.h
+++ b/include/ceed/jit-source/magma/weight-3d.h
@@ -8,7 +8,7 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 // weight basis action -- 3D
 template <typename T, int DIM_, int NCOMP_, int Q_, int iDIM, int iCOMP>
-__device__ __inline__ void magma_weight_3d_device(const T* sTweight, T rV[DIM_][NCOMP_][Q_], const int tx) {
+__device__ __inline__ void magma_weight_3d_device(const T *sTweight, T rV[DIM_][NCOMP_][Q_], const int tx) {
   // Assumptions
   // 1. 1D thread configuration of size Q_^2
   // 2. rV[][][] matches the storage used in other actions (interp, grad, ... etc)
@@ -27,8 +27,8 @@ __device__ __inline__ void magma_weight_3d_device(const T* sTweight, T rV[DIM_][
 }
 
 //////////////////////////////////////////////////////////////////////////////////////////
-extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(Q* Q, MAGMA_MAXTHREADS_3D)) __global__
-    void magma_weight_3d_kernel(const CeedScalar* dqweight1d, CeedScalar* dV, const int v_stride, const int nelem) {
+extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(Q *Q, MAGMA_MAXTHREADS_3D)) __global__
+    void magma_weight_3d_kernel(const CeedScalar *dqweight1d, CeedScalar *dV, const int v_stride, const int nelem) {
   MAGMA_DEVICE_SHARED(CeedScalar, shared_data)
 
   const int tx      = threadIdx.x;
@@ -42,7 +42,7 @@ extern "C" __launch_bounds__(MAGMA_BASIS_BOUNDS(Q* Q, MAGMA_MAXTHREADS_3D)) __gl
   dV += elem_id * v_stride;
 
   // shared memory pointers
-  CeedScalar* sTweight = (CeedScalar*)shared_data;
+  CeedScalar *sTweight = (CeedScalar *)shared_data;
 
   // read dqweight_1d
   if (tx < Q) {
diff --git a/include/ceed/jit-source/sycl/sycl-gen-templates.h b/include/ceed/jit-source/sycl/sycl-gen-templates.h
index 25a3a2d2..88aa7b98 100644
--- a/include/ceed/jit-source/sycl/sycl-gen-templates.h
+++ b/include/ceed/jit-source/sycl/sycl-gen-templates.h
@@ -20,7 +20,7 @@ typedef atomic_double CeedAtomicScalar;
 //------------------------------------------------------------------------------
 // Load matrices for basis actions
 //------------------------------------------------------------------------------
-inline void loadMatrix(const CeedInt N, const CeedScalar* restrict d_B, CeedScalar* restrict B) {
+inline void loadMatrix(const CeedInt N, const CeedScalar *restrict d_B, CeedScalar *restrict B) {
   const CeedInt item_id    = get_local_linear_id();
   const CeedInt group_size = get_local_size(0) * get_local_size(1) * get_local_size(2);
   for (CeedInt i = item_id; i < N; i += group_size) B[i] = d_B[i];
@@ -34,7 +34,7 @@ inline void loadMatrix(const CeedInt N, const CeedScalar* restrict d_B, CeedScal
 // L-vector -> E-vector, offsets provided
 //------------------------------------------------------------------------------
 inline void readDofsOffset1d(const CeedInt num_comp, const CeedInt strides_comp, const CeedInt P_1D, const CeedInt num_elem,
-                             const global CeedInt* restrict indices, const global CeedScalar* restrict d_u, private CeedScalar* restrict r_u) {
+                             const global CeedInt *restrict indices, const global CeedScalar *restrict d_u, private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt elem      = get_global_id(2);
 
@@ -51,8 +51,8 @@ inline void readDofsOffset1d(const CeedInt num_comp, const CeedInt strides_comp,
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
 inline void readDofsStrided1d(const CeedInt num_comp, const CeedInt P_1D, const CeedInt strides_node, const CeedInt strides_comp,
-                              const CeedInt strides_elem, const CeedInt num_elem, global const CeedScalar* restrict d_u,
-                              private CeedScalar* restrict r_u) {
+                              const CeedInt strides_elem, const CeedInt num_elem, global const CeedScalar *restrict d_u,
+                              private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt elem      = get_global_id(2);
 
@@ -69,7 +69,7 @@ inline void readDofsStrided1d(const CeedInt num_comp, const CeedInt P_1D, const
 // E-vector -> L-vector, offsets provided
 //------------------------------------------------------------------------------
 inline void writeDofsOffset1d(const CeedInt num_comp, const CeedInt strides_comp, const CeedInt P_1D, const CeedInt num_elem,
-                              const global CeedInt* restrict indices, const private CeedScalar* restrict r_v, global CeedAtomicScalar* restrict d_v) {
+                              const global CeedInt *restrict indices, const private CeedScalar *restrict r_v, global CeedAtomicScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt elem      = get_global_id(2);
 
@@ -85,8 +85,8 @@ inline void writeDofsOffset1d(const CeedInt num_comp, const CeedInt strides_comp
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
 inline void writeDofsStrided1d(const CeedInt num_comp, const CeedInt P_1D, const CeedInt strides_node, const CeedInt strides_comp,
-                               const CeedInt strides_elem, const CeedInt num_elem, private const CeedScalar* restrict r_v,
-                               global CeedScalar* restrict d_v) {
+                               const CeedInt strides_elem, const CeedInt num_elem, private const CeedScalar *restrict r_v,
+                               global CeedScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt elem      = get_global_id(2);
 
@@ -107,7 +107,7 @@ inline void writeDofsStrided1d(const CeedInt num_comp, const CeedInt P_1D, const
 // L-vector -> E-vector, offsets provided
 //------------------------------------------------------------------------------
 inline void readDofsOffset2d(const CeedInt num_comp, const CeedInt strides_comp, const CeedInt P_1D, const CeedInt num_elem,
-                             const global CeedInt* restrict indices, const global CeedScalar* restrict d_u, private CeedScalar* restrict r_u) {
+                             const global CeedInt *restrict indices, const global CeedScalar *restrict d_u, private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -123,8 +123,8 @@ inline void readDofsOffset2d(const CeedInt num_comp, const CeedInt strides_comp,
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
 inline void readDofsStrided2d(const CeedInt num_comp, const CeedInt P_1D, const CeedInt strides_node, const CeedInt strides_comp,
-                              const CeedInt strides_elem, const CeedInt num_elem, const global CeedScalar* restrict d_u,
-                              private CeedScalar* restrict r_u) {
+                              const CeedInt strides_elem, const CeedInt num_elem, const global CeedScalar *restrict d_u,
+                              private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -140,7 +140,7 @@ inline void readDofsStrided2d(const CeedInt num_comp, const CeedInt P_1D, const
 // E-vector -> L-vector, offsets provided
 //------------------------------------------------------------------------------
 inline void writeDofsOffset2d(const CeedInt num_comp, const CeedInt strides_comp, const CeedInt P_1D, const CeedInt num_elem,
-                              const global CeedInt* restrict indices, const private CeedScalar* restrict r_v, global CeedAtomicScalar* restrict d_v) {
+                              const global CeedInt *restrict indices, const private CeedScalar *restrict r_v, global CeedAtomicScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -157,8 +157,8 @@ inline void writeDofsOffset2d(const CeedInt num_comp, const CeedInt strides_comp
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
 inline void writeDofsStrided2d(const CeedInt num_comp, const CeedInt P_1D, const CeedInt strides_node, const CeedInt strides_comp,
-                               const CeedInt strides_elem, const CeedInt num_elem, const private CeedScalar* restrict r_v,
-                               global CeedScalar* restrict d_v) {
+                               const CeedInt strides_elem, const CeedInt num_elem, const private CeedScalar *restrict r_v,
+                               global CeedScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -178,7 +178,7 @@ inline void writeDofsStrided2d(const CeedInt num_comp, const CeedInt P_1D, const
 // L-vector -> E-vector, offsets provided
 //------------------------------------------------------------------------------
 inline void readDofsOffset3d(const CeedInt num_comp, const CeedInt strides_comp, const CeedInt P_1D, const CeedInt num_elem,
-                             const global CeedInt* restrict indices, const global CeedScalar* restrict d_u, private CeedScalar* restrict r_u) {
+                             const global CeedInt *restrict indices, const global CeedScalar *restrict d_u, private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -196,8 +196,8 @@ inline void readDofsOffset3d(const CeedInt num_comp, const CeedInt strides_comp,
 // L-vector -> E-vector, strided
 //------------------------------------------------------------------------------
 inline void readDofsStrided3d(const CeedInt num_comp, const CeedInt P_1D, const CeedInt strides_node, const CeedInt strides_comp,
-                              const CeedInt strides_elem, const CeedInt num_elem, const global CeedScalar* restrict d_u,
-                              private CeedScalar* restrict r_u) {
+                              const CeedInt strides_elem, const CeedInt num_elem, const global CeedScalar *restrict d_u,
+                              private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -215,7 +215,7 @@ inline void readDofsStrided3d(const CeedInt num_comp, const CeedInt P_1D, const
 // E-vector -> Q-vector, offests provided
 //------------------------------------------------------------------------------
 inline void readSliceQuadsOffset3d(const CeedInt num_comp, const CeedInt strides_comp, const CeedInt Q_1D, const CeedInt num_elem, const CeedInt q,
-                                   const global CeedInt* restrict indices, const global CeedScalar* restrict d_u, private CeedScalar* restrict r_u) {
+                                   const global CeedInt *restrict indices, const global CeedScalar *restrict d_u, private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -231,8 +231,8 @@ inline void readSliceQuadsOffset3d(const CeedInt num_comp, const CeedInt strides
 // E-vector -> Q-vector, strided
 //------------------------------------------------------------------------------
 inline void readSliceQuadsStrided3d(const CeedInt num_comp, const CeedInt Q_1D, CeedInt strides_node, CeedInt strides_comp, CeedInt strides_elem,
-                                    const CeedInt num_elem, const CeedInt q, const global CeedScalar* restrict d_u,
-                                    private CeedScalar* restrict r_u) {
+                                    const CeedInt num_elem, const CeedInt q, const global CeedScalar *restrict d_u,
+                                    private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -248,7 +248,7 @@ inline void readSliceQuadsStrided3d(const CeedInt num_comp, const CeedInt Q_1D,
 // E-vector -> L-vector, offsets provided
 //------------------------------------------------------------------------------
 inline void writeDofsOffset3d(const CeedInt num_comp, const CeedInt strides_comp, const CeedInt P_1D, const CeedInt num_elem,
-                              const global CeedInt* restrict indices, const private CeedScalar* restrict r_v, global CeedAtomicScalar* restrict d_v) {
+                              const global CeedInt *restrict indices, const private CeedScalar *restrict r_v, global CeedAtomicScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -267,8 +267,8 @@ inline void writeDofsOffset3d(const CeedInt num_comp, const CeedInt strides_comp
 // E-vector -> L-vector, strided
 //------------------------------------------------------------------------------
 inline void writeDofsStrided3d(const CeedInt num_comp, const CeedInt P_1D, const CeedInt strides_node, const CeedInt strides_comp,
-                               const CeedInt strides_elem, const CeedInt num_elem, const private CeedScalar* restrict r_v,
-                               global CeedScalar* restrict d_v) {
+                               const CeedInt strides_elem, const CeedInt num_elem, const private CeedScalar *restrict r_v,
+                               global CeedScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -285,8 +285,8 @@ inline void writeDofsStrided3d(const CeedInt num_comp, const CeedInt P_1D, const
 //------------------------------------------------------------------------------
 // 3D collocated derivatives computation
 //------------------------------------------------------------------------------
-inline void gradCollo3d(const CeedInt num_comp, const CeedInt Q_1D, const CeedInt q, const private CeedScalar* restrict r_U,
-                        const local CeedScalar* s_G, private CeedScalar* restrict r_V, local CeedScalar* restrict scratch) {
+inline void gradCollo3d(const CeedInt num_comp, const CeedInt Q_1D, const CeedInt q, const private CeedScalar *restrict r_U,
+                        const local CeedScalar *s_G, private CeedScalar *restrict r_V, local CeedScalar *restrict scratch) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
 
@@ -319,8 +319,8 @@ inline void gradCollo3d(const CeedInt num_comp, const CeedInt Q_1D, const CeedIn
 //------------------------------------------------------------------------------
 // 3D collocated derivatives transpose
 //------------------------------------------------------------------------------
-inline void gradColloTranspose3d(const CeedInt num_comp, const CeedInt Q_1D, const CeedInt q, const private CeedScalar* restrict r_U,
-                                 const local CeedScalar* restrict s_G, private CeedScalar* restrict r_V, local CeedScalar* restrict scratch) {
+inline void gradColloTranspose3d(const CeedInt num_comp, const CeedInt Q_1D, const CeedInt q, const private CeedScalar *restrict r_U,
+                                 const local CeedScalar *restrict s_G, private CeedScalar *restrict r_V, local CeedScalar *restrict scratch) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
 
diff --git a/include/ceed/jit-source/sycl/sycl-shared-basis-read-write-templates.h b/include/ceed/jit-source/sycl/sycl-shared-basis-read-write-templates.h
index be8471f7..eb04b926 100644
--- a/include/ceed/jit-source/sycl/sycl-shared-basis-read-write-templates.h
+++ b/include/ceed/jit-source/sycl/sycl-shared-basis-read-write-templates.h
@@ -16,7 +16,7 @@
 //------------------------------------------------------------------------------
 // Helper function: load matrices for basis actions
 //------------------------------------------------------------------------------
-inline void loadMatrix(const CeedInt N, const CeedScalar* restrict d_B, CeedScalar* restrict B) {
+inline void loadMatrix(const CeedInt N, const CeedScalar *restrict d_B, CeedScalar *restrict B) {
   const CeedInt item_id    = get_local_linear_id();
   const CeedInt group_size = get_local_size(0) * get_local_size(1) * get_local_size(2);
   for (CeedInt i = item_id; i < N; i += group_size) B[i] = d_B[i];
@@ -30,8 +30,8 @@ inline void loadMatrix(const CeedInt N, const CeedScalar* restrict d_B, CeedScal
 // E-vector -> single element
 //------------------------------------------------------------------------------
 inline void ReadElementStrided1d(const CeedInt NUM_COMP, const CeedInt P_1D, const CeedInt num_elem, const CeedInt strides_node,
-                                 const CeedInt strides_comp, const CeedInt strides_elem, global const CeedScalar* restrict d_u,
-                                 private CeedScalar* restrict r_u) {
+                                 const CeedInt strides_comp, const CeedInt strides_elem, global const CeedScalar *restrict d_u,
+                                 private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt elem      = get_global_id(2);
 
@@ -48,8 +48,8 @@ inline void ReadElementStrided1d(const CeedInt NUM_COMP, const CeedInt P_1D, con
 // Single element -> E-vector
 //------------------------------------------------------------------------------
 inline void WriteElementStrided1d(const CeedInt NUM_COMP, const CeedInt P_1D, const CeedInt num_elem, const CeedInt strides_node,
-                                  const CeedInt strides_comp, const CeedInt strides_elem, private const CeedScalar* restrict r_v,
-                                  global CeedScalar* restrict d_v) {
+                                  const CeedInt strides_comp, const CeedInt strides_elem, private const CeedScalar *restrict r_v,
+                                  global CeedScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt elem      = get_global_id(2);
 
@@ -70,8 +70,8 @@ inline void WriteElementStrided1d(const CeedInt NUM_COMP, const CeedInt P_1D, co
 // E-vector -> single element
 //------------------------------------------------------------------------------
 inline void ReadElementStrided2d(const CeedInt NUM_COMP, const CeedInt P_1D, const CeedInt num_elem, const CeedInt strides_node,
-                                 const CeedInt strides_comp, const CeedInt strides_elem, global const CeedScalar* restrict d_u,
-                                 private CeedScalar* restrict r_u) {
+                                 const CeedInt strides_comp, const CeedInt strides_elem, global const CeedScalar *restrict d_u,
+                                 private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -89,8 +89,8 @@ inline void ReadElementStrided2d(const CeedInt NUM_COMP, const CeedInt P_1D, con
 // Single element -> E-vector
 //------------------------------------------------------------------------------
 inline void WriteElementStrided2d(const CeedInt NUM_COMP, const CeedInt P_1D, const CeedInt num_elem, const CeedInt strides_node,
-                                  const CeedInt strides_comp, const CeedInt strides_elem, private const CeedScalar* restrict r_v,
-                                  global CeedScalar* restrict d_v) {
+                                  const CeedInt strides_comp, const CeedInt strides_elem, private const CeedScalar *restrict r_v,
+                                  global CeedScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -112,8 +112,8 @@ inline void WriteElementStrided2d(const CeedInt NUM_COMP, const CeedInt P_1D, co
 // E-vector -> single element
 //------------------------------------------------------------------------------
 inline void ReadElementStrided3d(const CeedInt NUM_COMP, const CeedInt P_1D, const CeedInt num_elem, const CeedInt strides_node,
-                                 const CeedInt strides_comp, const CeedInt strides_elem, global const CeedScalar* restrict d_u,
-                                 private CeedScalar* restrict r_u) {
+                                 const CeedInt strides_comp, const CeedInt strides_elem, global const CeedScalar *restrict d_u,
+                                 private CeedScalar *restrict r_u) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
@@ -133,8 +133,8 @@ inline void ReadElementStrided3d(const CeedInt NUM_COMP, const CeedInt P_1D, con
 // Single element -> E-vector
 //------------------------------------------------------------------------------
 inline void WriteElementStrided3d(const CeedInt NUM_COMP, const CeedInt P_1D, const CeedInt num_elem, const CeedInt strides_node,
-                                  const CeedInt strides_comp, const CeedInt strides_elem, private const CeedScalar* restrict r_v,
-                                  global CeedScalar* restrict d_v) {
+                                  const CeedInt strides_comp, const CeedInt strides_elem, private const CeedScalar *restrict r_v,
+                                  global CeedScalar *restrict d_v) {
   const CeedInt item_id_x = get_local_id(0);
   const CeedInt item_id_y = get_local_id(1);
   const CeedInt elem      = get_global_id(2);
diff --git a/include/ceed/jit-source/sycl/sycl-shared-basis-tensor.h b/include/ceed/jit-source/sycl/sycl-shared-basis-tensor.h
index 8803e1bc..37cd6448 100644
--- a/include/ceed/jit-source/sycl/sycl-shared-basis-tensor.h
+++ b/include/ceed/jit-source/sycl/sycl-shared-basis-tensor.h
@@ -22,8 +22,8 @@
 //------------------------------------------------------------------------------
 // Interp kernel by dim
 //------------------------------------------------------------------------------
-kernel void Interp(const CeedInt num_elem, global const CeedScalar* restrict d_interp_1d, global const CeedScalar* restrict d_U,
-                   global CeedScalar* restrict d_V) {
+kernel void Interp(const CeedInt num_elem, global const CeedScalar *restrict d_interp_1d, global const CeedScalar *restrict d_U,
+                   global CeedScalar *restrict d_V) {
   local CeedScalar s_B[BASIS_P_1D * BASIS_Q_1D];
  private
   CeedScalar r_U[BASIS_NUM_COMP * (BASIS_DIM > 2 ? BASIS_P_1D : 1)];
@@ -31,7 +31,7 @@ kernel void Interp(const CeedInt num_elem, global const CeedScalar* restrict d_i
   CeedScalar r_V[BASIS_NUM_COMP * (BASIS_DIM > 2 ? BASIS_Q_1D : 1)];
 
   local CeedScalar  scratch[BASIS_INTERP_SCRATCH_SIZE];
-  local CeedScalar* elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
+  local CeedScalar *elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
 
   loadMatrix(BASIS_P_1D * BASIS_Q_1D, d_interp_1d, s_B);
   work_group_barrier(CLK_LOCAL_MEM_FENCE);
@@ -53,8 +53,8 @@ kernel void Interp(const CeedInt num_elem, global const CeedScalar* restrict d_i
   }
 }
 
-kernel void InterpTranspose(const CeedInt num_elem, global const CeedScalar* restrict d_interp_1d, global const CeedScalar* restrict d_U,
-                            global CeedScalar* restrict d_V) {
+kernel void InterpTranspose(const CeedInt num_elem, global const CeedScalar *restrict d_interp_1d, global const CeedScalar *restrict d_U,
+                            global CeedScalar *restrict d_V) {
   // local size:
   // 1d: elems_per_block * T_1d
   // 2d,3d: elems_per_block * T_1d * T_1d
@@ -65,7 +65,7 @@ kernel void InterpTranspose(const CeedInt num_elem, global const CeedScalar* res
   CeedScalar r_V[BASIS_NUM_COMP * (BASIS_DIM > 2 ? BASIS_P_1D : 1)];
 
   local CeedScalar  scratch[BASIS_INTERP_SCRATCH_SIZE];
-  local CeedScalar* elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
+  local CeedScalar *elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
 
   loadMatrix(BASIS_P_1D * BASIS_Q_1D, d_interp_1d, s_B);
   work_group_barrier(CLK_LOCAL_MEM_FENCE);
@@ -90,8 +90,8 @@ kernel void InterpTranspose(const CeedInt num_elem, global const CeedScalar* res
 //------------------------------------------------------------------------------
 // Grad kernel by dim
 //------------------------------------------------------------------------------
-kernel void Grad(const CeedInt num_elem, global const CeedScalar* restrict d_interp_1d, global const CeedScalar* restrict d_grad_1d,
-                 global const CeedScalar* restrict d_U, global CeedScalar* restrict d_V) {
+kernel void Grad(const CeedInt num_elem, global const CeedScalar *restrict d_interp_1d, global const CeedScalar *restrict d_grad_1d,
+                 global const CeedScalar *restrict d_U, global CeedScalar *restrict d_V) {
   local CeedScalar s_B[BASIS_P_1D * BASIS_Q_1D];  // Todo, don't allocate s_B for dimension 1
   local CeedScalar s_G[BASIS_Q_1D * (BASIS_HAS_COLLOCATED_GRAD ? BASIS_Q_1D : BASIS_P_1D)];
 
@@ -101,7 +101,7 @@ kernel void Grad(const CeedInt num_elem, global const CeedScalar* restrict d_int
   CeedScalar r_V[BASIS_NUM_COMP * BASIS_DIM * (BASIS_DIM > 2 ? BASIS_Q_1D : 1)];
 
   local CeedScalar  scratch[BASIS_GRAD_SCRATCH_SIZE];
-  local CeedScalar* elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
+  local CeedScalar *elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
 
   loadMatrix(BASIS_P_1D * BASIS_Q_1D, d_interp_1d, s_B);
   loadMatrix(BASIS_Q_1D * (BASIS_HAS_COLLOCATED_GRAD ? BASIS_Q_1D : BASIS_P_1D), d_grad_1d, s_G);
@@ -125,8 +125,8 @@ kernel void Grad(const CeedInt num_elem, global const CeedScalar* restrict d_int
   }
 }
 
-kernel void GradTranspose(const CeedInt num_elem, global const CeedScalar* restrict d_interp_1d, global const CeedScalar* restrict d_grad_1d,
-                          global const CeedScalar* restrict d_U, global CeedScalar* restrict d_V) {
+kernel void GradTranspose(const CeedInt num_elem, global const CeedScalar *restrict d_interp_1d, global const CeedScalar *restrict d_grad_1d,
+                          global const CeedScalar *restrict d_U, global CeedScalar *restrict d_V) {
   local CeedScalar s_B[BASIS_P_1D * BASIS_Q_1D];  // Todo, don't allocate s_B for dimension 1
   local CeedScalar s_G[BASIS_Q_1D * (BASIS_HAS_COLLOCATED_GRAD ? BASIS_Q_1D : BASIS_P_1D)];
 
@@ -136,7 +136,7 @@ kernel void GradTranspose(const CeedInt num_elem, global const CeedScalar* restr
   CeedScalar r_V[BASIS_NUM_COMP * (BASIS_DIM > 2 ? BASIS_P_1D : 1)];
 
   local CeedScalar  scratch[BASIS_GRAD_SCRATCH_SIZE];
-  local CeedScalar* elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
+  local CeedScalar *elem_scratch = scratch + get_local_id(2) * T_1D * (BASIS_DIM > 1 ? T_1D : 1);
 
   loadMatrix(BASIS_P_1D * BASIS_Q_1D, d_interp_1d, s_B);
   loadMatrix(BASIS_Q_1D * (BASIS_HAS_COLLOCATED_GRAD ? BASIS_Q_1D : BASIS_P_1D), d_grad_1d, s_G);
@@ -163,7 +163,7 @@ kernel void GradTranspose(const CeedInt num_elem, global const CeedScalar* restr
 //------------------------------------------------------------------------------
 // Weight kernels by dim
 //------------------------------------------------------------------------------
-kernel void Weight(const CeedInt num_elem, global const CeedScalar* restrict q_weight_1d, global CeedScalar* restrict d_W) {
+kernel void Weight(const CeedInt num_elem, global const CeedScalar *restrict q_weight_1d, global CeedScalar *restrict d_W) {
  private
   CeedScalar r_W[BASIS_DIM > 2 ? BASIS_Q_1D : 1];
 
diff --git a/include/ceed/jit-source/sycl/sycl-types.h b/include/ceed/jit-source/sycl/sycl-types.h
index 71a44915..f93d9ec4 100644
--- a/include/ceed/jit-source/sycl/sycl-types.h
+++ b/include/ceed/jit-source/sycl/sycl-types.h
@@ -16,23 +16,23 @@
 
 #ifdef __OPENCL_C_VERSION__
 typedef struct {
-  global const CeedScalar* inputs[CEED_SYCL_NUMBER_FIELDS];
-  global CeedScalar*       outputs[CEED_SYCL_NUMBER_FIELDS];
+  global const CeedScalar *inputs[CEED_SYCL_NUMBER_FIELDS];
+  global CeedScalar       *outputs[CEED_SYCL_NUMBER_FIELDS];
 } Fields_Sycl;
 
 typedef struct {
-  global const CeedInt* inputs[CEED_SYCL_NUMBER_FIELDS];
-  global CeedInt*       outputs[CEED_SYCL_NUMBER_FIELDS];
+  global const CeedInt *inputs[CEED_SYCL_NUMBER_FIELDS];
+  global CeedInt       *outputs[CEED_SYCL_NUMBER_FIELDS];
 } FieldsInt_Sycl;
 #else
 typedef struct {
-  const CeedScalar* inputs[CEED_SYCL_NUMBER_FIELDS];
-  CeedScalar*       outputs[CEED_SYCL_NUMBER_FIELDS];
+  const CeedScalar *inputs[CEED_SYCL_NUMBER_FIELDS];
+  CeedScalar       *outputs[CEED_SYCL_NUMBER_FIELDS];
 } Fields_Sycl;
 
 typedef struct {
-  const CeedInt* inputs[CEED_SYCL_NUMBER_FIELDS];
-  CeedInt*       outputs[CEED_SYCL_NUMBER_FIELDS];
+  const CeedInt *inputs[CEED_SYCL_NUMBER_FIELDS];
+  CeedInt       *outputs[CEED_SYCL_NUMBER_FIELDS];
 } FieldsInt_Sycl;
 #endif
 
diff --git a/interface/ceed-operator.c b/interface/ceed-operator.c
index 1c85459a..3c2f8494 100644
--- a/interface/ceed-operator.c
+++ b/interface/ceed-operator.c
@@ -26,31 +26,31 @@
 
   @param[in] ceed     Ceed object for error handling
   @param[in] qf_field QFunction Field matching Operator Field
-  @param[in] r        Operator Field ElemRestriction
-  @param[in] b        Operator Field Basis
+  @param[in] rstr     Operator Field ElemRestriction
+  @param[in] basis    Operator Field Basis
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref Developer
 **/
-static int CeedOperatorCheckField(Ceed ceed, CeedQFunctionField qf_field, CeedElemRestriction r, CeedBasis b) {
+static int CeedOperatorCheckField(Ceed ceed, CeedQFunctionField qf_field, CeedElemRestriction rstr, CeedBasis basis) {
   CeedInt      dim = 1, num_comp = 1, q_comp = 1, rstr_num_comp = 1, size = qf_field->size;
   CeedEvalMode eval_mode = qf_field->eval_mode;
 
   // Restriction
-  CeedCheck((r == CEED_ELEMRESTRICTION_NONE) == (eval_mode == CEED_EVAL_WEIGHT), ceed, CEED_ERROR_INCOMPATIBLE,
+  CeedCheck((rstr == CEED_ELEMRESTRICTION_NONE) == (eval_mode == CEED_EVAL_WEIGHT), ceed, CEED_ERROR_INCOMPATIBLE,
             "CEED_ELEMRESTRICTION_NONE and CEED_EVAL_WEIGHT must be used together.");
-  if (r != CEED_ELEMRESTRICTION_NONE) {
-    CeedCall(CeedElemRestrictionGetNumComponents(r, &rstr_num_comp));
+  if (rstr != CEED_ELEMRESTRICTION_NONE) {
+    CeedCall(CeedElemRestrictionGetNumComponents(rstr, &rstr_num_comp));
   }
   // Basis
-  CeedCheck((b == CEED_BASIS_NONE) == (eval_mode == CEED_EVAL_NONE), ceed, CEED_ERROR_INCOMPATIBLE,
+  CeedCheck((basis == CEED_BASIS_NONE) == (eval_mode == CEED_EVAL_NONE), ceed, CEED_ERROR_INCOMPATIBLE,
             "CEED_BASIS_NONE and CEED_EVAL_NONE must be used together.");
-  if (b != CEED_BASIS_NONE) {
-    CeedCall(CeedBasisGetDimension(b, &dim));
-    CeedCall(CeedBasisGetNumComponents(b, &num_comp));
-    CeedCall(CeedBasisGetNumQuadratureComponents(b, eval_mode, &q_comp));
-    CeedCheck(r == CEED_ELEMRESTRICTION_NONE || rstr_num_comp == num_comp, ceed, CEED_ERROR_DIMENSION,
+  if (basis != CEED_BASIS_NONE) {
+    CeedCall(CeedBasisGetDimension(basis, &dim));
+    CeedCall(CeedBasisGetNumComponents(basis, &num_comp));
+    CeedCall(CeedBasisGetNumQuadratureComponents(basis, eval_mode, &q_comp));
+    CeedCheck(rstr == CEED_ELEMRESTRICTION_NONE || rstr_num_comp == num_comp, ceed, CEED_ERROR_DIMENSION,
               "Field '%s' of size %" CeedInt_FMT " and EvalMode %s: ElemRestriction has %" CeedInt_FMT " components, but Basis has %" CeedInt_FMT
               " components",
               qf_field->field_name, qf_field->size, CeedEvalModes[qf_field->eval_mode], rstr_num_comp, num_comp);
@@ -168,10 +168,48 @@ int CeedOperatorGetActiveBasis(CeedOperator op, CeedBasis *active_basis) {
   return CEED_ERROR_SUCCESS;
 }
 
+/**
+  @brief Find the active input and output vector bases for a non-composite CeedOperator
+
+  @param[in] op                   CeedOperator to find active bases for
+  @param[out] active_input_basis  Basis for active input vector or NULL for composite operator
+  @param[out] active_output_basis Basis for active output vector or NULL for composite operator
+
+  @return An error code: 0 - success, otherwise - failure
+
+  @ref Developer
+**/
+int CeedOperatorGetActiveBases(CeedOperator op, CeedBasis *active_input_basis, CeedBasis *active_output_basis) {
+  Ceed ceed;
+
+  CeedCall(CeedOperatorGetCeed(op, &ceed));
+
+  *active_input_basis = *active_output_basis = NULL;
+  if (op->is_composite) return CEED_ERROR_SUCCESS;
+  for (CeedInt i = 0; i < op->qf->num_input_fields; i++) {
+    if (op->input_fields[i]->vec == CEED_VECTOR_ACTIVE) {
+      CeedCheck(!*active_input_basis || *active_input_basis == op->input_fields[i]->basis, ceed, CEED_ERROR_MINOR,
+                "Multiple active input CeedBases found");
+      *active_input_basis = op->input_fields[i]->basis;
+    }
+  }
+  for (CeedInt i = 0; i < op->qf->num_output_fields; i++) {
+    if (op->output_fields[i]->vec == CEED_VECTOR_ACTIVE) {
+      CeedCheck(!*active_output_basis || *active_output_basis == op->output_fields[i]->basis, ceed, CEED_ERROR_MINOR,
+                "Multiple active output CeedBases found");
+      *active_output_basis = op->output_fields[i]->basis;
+    }
+  }
+
+  CeedCheck(*active_input_basis, ceed, CEED_ERROR_INCOMPLETE, "No active input CeedBasis found");
+  CeedCheck(*active_output_basis, ceed, CEED_ERROR_INCOMPLETE, "No active output CeedBasis found");
+  return CEED_ERROR_SUCCESS;
+}
+
 /**
   @brief Find the active vector ElemRestriction for a non-composite CeedOperator
 
-  @param[in] op           CeedOperator to find active basis for
+  @param[in] op           CeedOperator to find active ElemRestriction for
   @param[out] active_rstr ElemRestriction for active input vector or NULL for composite operator
 
   @return An error code: 0 - success, otherwise - failure
@@ -197,6 +235,44 @@ int CeedOperatorGetActiveElemRestriction(CeedOperator op, CeedElemRestriction *a
   return CEED_ERROR_SUCCESS;
 }
 
+/**
+  @brief Find the active input and output vector ElemRestrictions for a non-composite CeedOperator
+
+  @param[in] op                  CeedOperator to find active ElemRestrictions for
+  @param[out] active_input_rstr  ElemRestriction for active input vector or NULL for composite operator
+  @param[out] active_output_rstr ElemRestriction for active output vector or NULL for composite operator
+
+  @return An error code: 0 - success, otherwise - failure
+
+  @ref Utility
+**/
+int CeedOperatorGetActiveElemRestrictions(CeedOperator op, CeedElemRestriction *active_input_rstr, CeedElemRestriction *active_output_rstr) {
+  Ceed ceed;
+
+  CeedCall(CeedOperatorGetCeed(op, &ceed));
+
+  *active_input_rstr = *active_output_rstr = NULL;
+  if (op->is_composite) return CEED_ERROR_SUCCESS;
+  for (CeedInt i = 0; i < op->qf->num_input_fields; i++) {
+    if (op->input_fields[i]->vec == CEED_VECTOR_ACTIVE) {
+      CeedCheck(!*active_input_rstr || *active_input_rstr == op->input_fields[i]->elem_rstr, ceed, CEED_ERROR_MINOR,
+                "Multiple active input CeedElemRestrictions found");
+      *active_input_rstr = op->input_fields[i]->elem_rstr;
+    }
+  }
+  for (CeedInt i = 0; i < op->qf->num_output_fields; i++) {
+    if (op->output_fields[i]->vec == CEED_VECTOR_ACTIVE) {
+      CeedCheck(!*active_output_rstr || *active_output_rstr == op->output_fields[i]->elem_rstr, ceed, CEED_ERROR_MINOR,
+                "Multiple active output CeedElemRestrictions found");
+      *active_output_rstr = op->output_fields[i]->elem_rstr;
+    }
+  }
+
+  CeedCheck(*active_input_rstr, ceed, CEED_ERROR_INCOMPLETE, "No active input CeedElemRestriction found");
+  CeedCheck(*active_output_rstr, ceed, CEED_ERROR_INCOMPLETE, "No active output CeedElemRestriction found");
+  return CEED_ERROR_SUCCESS;
+}
+
 /**
   @brief Set QFunctionContext field values of the specified type.
 
@@ -610,12 +686,12 @@ int CeedOperatorReferenceCopy(CeedOperator op, CeedOperator *op_copy) {
   There can be at most one active input CeedVector and at most one active output CeedVector passed to CeedOperatorApply().
 
   The number of quadrature points must agree across all points.
-  When using @ref CEED_BASIS_NONE, the number of quadrature points is determined by the element size of r.
+  When using @ref CEED_BASIS_NONE, the number of quadrature points is determined by the element size of rstr.
 
   @param[in,out] op         CeedOperator on which to provide the field
   @param[in]     field_name Name of the field (to be matched with the name used by CeedQFunction)
-  @param[in]     r          CeedElemRestriction
-  @param[in]     b          CeedBasis in which the field resides or @ref CEED_BASIS_NONE if collocated with quadrature points
+  @param[in]     rstr       CeedElemRestriction
+  @param[in]     basis      CeedBasis in which the field resides or @ref CEED_BASIS_NONE if collocated with quadrature points
   @param[in]     v          CeedVector to be used by CeedOperator or @ref CEED_VECTOR_ACTIVE if field is active or @ref CEED_VECTOR_NONE
                               if using @ref CEED_EVAL_WEIGHT in the QFunction
 
@@ -623,7 +699,7 @@ int CeedOperatorReferenceCopy(CeedOperator op, CeedOperator *op_copy) {
 
   @ref User
 **/
-int CeedOperatorSetField(CeedOperator op, const char *field_name, CeedElemRestriction r, CeedBasis b, CeedVector v) {
+int CeedOperatorSetField(CeedOperator op, const char *field_name, CeedElemRestriction rstr, CeedBasis basis, CeedVector v) {
   bool               is_input = true;
   CeedInt            num_elem = 0, num_qpts = 0;
   CeedQFunctionField qf_field;
@@ -631,20 +707,20 @@ int CeedOperatorSetField(CeedOperator op, const char *field_name, CeedElemRestri
 
   CeedCheck(!op->is_composite, op->ceed, CEED_ERROR_INCOMPATIBLE, "Cannot add field to composite operator.");
   CeedCheck(!op->is_immutable, op->ceed, CEED_ERROR_MAJOR, "Operator cannot be changed after set as immutable");
-  CeedCheck(r, op->ceed, CEED_ERROR_INCOMPATIBLE, "ElemRestriction r for field \"%s\" must be non-NULL.", field_name);
-  CeedCheck(b, op->ceed, CEED_ERROR_INCOMPATIBLE, "Basis b for field \"%s\" must be non-NULL.", field_name);
-  CeedCheck(v, op->ceed, CEED_ERROR_INCOMPATIBLE, "Vector v for field \"%s\" must be non-NULL.", field_name);
+  CeedCheck(rstr, op->ceed, CEED_ERROR_INCOMPATIBLE, "ElemRestriction for field \"%s\" must be non-NULL.", field_name);
+  CeedCheck(basis, op->ceed, CEED_ERROR_INCOMPATIBLE, "Basis for field \"%s\" must be non-NULL.", field_name);
+  CeedCheck(v, op->ceed, CEED_ERROR_INCOMPATIBLE, "Vector for field \"%s\" must be non-NULL.", field_name);
 
-  CeedCall(CeedElemRestrictionGetNumElements(r, &num_elem));
-  CeedCheck(r == CEED_ELEMRESTRICTION_NONE || !op->has_restriction || op->num_elem == num_elem, op->ceed, CEED_ERROR_DIMENSION,
+  CeedCall(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCheck(rstr == CEED_ELEMRESTRICTION_NONE || !op->has_restriction || op->num_elem == num_elem, op->ceed, CEED_ERROR_DIMENSION,
             "ElemRestriction with %" CeedInt_FMT " elements incompatible with prior %" CeedInt_FMT " elements", num_elem, op->num_elem);
 
-  if (b == CEED_BASIS_NONE) CeedCall(CeedElemRestrictionGetElementSize(r, &num_qpts));
-  else CeedCall(CeedBasisGetNumQuadraturePoints(b, &num_qpts));
+  if (basis == CEED_BASIS_NONE) CeedCall(CeedElemRestrictionGetElementSize(rstr, &num_qpts));
+  else CeedCall(CeedBasisGetNumQuadraturePoints(basis, &num_qpts));
   CeedCheck(op->num_qpts == 0 || op->num_qpts == num_qpts, op->ceed, CEED_ERROR_DIMENSION,
             "%s must correspond to the same number of quadrature points as previously added Bases. Found %" CeedInt_FMT
             " quadrature points but expected %" CeedInt_FMT " quadrature points.",
-            b == CEED_BASIS_NONE ? "ElemRestriction" : "Basis", num_qpts, op->num_qpts);
+            basis == CEED_BASIS_NONE ? "ElemRestriction" : "Basis", num_qpts, op->num_qpts);
   for (CeedInt i = 0; i < op->qf->num_input_fields; i++) {
     if (!strcmp(field_name, (*op->qf->input_fields[i]).field_name)) {
       qf_field = op->qf->input_fields[i];
@@ -664,13 +740,13 @@ int CeedOperatorSetField(CeedOperator op, const char *field_name, CeedElemRestri
   return CeedError(op->ceed, CEED_ERROR_INCOMPLETE, "QFunction has no knowledge of field '%s'", field_name);
   // LCOV_EXCL_STOP
 found:
-  CeedCall(CeedOperatorCheckField(op->ceed, qf_field, r, b));
+  CeedCall(CeedOperatorCheckField(op->ceed, qf_field, rstr, basis));
   CeedCall(CeedCalloc(1, op_field));
 
   if (v == CEED_VECTOR_ACTIVE) {
     CeedSize l_size;
 
-    CeedCall(CeedElemRestrictionGetLVectorSize(r, &l_size));
+    CeedCall(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
     if (is_input) {
       if (op->input_size == -1) op->input_size = l_size;
       CeedCheck(l_size == op->input_size, op->ceed, CEED_ERROR_INCOMPATIBLE, "LVector size %td does not match previous size %td", l_size,
@@ -683,12 +759,12 @@ found:
   }
 
   CeedCall(CeedVectorReferenceCopy(v, &(*op_field)->vec));
-  CeedCall(CeedElemRestrictionReferenceCopy(r, &(*op_field)->elem_rstr));
-  if (r != CEED_ELEMRESTRICTION_NONE) {
+  CeedCall(CeedElemRestrictionReferenceCopy(rstr, &(*op_field)->elem_rstr));
+  if (rstr != CEED_ELEMRESTRICTION_NONE) {
     op->num_elem        = num_elem;
     op->has_restriction = true;  // Restriction set, but num_elem may be 0
   }
-  CeedCall(CeedBasisReferenceCopy(b, &(*op_field)->basis));
+  CeedCall(CeedBasisReferenceCopy(basis, &(*op_field)->basis));
   if (op->num_qpts == 0) CeedCall(CeedOperatorSetNumQuadraturePoints(op, num_qpts));
 
   op->num_fields += 1;
diff --git a/interface/ceed-preconditioning.c b/interface/ceed-preconditioning.c
index 4d700ad1..6d96501e 100644
--- a/interface/ceed-preconditioning.c
+++ b/interface/ceed-preconditioning.c
@@ -206,138 +206,70 @@ int CeedOperatorGetFallbackParentCeed(CeedOperator op, Ceed *parent) {
 }
 
 /**
-  @brief Select correct basis matrix pointer based on CeedEvalMode
-
-  @param[in]  basis     CeedBasis from which to get the basis matrix
-  @param[in]  eval_mode Current basis evaluation mode
-  @param[in]  identity  Pointer to identity matrix
-  @param[out] basis_ptr Basis pointer to set
-
-  @ref Developer
-**/
-static inline int CeedOperatorGetBasisPointer(CeedBasis basis, CeedEvalMode eval_mode, const CeedScalar *identity, const CeedScalar **basis_ptr) {
-  switch (eval_mode) {
-    case CEED_EVAL_NONE:
-      *basis_ptr = identity;
-      break;
-    case CEED_EVAL_INTERP:
-      CeedCall(CeedBasisGetInterp(basis, basis_ptr));
-      break;
-    case CEED_EVAL_GRAD:
-      CeedCall(CeedBasisGetGrad(basis, basis_ptr));
-      break;
-    case CEED_EVAL_DIV:
-      CeedCall(CeedBasisGetDiv(basis, basis_ptr));
-      break;
-    case CEED_EVAL_CURL:
-      CeedCall(CeedBasisGetCurl(basis, basis_ptr));
-      break;
-    case CEED_EVAL_WEIGHT:
-      break;  // Caught by QF Assembly
-  }
-  assert(*basis_ptr != NULL);
-  return CEED_ERROR_SUCCESS;
-}
-
-/**
-  @brief Create point block restriction for active operator field
+  @brief Core logic for assembling operator diagonal or point block diagonal
 
-  @param[in]  rstr            Original CeedElemRestriction for active field
-  @param[out] pointblock_rstr Address of the variable where the newly created CeedElemRestriction will be stored
+  @param[in]  op             CeedOperator to assemble point block diagonal
+  @param[in]  request        Address of CeedRequest for non-blocking completion, else CEED_REQUEST_IMMEDIATE
+  @param[in]  is_point_block Boolean flag to assemble diagonal or point block diagonal
+  @param[out] assembled      CeedVector to store assembled diagonal
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref Developer
 **/
-static int CeedOperatorCreateActivePointBlockRestriction(CeedElemRestriction rstr, CeedElemRestriction *pointblock_rstr) {
-  Ceed           ceed;
-  CeedInt        num_elem, num_comp, shift, elem_size, comp_stride, *pointblock_offsets;
-  CeedSize       l_size;
-  const CeedInt *offsets;
-
-  CeedCall(CeedElemRestrictionGetCeed(rstr, &ceed));
-  CeedCall(CeedElemRestrictionGetOffsets(rstr, CEED_MEM_HOST, &offsets));
-
-  // Expand offsets
-  CeedCall(CeedElemRestrictionGetNumElements(rstr, &num_elem));
-  CeedCall(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
-  CeedCall(CeedElemRestrictionGetElementSize(rstr, &elem_size));
-  CeedCall(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
-  CeedCall(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
-  shift = num_comp;
-  if (comp_stride != 1) shift *= num_comp;
-  CeedCall(CeedCalloc(num_elem * elem_size, &pointblock_offsets));
-  for (CeedInt i = 0; i < num_elem * elem_size; i++) {
-    pointblock_offsets[i] = offsets[i] * shift;
-  }
-
-  // Create new restriction
-  CeedCall(CeedElemRestrictionCreate(ceed, num_elem, elem_size, num_comp * num_comp, 1, l_size * num_comp, CEED_MEM_HOST, CEED_OWN_POINTER,
-                                     pointblock_offsets, pointblock_rstr));
-
-  // Cleanup
-  CeedCall(CeedElemRestrictionRestoreOffsets(rstr, &offsets));
-  return CEED_ERROR_SUCCESS;
-}
-
-/**
-  @brief Core logic for assembling operator diagonal or point block diagonal
-
-  @param[in]  op            CeedOperator to assemble point block diagonal
-  @param[in]  request       Address of CeedRequest for non-blocking completion, else CEED_REQUEST_IMMEDIATE
-  @param[in]  is_pointblock Boolean flag to assemble diagonal or point block diagonal
-  @param[out] assembled     CeedVector to store assembled diagonal
+static inline int CeedSingleOperatorAssembleAddDiagonal_Core(CeedOperator op, CeedRequest *request, const bool is_point_block, CeedVector assembled) {
+  Ceed ceed;
+  bool is_composite;
 
-  @return An error code: 0 - success, otherwise - failure
+  CeedCall(CeedOperatorGetCeed(op, &ceed));
+  CeedCall(CeedOperatorIsComposite(op, &is_composite));
+  CeedCheck(!is_composite, ceed, CEED_ERROR_UNSUPPORTED, "Composite operator not supported");
 
-  @ref Developer
-**/
-static inline int CeedSingleOperatorAssembleAddDiagonal_Core(CeedOperator op, CeedRequest *request, const bool is_pointblock, CeedVector assembled) {
-  Ceed                ceed;
-  CeedInt             num_input_fields, num_output_fields;
-  CeedInt             layout[3];
+  // Assemble QFunction
+  CeedInt             layout_qf[3];
   const CeedScalar   *assembled_qf_array;
   CeedVector          assembled_qf        = NULL;
   CeedElemRestriction assembled_elem_rstr = NULL;
-  CeedQFunction       qf;
-
-  CeedCall(CeedOperatorGetCeed(op, &ceed));
 
-  // Assemble QFunction
-  CeedCall(CeedOperatorGetQFunction(op, &qf));
-  CeedCall(CeedQFunctionGetNumArgs(qf, &num_input_fields, &num_output_fields));
   CeedCall(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &assembled_elem_rstr, request));
-  CeedCall(CeedElemRestrictionGetELayout(assembled_elem_rstr, &layout));
+  CeedCall(CeedElemRestrictionGetELayout(assembled_elem_rstr, &layout_qf));
   CeedCall(CeedElemRestrictionDestroy(&assembled_elem_rstr));
   CeedCall(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_HOST, &assembled_qf_array));
 
   // Get assembly data
   const CeedEvalMode     **eval_modes_in, **eval_modes_out;
-  CeedInt                 *num_eval_modes_in, *num_eval_modes_out, num_active_bases;
+  CeedInt                  num_active_bases_in, *num_eval_modes_in, num_active_bases_out, *num_eval_modes_out;
   CeedSize               **eval_mode_offsets_in, **eval_mode_offsets_out, num_output_components;
-  CeedElemRestriction     *active_elem_rstrs;
-  CeedBasis               *active_bases;
+  CeedBasis               *active_bases_in, *active_bases_out;
+  CeedElemRestriction     *active_elem_rstrs_in, *active_elem_rstrs_out;
   CeedOperatorAssemblyData data;
 
   CeedCall(CeedOperatorGetOperatorAssemblyData(op, &data));
-  CeedCall(CeedOperatorAssemblyDataGetEvalModes(data, &num_active_bases, &num_eval_modes_in, &eval_modes_in, &eval_mode_offsets_in,
-                                                &num_eval_modes_out, &eval_modes_out, &eval_mode_offsets_out, &num_output_components));
-  CeedCall(CeedOperatorAssemblyDataGetBases(data, NULL, &active_bases, NULL, NULL));
-  CeedCall(CeedOperatorAssemblyDataGetElemRestrictions(data, NULL, &active_elem_rstrs));
+  CeedCall(CeedOperatorAssemblyDataGetEvalModes(data, &num_active_bases_in, &num_eval_modes_in, &eval_modes_in, &eval_mode_offsets_in,
+                                                &num_active_bases_out, &num_eval_modes_out, &eval_modes_out, &eval_mode_offsets_out,
+                                                &num_output_components));
+  CeedCall(CeedOperatorAssemblyDataGetBases(data, NULL, &active_bases_in, NULL, NULL, &active_bases_out, NULL));
+  CeedCall(CeedOperatorAssemblyDataGetElemRestrictions(data, NULL, &active_elem_rstrs_in, NULL, &active_elem_rstrs_out));
+
+  CeedCheck(num_active_bases_in == num_active_bases_out, ceed, CEED_ERROR_UNSUPPORTED,
+            "Cannot assemble operator diagonal with different numbers of input and output active bases");
 
   // Loop over all active bases
-  for (CeedInt b = 0; b < num_active_bases; b++) {
+  for (CeedInt b = 0; b < num_active_bases_in; b++) {
     bool                has_eval_none = false;
-    CeedInt             num_elem, num_nodes, num_qpts, num_components;
+    CeedInt             num_elem, num_nodes, num_qpts, num_comp;
     CeedScalar         *elem_diag_array, *identity = NULL;
     CeedVector          elem_diag;
     CeedElemRestriction diag_elem_rstr;
 
+    CeedCheck(active_elem_rstrs_in[b] == active_elem_rstrs_out[b], ceed, CEED_ERROR_UNSUPPORTED,
+              "Cannot assemble operator diagonal with different input and output active element restrictions");
+
     // Assemble point block diagonal restriction, if needed
-    if (is_pointblock) {
-      CeedCall(CeedOperatorCreateActivePointBlockRestriction(active_elem_rstrs[b], &diag_elem_rstr));
+    if (is_point_block) {
+      CeedCall(CeedOperatorCreateActivePointBlockRestriction(active_elem_rstrs_in[b], &diag_elem_rstr));
     } else {
-      CeedCall(CeedElemRestrictionCreateUnsignedCopy(active_elem_rstrs[b], &diag_elem_rstr));
+      CeedCall(CeedElemRestrictionCreateUnsignedCopy(active_elem_rstrs_in[b], &diag_elem_rstr));
     }
 
     // Create diagonal vector
@@ -347,9 +279,23 @@ static inline int CeedSingleOperatorAssembleAddDiagonal_Core(CeedOperator op, Ce
     CeedCall(CeedVectorSetValue(elem_diag, 0.0));
     CeedCall(CeedVectorGetArray(elem_diag, CEED_MEM_HOST, &elem_diag_array));
     CeedCall(CeedElemRestrictionGetNumElements(diag_elem_rstr, &num_elem));
-    CeedCall(CeedBasisGetNumNodes(active_bases[b], &num_nodes));
-    CeedCall(CeedBasisGetNumComponents(active_bases[b], &num_components));
-    CeedCall(CeedBasisGetNumQuadraturePoints(active_bases[b], &num_qpts));
+    CeedCall(CeedBasisGetNumNodes(active_bases_in[b], &num_nodes));
+    CeedCall(CeedBasisGetNumComponents(active_bases_in[b], &num_comp));
+    if (active_bases_in[b] == CEED_BASIS_NONE) num_qpts = num_nodes;
+    else CeedCall(CeedBasisGetNumQuadraturePoints(active_bases_in[b], &num_qpts));
+
+    if (active_bases_in[b] != active_bases_out[b]) {
+      CeedInt num_nodes_out, num_qpts_out, num_comp_out;
+
+      CeedCall(CeedBasisGetNumNodes(active_bases_out[b], &num_nodes_out));
+      CeedCheck(num_nodes == num_nodes_out, ceed, CEED_ERROR_UNSUPPORTED, "Active input and output bases must have the same number of nodes");
+      CeedCall(CeedBasisGetNumComponents(active_bases_out[b], &num_comp_out));
+      CeedCheck(num_comp == num_comp_out, ceed, CEED_ERROR_UNSUPPORTED, "Active input and output bases must have the same number of components");
+      if (active_bases_out[b] == CEED_BASIS_NONE) num_qpts_out = num_nodes_out;
+      else CeedCall(CeedBasisGetNumQuadraturePoints(active_bases_out[b], &num_qpts_out));
+      CeedCheck(num_qpts == num_qpts_out, ceed, CEED_ERROR_UNSUPPORTED,
+                "Active input and output bases must have the same number of quadrature points");
+    }
 
     // Construct identity matrix for basis if required
     for (CeedInt i = 0; i < num_eval_modes_in[b]; i++) {
@@ -375,8 +321,8 @@ static inline int CeedSingleOperatorAssembleAddDiagonal_Core(CeedOperator op, Ce
         const CeedScalar *B_t               = NULL;
         CeedEvalMode      eval_mode_in_prev = CEED_EVAL_NONE;
 
-        CeedOperatorGetBasisPointer(active_bases[b], eval_modes_out[b][e_out], identity, &B_t);
-        CeedCall(CeedBasisGetNumQuadratureComponents(active_bases[b], eval_modes_out[b][e_out], &q_comp_out));
+        CeedCall(CeedOperatorGetBasisPointer(active_bases_out[b], eval_modes_out[b][e_out], identity, &B_t));
+        CeedCall(CeedBasisGetNumQuadratureComponents(active_bases_out[b], eval_modes_out[b][e_out], &q_comp_out));
         if (q_comp_out > 1) {
           if (e_out == 0 || eval_modes_out[b][e_out] != eval_mode_out_prev) d_out = 0;
           else B_t = &B_t[(++d_out) * num_qpts * num_nodes];
@@ -386,8 +332,8 @@ static inline int CeedSingleOperatorAssembleAddDiagonal_Core(CeedOperator op, Ce
         for (CeedInt e_in = 0; e_in < num_eval_modes_in[b]; e_in++) {
           const CeedScalar *B = NULL;
 
-          CeedOperatorGetBasisPointer(active_bases[b], eval_modes_in[b][e_in], identity, &B);
-          CeedCall(CeedBasisGetNumQuadratureComponents(active_bases[b], eval_modes_in[b][e_in], &q_comp_in));
+          CeedCall(CeedOperatorGetBasisPointer(active_bases_in[b], eval_modes_in[b][e_in], identity, &B));
+          CeedCall(CeedBasisGetNumQuadratureComponents(active_bases_in[b], eval_modes_in[b][e_in], &q_comp_in));
           if (q_comp_in > 1) {
             if (e_in == 0 || eval_modes_in[b][e_in] != eval_mode_in_prev) d_in = 0;
             else B = &B[(++d_in) * num_qpts * num_nodes];
@@ -395,27 +341,27 @@ static inline int CeedSingleOperatorAssembleAddDiagonal_Core(CeedOperator op, Ce
           eval_mode_in_prev = eval_modes_in[b][e_in];
 
           // Each component
-          for (CeedInt c_out = 0; c_out < num_components; c_out++) {
+          for (CeedInt c_out = 0; c_out < num_comp; c_out++) {
             // Each qpt/node pair
             for (CeedInt q = 0; q < num_qpts; q++) {
-              if (is_pointblock) {
+              if (is_point_block) {
                 // Point Block Diagonal
-                for (CeedInt c_in = 0; c_in < num_components; c_in++) {
+                for (CeedInt c_in = 0; c_in < num_comp; c_in++) {
                   const CeedSize c_offset = (eval_mode_offsets_in[b][e_in] + c_in) * num_output_components + eval_mode_offsets_out[b][e_out] + c_out;
-                  const CeedScalar qf_value = assembled_qf_array[q * layout[0] + c_offset * layout[1] + e * layout[2]];
+                  const CeedScalar qf_value = assembled_qf_array[q * layout_qf[0] + c_offset * layout_qf[1] + e * layout_qf[2]];
 
                   for (CeedInt n = 0; n < num_nodes; n++) {
-                    elem_diag_array[((e * num_components + c_out) * num_components + c_in) * num_nodes + n] +=
+                    elem_diag_array[((e * num_comp + c_out) * num_comp + c_in) * num_nodes + n] +=
                         B_t[q * num_nodes + n] * qf_value * B[q * num_nodes + n];
                   }
                 }
               } else {
                 // Diagonal Only
                 const CeedInt    c_offset = (eval_mode_offsets_in[b][e_in] + c_out) * num_output_components + eval_mode_offsets_out[b][e_out] + c_out;
-                const CeedScalar qf_value = assembled_qf_array[q * layout[0] + c_offset * layout[1] + e * layout[2]];
+                const CeedScalar qf_value = assembled_qf_array[q * layout_qf[0] + c_offset * layout_qf[1] + e * layout_qf[2]];
 
                 for (CeedInt n = 0; n < num_nodes; n++) {
-                  elem_diag_array[(e * num_components + c_out) * num_nodes + n] += B_t[q * num_nodes + n] * qf_value * B[q * num_nodes + n];
+                  elem_diag_array[(e * num_comp + c_out) * num_nodes + n] += B_t[q * num_nodes + n] * qf_value * B[q * num_nodes + n];
                 }
               }
             }
@@ -441,16 +387,16 @@ static inline int CeedSingleOperatorAssembleAddDiagonal_Core(CeedOperator op, Ce
 /**
   @brief Core logic for assembling composite operator diagonal
 
-  @param[in]  op            CeedOperator to assemble point block diagonal
-  @param[in]  request       Address of CeedRequest for non-blocking completion, else CEED_REQUEST_IMMEDIATE
-  @param[in]  is_pointblock Boolean flag to assemble diagonal or point block diagonal
-  @param[out] assembled     CeedVector to store assembled diagonal
+  @param[in]  op             CeedOperator to assemble point block diagonal
+  @param[in]  request        Address of CeedRequest for non-blocking completion, else CEED_REQUEST_IMMEDIATE
+  @param[in]  is_point_block Boolean flag to assemble diagonal or point block diagonal
+  @param[out] assembled      CeedVector to store assembled diagonal
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref Developer
 **/
-static inline int CeedCompositeOperatorLinearAssembleAddDiagonal(CeedOperator op, CeedRequest *request, const bool is_pointblock,
+static inline int CeedCompositeOperatorLinearAssembleAddDiagonal(CeedOperator op, CeedRequest *request, const bool is_point_block,
                                                                  CeedVector assembled) {
   CeedInt       num_sub;
   CeedOperator *suboperators;
@@ -458,7 +404,7 @@ static inline int CeedCompositeOperatorLinearAssembleAddDiagonal(CeedOperator op
   CeedCall(CeedCompositeOperatorGetNumSub(op, &num_sub));
   CeedCall(CeedCompositeOperatorGetSubList(op, &suboperators));
   for (CeedInt i = 0; i < num_sub; i++) {
-    if (is_pointblock) {
+    if (is_point_block) {
       CeedCall(CeedOperatorLinearAssembleAddPointBlockDiagonal(suboperators[i], assembled, request));
     } else {
       CeedCall(CeedOperatorLinearAssembleAddDiagonal(suboperators[i], assembled, request));
@@ -484,47 +430,79 @@ static inline int CeedCompositeOperatorLinearAssembleAddDiagonal(CeedOperator op
 static int CeedSingleOperatorAssembleSymbolic(CeedOperator op, CeedInt offset, CeedInt *rows, CeedInt *cols) {
   Ceed                ceed;
   bool                is_composite;
-  CeedInt             num_elem, elem_size, num_comp, layout_er[3], local_num_entries;
-  CeedSize            num_nodes, count = 0;
+  CeedSize            num_nodes_in, num_nodes_out, count = 0;
+  CeedInt             num_elem_in, elem_size_in, num_comp_in, layout_er_in[3];
+  CeedInt             num_elem_out, elem_size_out, num_comp_out, layout_er_out[3], local_num_entries;
   CeedScalar         *array;
-  const CeedScalar   *elem_dof_a;
-  CeedVector          index_vec, elem_dof;
-  CeedElemRestriction active_rstr, index_elem_rstr;
+  const CeedScalar   *elem_dof_a_in, *elem_dof_a_out;
+  CeedVector          index_vec_in, index_vec_out, elem_dof_in, elem_dof_out;
+  CeedElemRestriction elem_rstr_in, elem_rstr_out, index_elem_rstr_in, index_elem_rstr_out;
 
   CeedCall(CeedOperatorGetCeed(op, &ceed));
   CeedCall(CeedOperatorIsComposite(op, &is_composite));
   CeedCheck(!is_composite, ceed, CEED_ERROR_UNSUPPORTED, "Composite operator not supported");
 
-  CeedCall(CeedOperatorGetActiveVectorLengths(op, &num_nodes, NULL));
-  CeedCall(CeedOperatorGetActiveElemRestriction(op, &active_rstr));
-  CeedCall(CeedElemRestrictionCreateUnorientedCopy(active_rstr, &index_elem_rstr));
-  CeedCall(CeedElemRestrictionGetNumElements(index_elem_rstr, &num_elem));
-  CeedCall(CeedElemRestrictionGetElementSize(index_elem_rstr, &elem_size));
-  CeedCall(CeedElemRestrictionGetNumComponents(index_elem_rstr, &num_comp));
-  CeedCall(CeedElemRestrictionGetELayout(index_elem_rstr, &layout_er));
-  local_num_entries = elem_size * num_comp * elem_size * num_comp * num_elem;
-
-  // Determine elem_dof relation
-  CeedCall(CeedVectorCreate(ceed, num_nodes, &index_vec));
-  CeedCall(CeedVectorGetArrayWrite(index_vec, CEED_MEM_HOST, &array));
-  for (CeedInt i = 0; i < num_nodes; i++) array[i] = i;
-  CeedCall(CeedVectorRestoreArray(index_vec, &array));
-  CeedCall(CeedVectorCreate(ceed, num_elem * elem_size * num_comp, &elem_dof));
-  CeedCall(CeedVectorSetValue(elem_dof, 0.0));
-  CeedCall(CeedElemRestrictionApply(index_elem_rstr, CEED_NOTRANSPOSE, index_vec, elem_dof, CEED_REQUEST_IMMEDIATE));
-  CeedCall(CeedVectorGetArrayRead(elem_dof, CEED_MEM_HOST, &elem_dof_a));
-  CeedCall(CeedVectorDestroy(&index_vec));
+  CeedCall(CeedOperatorGetActiveVectorLengths(op, &num_nodes_in, &num_nodes_out));
+  CeedCall(CeedOperatorGetActiveElemRestrictions(op, &elem_rstr_in, &elem_rstr_out));
+  CeedCall(CeedElemRestrictionGetNumElements(elem_rstr_in, &num_elem_in));
+  CeedCall(CeedElemRestrictionGetElementSize(elem_rstr_in, &elem_size_in));
+  CeedCall(CeedElemRestrictionGetNumComponents(elem_rstr_in, &num_comp_in));
+  CeedCall(CeedElemRestrictionGetELayout(elem_rstr_in, &layout_er_in));
+
+  // Determine elem_dof relation for input
+  CeedCall(CeedVectorCreate(ceed, num_nodes_in, &index_vec_in));
+  CeedCall(CeedVectorGetArrayWrite(index_vec_in, CEED_MEM_HOST, &array));
+  for (CeedInt i = 0; i < num_nodes_in; i++) array[i] = i;
+  CeedCall(CeedVectorRestoreArray(index_vec_in, &array));
+  CeedCall(CeedVectorCreate(ceed, num_elem_in * elem_size_in * num_comp_in, &elem_dof_in));
+  CeedCall(CeedVectorSetValue(elem_dof_in, 0.0));
+  CeedCall(CeedElemRestrictionCreateUnorientedCopy(elem_rstr_in, &index_elem_rstr_in));
+  CeedCall(CeedElemRestrictionApply(index_elem_rstr_in, CEED_NOTRANSPOSE, index_vec_in, elem_dof_in, CEED_REQUEST_IMMEDIATE));
+  CeedCall(CeedVectorGetArrayRead(elem_dof_in, CEED_MEM_HOST, &elem_dof_a_in));
+  CeedCall(CeedVectorDestroy(&index_vec_in));
+  CeedCall(CeedElemRestrictionDestroy(&index_elem_rstr_in));
+
+  if (elem_rstr_in != elem_rstr_out) {
+    CeedCall(CeedElemRestrictionGetNumElements(elem_rstr_out, &num_elem_out));
+    CeedCheck(num_elem_in == num_elem_out, ceed, CEED_ERROR_UNSUPPORTED,
+              "Active input and output operator restrictions must have the same number of elements");
+    CeedCall(CeedElemRestrictionGetElementSize(elem_rstr_out, &elem_size_out));
+    CeedCall(CeedElemRestrictionGetNumComponents(elem_rstr_out, &num_comp_out));
+    CeedCall(CeedElemRestrictionGetELayout(elem_rstr_out, &layout_er_out));
+
+    // Determine elem_dof relation for output
+    CeedCall(CeedVectorCreate(ceed, num_nodes_out, &index_vec_out));
+    CeedCall(CeedVectorGetArrayWrite(index_vec_out, CEED_MEM_HOST, &array));
+    for (CeedInt i = 0; i < num_nodes_out; i++) array[i] = i;
+    CeedCall(CeedVectorRestoreArray(index_vec_out, &array));
+    CeedCall(CeedVectorCreate(ceed, num_elem_out * elem_size_out * num_comp_out, &elem_dof_out));
+    CeedCall(CeedVectorSetValue(elem_dof_out, 0.0));
+    CeedCall(CeedElemRestrictionCreateUnorientedCopy(elem_rstr_out, &index_elem_rstr_out));
+    CeedCall(CeedElemRestrictionApply(index_elem_rstr_out, CEED_NOTRANSPOSE, index_vec_out, elem_dof_out, CEED_REQUEST_IMMEDIATE));
+    CeedCall(CeedVectorGetArrayRead(elem_dof_out, CEED_MEM_HOST, &elem_dof_a_out));
+    CeedCall(CeedVectorDestroy(&index_vec_out));
+    CeedCall(CeedElemRestrictionDestroy(&index_elem_rstr_out));
+  } else {
+    num_elem_out     = num_elem_in;
+    elem_size_out    = elem_size_in;
+    num_comp_out     = num_comp_in;
+    layout_er_out[0] = layout_er_in[0];
+    layout_er_out[1] = layout_er_in[1];
+    layout_er_out[2] = layout_er_in[2];
+    elem_dof_a_out   = elem_dof_a_in;
+  }
+  local_num_entries = elem_size_out * num_comp_out * elem_size_in * num_comp_in * num_elem_in;
 
   // Determine i, j locations for element matrices
-  for (CeedInt e = 0; e < num_elem; e++) {
-    for (CeedInt comp_in = 0; comp_in < num_comp; comp_in++) {
-      for (CeedInt comp_out = 0; comp_out < num_comp; comp_out++) {
-        for (CeedInt i = 0; i < elem_size; i++) {
-          for (CeedInt j = 0; j < elem_size; j++) {
-            const CeedInt elem_dof_index_row = i * layout_er[0] + (comp_out)*layout_er[1] + e * layout_er[2];
-            const CeedInt elem_dof_index_col = j * layout_er[0] + comp_in * layout_er[1] + e * layout_er[2];
-            const CeedInt row                = elem_dof_a[elem_dof_index_row];
-            const CeedInt col                = elem_dof_a[elem_dof_index_col];
+  for (CeedInt e = 0; e < num_elem_in; e++) {
+    for (CeedInt comp_in = 0; comp_in < num_comp_in; comp_in++) {
+      for (CeedInt comp_out = 0; comp_out < num_comp_out; comp_out++) {
+        for (CeedInt i = 0; i < elem_size_out; i++) {
+          for (CeedInt j = 0; j < elem_size_in; j++) {
+            const CeedInt elem_dof_index_row = i * layout_er_out[0] + comp_out * layout_er_out[1] + e * layout_er_out[2];
+            const CeedInt elem_dof_index_col = j * layout_er_in[0] + comp_in * layout_er_in[1] + e * layout_er_in[2];
+            const CeedInt row                = elem_dof_a_out[elem_dof_index_row];
+            const CeedInt col                = elem_dof_a_in[elem_dof_index_col];
 
             rows[offset + count] = row;
             cols[offset + count] = col;
@@ -535,9 +513,12 @@ static int CeedSingleOperatorAssembleSymbolic(CeedOperator op, CeedInt offset, C
     }
   }
   CeedCheck(count == local_num_entries, ceed, CEED_ERROR_MAJOR, "Error computing assembled entries");
-  CeedCall(CeedVectorRestoreArrayRead(elem_dof, &elem_dof_a));
-  CeedCall(CeedVectorDestroy(&elem_dof));
-  CeedCall(CeedElemRestrictionDestroy(&index_elem_rstr));
+  CeedCall(CeedVectorRestoreArrayRead(elem_dof_in, &elem_dof_a_in));
+  CeedCall(CeedVectorDestroy(&elem_dof_in));
+  if (elem_rstr_in != elem_rstr_out) {
+    CeedCall(CeedVectorRestoreArrayRead(elem_dof_out, &elem_dof_a_out));
+    CeedCall(CeedVectorDestroy(&elem_dof_out));
+  }
   return CEED_ERROR_SUCCESS;
 }
 
@@ -560,7 +541,6 @@ static int CeedSingleOperatorAssemble(CeedOperator op, CeedInt offset, CeedVecto
 
   CeedCall(CeedOperatorGetCeed(op, &ceed));
   CeedCall(CeedOperatorIsComposite(op, &is_composite));
-
   CeedCheck(!is_composite, ceed, CEED_ERROR_UNSUPPORTED, "Composite operator not supported");
 
   // Early exit for empty operator
@@ -587,74 +567,106 @@ static int CeedSingleOperatorAssemble(CeedOperator op, CeedInt offset, CeedVecto
   }
 
   // Assemble QFunction
-  const bool     *orients      = NULL;
-  const CeedInt8 *curl_orients = NULL;
-  CeedInt *num_eval_modes_in, *num_eval_modes_out, num_active_bases, num_input_fields, num_output_fields, num_elem, elem_size, num_qpts, num_comp,
-      local_num_entries, layout_qf[3];
-  const CeedScalar        *assembled_qf_array;
-  CeedVector               assembled_qf = NULL;
-  CeedRestrictionType      rstr_type;
-  CeedElemRestriction      rstr_q = NULL, active_rstr;
-  const CeedEvalMode     **eval_modes_in, **eval_modes_out;
-  CeedBasis               *bases, basis_in;
-  CeedQFunction            qf;
-  CeedOperatorAssemblyData data;
-  CeedOperatorField       *input_fields, *output_fields;
+  CeedInt             layout_qf[3];
+  const CeedScalar   *assembled_qf_array;
+  CeedVector          assembled_qf        = NULL;
+  CeedElemRestriction assembled_elem_rstr = NULL;
 
-  CeedCall(CeedOperatorGetQFunction(op, &qf));
-  CeedCall(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &rstr_q, CEED_REQUEST_IMMEDIATE));
-  CeedCall(CeedOperatorGetFields(op, &num_input_fields, &input_fields, &num_output_fields, &output_fields));
+  CeedCall(CeedOperatorLinearAssembleQFunctionBuildOrUpdate(op, &assembled_qf, &assembled_elem_rstr, CEED_REQUEST_IMMEDIATE));
+  CeedCall(CeedElemRestrictionGetELayout(assembled_elem_rstr, &layout_qf));
+  CeedCall(CeedElemRestrictionDestroy(&assembled_elem_rstr));
+  CeedCall(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_HOST, &assembled_qf_array));
 
   // Get assembly data
-  CeedCall(CeedOperatorGetOperatorAssemblyData(op, &data));
-  CeedCall(CeedOperatorAssemblyDataGetEvalModes(data, &num_active_bases, &num_eval_modes_in, &eval_modes_in, NULL, &num_eval_modes_out,
-                                                &eval_modes_out, NULL, NULL));
-  CeedCall(CeedOperatorAssemblyDataGetBases(data, NULL, &bases, NULL, NULL));
-  basis_in = bases[0];
-
-  CeedCheck(num_active_bases == 1, ceed, CEED_ERROR_UNSUPPORTED, "Cannot assemble operator with multiple active bases");
-  CeedCheck(num_eval_modes_in[0] > 0 && num_eval_modes_out[0] > 0, ceed, CEED_ERROR_UNSUPPORTED, "Cannot assemble operator with out inputs/outputs");
+  CeedInt                  num_elem_in, elem_size_in, num_comp_in, num_qpts_in;
+  CeedInt                  num_elem_out, elem_size_out, num_comp_out, num_qpts_out, local_num_entries;
+  const CeedEvalMode     **eval_modes_in, **eval_modes_out;
+  CeedInt                  num_active_bases_in, *num_eval_modes_in, num_active_bases_out, *num_eval_modes_out;
+  CeedBasis               *active_bases_in, *active_bases_out, basis_in, basis_out;
+  const CeedScalar       **B_mats_in, **B_mats_out, *B_mat_in, *B_mat_out;
+  CeedElemRestriction      elem_rstr_in, elem_rstr_out;
+  CeedRestrictionType      elem_rstr_type_in, elem_rstr_type_out;
+  const bool              *elem_rstr_orients_in = NULL, *elem_rstr_orients_out = NULL;
+  const CeedInt8          *elem_rstr_curl_orients_in = NULL, *elem_rstr_curl_orients_out = NULL;
+  CeedOperatorAssemblyData data;
 
-  CeedCall(CeedOperatorGetActiveElemRestriction(op, &active_rstr));
-  CeedCall(CeedElemRestrictionGetNumElements(active_rstr, &num_elem));
-  CeedCall(CeedElemRestrictionGetElementSize(active_rstr, &elem_size));
-  CeedCall(CeedElemRestrictionGetNumComponents(active_rstr, &num_comp));
-  CeedCall(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts));
-  local_num_entries = elem_size * num_comp * elem_size * num_comp * num_elem;
+  CeedCall(CeedOperatorGetOperatorAssemblyData(op, &data));
+  CeedCall(CeedOperatorAssemblyDataGetEvalModes(data, &num_active_bases_in, &num_eval_modes_in, &eval_modes_in, NULL, &num_active_bases_out,
+                                                &num_eval_modes_out, &eval_modes_out, NULL, NULL));
+
+  CeedCheck(num_active_bases_in == num_active_bases_out && num_active_bases_in == 1, ceed, CEED_ERROR_UNSUPPORTED,
+            "Cannot assemble operator with multiple active bases");
+  CeedCheck(num_eval_modes_in[0] > 0 && num_eval_modes_out[0] > 0, ceed, CEED_ERROR_UNSUPPORTED, "Cannot assemble operator without inputs/outputs");
+
+  CeedCall(CeedOperatorAssemblyDataGetBases(data, NULL, &active_bases_in, &B_mats_in, NULL, &active_bases_out, &B_mats_out));
+  CeedCall(CeedOperatorGetActiveElemRestrictions(op, &elem_rstr_in, &elem_rstr_out));
+  basis_in  = active_bases_in[0];
+  basis_out = active_bases_out[0];
+  B_mat_in  = B_mats_in[0];
+  B_mat_out = B_mats_out[0];
+
+  CeedCall(CeedElemRestrictionGetNumElements(elem_rstr_in, &num_elem_in));
+  CeedCall(CeedElemRestrictionGetElementSize(elem_rstr_in, &elem_size_in));
+  CeedCall(CeedElemRestrictionGetNumComponents(elem_rstr_in, &num_comp_in));
+  if (basis_in == CEED_BASIS_NONE) num_qpts_in = elem_size_in;
+  else CeedCall(CeedBasisGetNumQuadraturePoints(basis_in, &num_qpts_in));
+
+  CeedCall(CeedElemRestrictionGetType(elem_rstr_in, &elem_rstr_type_in));
+  if (elem_rstr_type_in == CEED_RESTRICTION_ORIENTED) {
+    CeedCall(CeedElemRestrictionGetOrientations(elem_rstr_in, CEED_MEM_HOST, &elem_rstr_orients_in));
+  } else if (elem_rstr_type_in == CEED_RESTRICTION_CURL_ORIENTED) {
+    CeedCall(CeedElemRestrictionGetCurlOrientations(elem_rstr_in, CEED_MEM_HOST, &elem_rstr_curl_orients_in));
+  }
+
+  if (elem_rstr_in != elem_rstr_out) {
+    CeedCall(CeedElemRestrictionGetNumElements(elem_rstr_out, &num_elem_out));
+    CeedCheck(num_elem_in == num_elem_out, ceed, CEED_ERROR_UNSUPPORTED,
+              "Active input and output operator restrictions must have the same number of elements");
+    CeedCall(CeedElemRestrictionGetElementSize(elem_rstr_out, &elem_size_out));
+    CeedCall(CeedElemRestrictionGetNumComponents(elem_rstr_out, &num_comp_out));
+    if (basis_out == CEED_BASIS_NONE) num_qpts_out = elem_size_out;
+    else CeedCall(CeedBasisGetNumQuadraturePoints(basis_out, &num_qpts_out));
+    CeedCheck(num_qpts_in == num_qpts_out, ceed, CEED_ERROR_UNSUPPORTED,
+              "Active input and output bases must have the same number of quadrature points");
+
+    CeedCall(CeedElemRestrictionGetType(elem_rstr_out, &elem_rstr_type_out));
+    if (elem_rstr_type_out == CEED_RESTRICTION_ORIENTED) {
+      CeedCall(CeedElemRestrictionGetOrientations(elem_rstr_out, CEED_MEM_HOST, &elem_rstr_orients_out));
+    } else if (elem_rstr_type_out == CEED_RESTRICTION_CURL_ORIENTED) {
+      CeedCall(CeedElemRestrictionGetCurlOrientations(elem_rstr_out, CEED_MEM_HOST, &elem_rstr_curl_orients_out));
+    }
+  } else {
+    num_elem_out  = num_elem_in;
+    elem_size_out = elem_size_in;
+    num_comp_out  = num_comp_in;
+    num_qpts_out  = num_qpts_in;
 
-  CeedCall(CeedElemRestrictionGetType(active_rstr, &rstr_type));
-  if (rstr_type == CEED_RESTRICTION_ORIENTED) {
-    CeedCall(CeedElemRestrictionGetOrientations(active_rstr, CEED_MEM_HOST, &orients));
-  } else if (rstr_type == CEED_RESTRICTION_CURL_ORIENTED) {
-    CeedCall(CeedElemRestrictionGetCurlOrientations(active_rstr, CEED_MEM_HOST, &curl_orients));
+    elem_rstr_orients_out      = elem_rstr_orients_in;
+    elem_rstr_curl_orients_out = elem_rstr_curl_orients_in;
   }
+  local_num_entries = elem_size_out * num_comp_out * elem_size_in * num_comp_in * num_elem_in;
 
   // Loop over elements and put in data structure
-  CeedCall(CeedVectorGetArrayRead(assembled_qf, CEED_MEM_HOST, &assembled_qf_array));
-  CeedCall(CeedElemRestrictionGetELayout(rstr_q, &layout_qf));
-  CeedCall(CeedElemRestrictionDestroy(&rstr_q));
-
   // We store B_mat_in, B_mat_out, BTD, elem_mat in row-major order
-  CeedSize           count = 0;
-  CeedScalar        *vals, BTD_mat[elem_size * num_qpts * num_eval_modes_in[0]], elem_mat[elem_size * elem_size];
-  const CeedScalar **B_mats_in, **B_mats_out;
-  CeedCall(CeedOperatorAssemblyDataGetBases(data, NULL, NULL, &B_mats_in, &B_mats_out));
-  const CeedScalar *B_mat_in = B_mats_in[0], *B_mat_out = B_mats_out[0];
+  CeedSize    count = 0;
+  CeedScalar *vals, BTD_mat[elem_size_out * num_qpts_in * num_eval_modes_in[0]], elem_mat[elem_size_out * elem_size_in], *elem_mat_b = NULL;
+
+  if (elem_rstr_curl_orients_in || elem_rstr_curl_orients_out) CeedCall(CeedCalloc(elem_size_out * elem_size_in, &elem_mat_b));
 
   CeedCall(CeedVectorGetArray(values, CEED_MEM_HOST, &vals));
-  for (CeedSize e = 0; e < num_elem; e++) {
-    for (CeedInt comp_in = 0; comp_in < num_comp; comp_in++) {
-      for (CeedInt comp_out = 0; comp_out < num_comp; comp_out++) {
+  for (CeedSize e = 0; e < num_elem_in; e++) {
+    for (CeedInt comp_in = 0; comp_in < num_comp_in; comp_in++) {
+      for (CeedInt comp_out = 0; comp_out < num_comp_out; comp_out++) {
         // Compute B^T*D
-        for (CeedSize n = 0; n < elem_size; n++) {
-          for (CeedSize q = 0; q < num_qpts; q++) {
+        for (CeedSize n = 0; n < elem_size_out; n++) {
+          for (CeedSize q = 0; q < num_qpts_in; q++) {
             for (CeedInt e_in = 0; e_in < num_eval_modes_in[0]; e_in++) {
-              const CeedSize btd_index = n * (num_qpts * num_eval_modes_in[0]) + (num_eval_modes_in[0] * q + e_in);
+              const CeedSize btd_index = n * (num_qpts_in * num_eval_modes_in[0]) + q * num_eval_modes_in[0] + e_in;
               CeedScalar     sum       = 0.0;
 
               for (CeedInt e_out = 0; e_out < num_eval_modes_out[0]; e_out++) {
-                const CeedSize b_out_index     = (num_eval_modes_out[0] * q + e_out) * elem_size + n;
-                const CeedSize eval_mode_index = ((e_in * num_comp + comp_in) * num_eval_modes_out[0] + e_out) * num_comp + comp_out;
+                const CeedSize b_out_index     = (q * num_eval_modes_out[0] + e_out) * elem_size_out + n;
+                const CeedSize eval_mode_index = ((e_in * num_comp_in + comp_in) * num_eval_modes_out[0] + e_out) * num_comp_out + comp_out;
                 const CeedSize qf_index        = q * layout_qf[0] + eval_mode_index * layout_qf[1] + e * layout_qf[2];
 
                 sum += B_mat_out[b_out_index] * assembled_qf_array[qf_index];
@@ -665,44 +677,58 @@ static int CeedSingleOperatorAssemble(CeedOperator op, CeedInt offset, CeedVecto
         }
 
         // Form element matrix itself (for each block component)
-        CeedCall(CeedMatrixMatrixMultiply(ceed, BTD_mat, B_mat_in, elem_mat, elem_size, elem_size, num_qpts * num_eval_modes_in[0]));
+        CeedCall(CeedMatrixMatrixMultiply(ceed, BTD_mat, B_mat_in, elem_mat, elem_size_out, elem_size_in, num_qpts_in * num_eval_modes_in[0]));
 
         // Transform the element matrix if required
-        if (orients) {
-          const bool *elem_orients = &orients[e * elem_size];
+        if (elem_rstr_orients_out) {
+          const bool *elem_orients = &elem_rstr_orients_out[e * elem_size_out];
+
+          for (CeedInt i = 0; i < elem_size_out; i++) {
+            const double orient = elem_orients[i] ? -1.0 : 1.0;
 
-          for (CeedInt i = 0; i < elem_size; i++) {
-            for (CeedInt j = 0; j < elem_size; j++) {
-              elem_mat[i * elem_size + j] *= elem_orients[i] ? -1.0 : 1.0;
-              elem_mat[i * elem_size + j] *= elem_orients[j] ? -1.0 : 1.0;
+            for (CeedInt j = 0; j < elem_size_in; j++) {
+              elem_mat[i * elem_size_in + j] *= orient;
             }
           }
-        } else if (curl_orients) {
-          const CeedInt8 *elem_curl_orients = &curl_orients[e * 3 * elem_size];
-          CeedScalar      o_elem_mat[elem_size * elem_size];
+        } else if (elem_rstr_curl_orients_out) {
+          const CeedInt8 *elem_curl_orients = &elem_rstr_curl_orients_out[e * 3 * elem_size_out];
 
           // T^T*(B^T*D*B)
-          for (CeedInt i = 0; i < elem_size; i++) {
-            for (CeedInt j = 0; j < elem_size; j++) {
-              o_elem_mat[i * elem_size + j] = elem_mat[i * elem_size + j] * elem_curl_orients[3 * i + 1] +
-                                              (i > 0 ? elem_mat[(i - 1) * elem_size + j] * elem_curl_orients[3 * i - 1] : 0.0) +
-                                              (i < elem_size - 1 ? elem_mat[(i + 1) * elem_size + j] * elem_curl_orients[3 * i + 3] : 0.0);
+          memcpy(elem_mat_b, elem_mat, elem_size_out * elem_size_in * sizeof(CeedScalar));
+          for (CeedInt i = 0; i < elem_size_out; i++) {
+            for (CeedInt j = 0; j < elem_size_in; j++) {
+              elem_mat[i * elem_size_in + j] = elem_mat_b[i * elem_size_in + j] * elem_curl_orients[3 * i + 1] +
+                                               (i > 0 ? elem_mat_b[(i - 1) * elem_size_in + j] * elem_curl_orients[3 * i - 1] : 0.0) +
+                                               (i < elem_size_out - 1 ? elem_mat_b[(i + 1) * elem_size_in + j] * elem_curl_orients[3 * i + 3] : 0.0);
             }
           }
-          // T^T*(B^T*D*B)*T
-          for (CeedInt i = 0; i < elem_size; i++) {
-            for (CeedInt j = 0; j < elem_size; j++) {
-              elem_mat[i * elem_size + j] = o_elem_mat[i * elem_size + j] * elem_curl_orients[3 * j + 1] +
-                                            (j > 0 ? o_elem_mat[i * elem_size + j - 1] * elem_curl_orients[3 * j - 1] : 0.0) +
-                                            (j < elem_size - 1 ? o_elem_mat[i * elem_size + j + 1] * elem_curl_orients[3 * j + 3] : 0.0);
+        }
+        if (elem_rstr_orients_in) {
+          const bool *elem_orients = &elem_rstr_orients_in[e * elem_size_in];
+
+          for (CeedInt i = 0; i < elem_size_out; i++) {
+            for (CeedInt j = 0; j < elem_size_in; j++) {
+              elem_mat[i * elem_size_in + j] *= elem_orients[j] ? -1.0 : 1.0;
+            }
+          }
+        } else if (elem_rstr_curl_orients_in) {
+          const CeedInt8 *elem_curl_orients = &elem_rstr_curl_orients_in[e * 3 * elem_size_in];
+
+          // (B^T*D*B)*T
+          memcpy(elem_mat_b, elem_mat, elem_size_out * elem_size_in * sizeof(CeedScalar));
+          for (CeedInt i = 0; i < elem_size_out; i++) {
+            for (CeedInt j = 0; j < elem_size_in; j++) {
+              elem_mat[i * elem_size_in + j] = elem_mat_b[i * elem_size_in + j] * elem_curl_orients[3 * j + 1] +
+                                               (j > 0 ? elem_mat_b[i * elem_size_in + j - 1] * elem_curl_orients[3 * j - 1] : 0.0) +
+                                               (j < elem_size_in - 1 ? elem_mat_b[i * elem_size_in + j + 1] * elem_curl_orients[3 * j + 3] : 0.0);
             }
           }
         }
 
         // Put element matrix in coordinate data structure
-        for (CeedInt i = 0; i < elem_size; i++) {
-          for (CeedInt j = 0; j < elem_size; j++) {
-            vals[offset + count] = elem_mat[i * elem_size + j];
+        for (CeedInt i = 0; i < elem_size_out; i++) {
+          for (CeedInt j = 0; j < elem_size_in; j++) {
+            vals[offset + count] = elem_mat[i * elem_size_in + j];
             count++;
           }
         }
@@ -712,14 +738,22 @@ static int CeedSingleOperatorAssemble(CeedOperator op, CeedInt offset, CeedVecto
   CeedCheck(count == local_num_entries, ceed, CEED_ERROR_MAJOR, "Error computing entries");
   CeedCall(CeedVectorRestoreArray(values, &vals));
 
+  // Cleanup
+  CeedCall(CeedFree(&elem_mat_b));
+  if (elem_rstr_type_in == CEED_RESTRICTION_ORIENTED) {
+    CeedCall(CeedElemRestrictionRestoreOrientations(elem_rstr_in, &elem_rstr_orients_in));
+  } else if (elem_rstr_type_in == CEED_RESTRICTION_CURL_ORIENTED) {
+    CeedCall(CeedElemRestrictionRestoreCurlOrientations(elem_rstr_in, &elem_rstr_curl_orients_in));
+  }
+  if (elem_rstr_in != elem_rstr_out) {
+    if (elem_rstr_type_out == CEED_RESTRICTION_ORIENTED) {
+      CeedCall(CeedElemRestrictionRestoreOrientations(elem_rstr_out, &elem_rstr_orients_out));
+    } else if (elem_rstr_type_out == CEED_RESTRICTION_CURL_ORIENTED) {
+      CeedCall(CeedElemRestrictionRestoreCurlOrientations(elem_rstr_out, &elem_rstr_curl_orients_out));
+    }
+  }
   CeedCall(CeedVectorRestoreArrayRead(assembled_qf, &assembled_qf_array));
   CeedCall(CeedVectorDestroy(&assembled_qf));
-
-  if (rstr_type == CEED_RESTRICTION_ORIENTED) {
-    CeedCall(CeedElemRestrictionRestoreOrientations(active_rstr, &orients));
-  } else if (rstr_type == CEED_RESTRICTION_CURL_ORIENTED) {
-    CeedCall(CeedElemRestrictionRestoreCurlOrientations(active_rstr, &curl_orients));
-  }
   return CEED_ERROR_SUCCESS;
 }
 
@@ -735,16 +769,28 @@ static int CeedSingleOperatorAssemble(CeedOperator op, CeedInt offset, CeedVecto
 **/
 static int CeedSingleOperatorAssemblyCountEntries(CeedOperator op, CeedSize *num_entries) {
   bool                is_composite;
-  CeedInt             num_elem, elem_size, num_comp;
-  CeedElemRestriction rstr;
+  CeedInt             num_elem_in, elem_size_in, num_comp_in, num_elem_out, elem_size_out, num_comp_out;
+  CeedElemRestriction rstr_in, rstr_out;
 
   CeedCall(CeedOperatorIsComposite(op, &is_composite));
   CeedCheck(!is_composite, op->ceed, CEED_ERROR_UNSUPPORTED, "Composite operator not supported");
-  CeedCall(CeedOperatorGetActiveElemRestriction(op, &rstr));
-  CeedCall(CeedElemRestrictionGetNumElements(rstr, &num_elem));
-  CeedCall(CeedElemRestrictionGetElementSize(rstr, &elem_size));
-  CeedCall(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
-  *num_entries = (CeedSize)elem_size * num_comp * elem_size * num_comp * num_elem;
+
+  CeedCall(CeedOperatorGetActiveElemRestrictions(op, &rstr_in, &rstr_out));
+  CeedCall(CeedElemRestrictionGetNumElements(rstr_in, &num_elem_in));
+  CeedCall(CeedElemRestrictionGetElementSize(rstr_in, &elem_size_in));
+  CeedCall(CeedElemRestrictionGetNumComponents(rstr_in, &num_comp_in));
+  if (rstr_in != rstr_out) {
+    CeedCall(CeedElemRestrictionGetNumElements(rstr_out, &num_elem_out));
+    CeedCheck(num_elem_in == num_elem_out, op->ceed, CEED_ERROR_UNSUPPORTED,
+              "Active input and output operator restrictions must have the same number of elements");
+    CeedCall(CeedElemRestrictionGetElementSize(rstr_out, &elem_size_out));
+    CeedCall(CeedElemRestrictionGetNumComponents(rstr_out, &num_comp_out));
+  } else {
+    num_elem_out  = num_elem_in;
+    elem_size_out = elem_size_in;
+    num_comp_out  = num_comp_in;
+  }
+  *num_entries = (CeedSize)elem_size_in * num_comp_in * elem_size_out * num_comp_out * num_elem_in;
   return CEED_ERROR_SUCCESS;
 }
 
@@ -758,14 +804,14 @@ static int CeedSingleOperatorAssemblyCountEntries(CeedOperator op, CeedSize *num
   @param[in]  basis_c_to_f Basis for coarse to fine interpolation, or NULL if not creating prolongation/restriction operators
   @param[out] op_coarse    Coarse grid operator
   @param[out] op_prolong   Coarse to fine operator, or NULL
-  @param[out] op_rstrict  Fine to coarse operator, or NULL
+  @param[out] op_restrict  Fine to coarse operator, or NULL
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref Developer
 **/
 static int CeedSingleOperatorMultigridLevel(CeedOperator op_fine, CeedVector p_mult_fine, CeedElemRestriction rstr_coarse, CeedBasis basis_coarse,
-                                            CeedBasis basis_c_to_f, CeedOperator *op_coarse, CeedOperator *op_prolong, CeedOperator *op_rstrict) {
+                                            CeedBasis basis_c_to_f, CeedOperator *op_coarse, CeedOperator *op_prolong, CeedOperator *op_restrict) {
   bool                is_composite;
   Ceed                ceed;
   CeedInt             num_comp;
@@ -803,7 +849,7 @@ static int CeedSingleOperatorMultigridLevel(CeedOperator op_fine, CeedVector p_m
   CeedCall(CeedQFunctionAssemblyDataReferenceCopy(op_fine->qf_assembled, &(*op_coarse)->qf_assembled));
 
   // Multiplicity vector
-  if (op_rstrict || op_prolong) {
+  if (op_restrict || op_prolong) {
     CeedVector          mult_e_vec;
     CeedRestrictionType rstr_type;
 
@@ -826,49 +872,49 @@ static int CeedSingleOperatorMultigridLevel(CeedOperator op_fine, CeedVector p_m
   size_t name_len = op_fine->name ? strlen(op_fine->name) : 0;
   CeedCall(CeedOperatorSetName(*op_coarse, op_fine->name));
 
-  // Check that coarse to fine basis is provided if prolong/rstrict operators are requested
-  CeedCheck(basis_c_to_f || (!op_rstrict && !op_prolong), ceed, CEED_ERROR_INCOMPATIBLE,
+  // Check that coarse to fine basis is provided if prolong/restrict operators are requested
+  CeedCheck(basis_c_to_f || (!op_restrict && !op_prolong), ceed, CEED_ERROR_INCOMPATIBLE,
             "Prolongation or restriction operator creation requires coarse-to-fine basis");
 
   // Restriction/Prolongation Operators
   CeedCall(CeedBasisGetNumComponents(basis_coarse, &num_comp));
 
   // Restriction
-  if (op_rstrict) {
+  if (op_restrict) {
     CeedInt             *num_comp_r_data;
     CeedQFunctionContext ctx_r;
-    CeedQFunction        qf_rstrict;
+    CeedQFunction        qf_restrict;
 
-    CeedCall(CeedQFunctionCreateInteriorByName(ceed, "Scale", &qf_rstrict));
+    CeedCall(CeedQFunctionCreateInteriorByName(ceed, "Scale", &qf_restrict));
     CeedCall(CeedCalloc(1, &num_comp_r_data));
     num_comp_r_data[0] = num_comp;
     CeedCall(CeedQFunctionContextCreate(ceed, &ctx_r));
     CeedCall(CeedQFunctionContextSetData(ctx_r, CEED_MEM_HOST, CEED_OWN_POINTER, sizeof(*num_comp_r_data), num_comp_r_data));
-    CeedCall(CeedQFunctionSetContext(qf_rstrict, ctx_r));
+    CeedCall(CeedQFunctionSetContext(qf_restrict, ctx_r));
     CeedCall(CeedQFunctionContextDestroy(&ctx_r));
-    CeedCall(CeedQFunctionAddInput(qf_rstrict, "input", num_comp, CEED_EVAL_NONE));
-    CeedCall(CeedQFunctionAddInput(qf_rstrict, "scale", num_comp, CEED_EVAL_NONE));
-    CeedCall(CeedQFunctionAddOutput(qf_rstrict, "output", num_comp, CEED_EVAL_INTERP));
-    CeedCall(CeedQFunctionSetUserFlopsEstimate(qf_rstrict, num_comp));
+    CeedCall(CeedQFunctionAddInput(qf_restrict, "input", num_comp, CEED_EVAL_NONE));
+    CeedCall(CeedQFunctionAddInput(qf_restrict, "scale", num_comp, CEED_EVAL_NONE));
+    CeedCall(CeedQFunctionAddOutput(qf_restrict, "output", num_comp, CEED_EVAL_INTERP));
+    CeedCall(CeedQFunctionSetUserFlopsEstimate(qf_restrict, num_comp));
 
-    CeedCall(CeedOperatorCreate(ceed, qf_rstrict, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, op_rstrict));
-    CeedCall(CeedOperatorSetField(*op_rstrict, "input", rstr_fine, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE));
-    CeedCall(CeedOperatorSetField(*op_rstrict, "scale", rstr_p_mult_fine, CEED_BASIS_NONE, mult_vec));
-    CeedCall(CeedOperatorSetField(*op_rstrict, "output", rstr_coarse, basis_c_to_f, CEED_VECTOR_ACTIVE));
+    CeedCall(CeedOperatorCreate(ceed, qf_restrict, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, op_restrict));
+    CeedCall(CeedOperatorSetField(*op_restrict, "input", rstr_fine, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE));
+    CeedCall(CeedOperatorSetField(*op_restrict, "scale", rstr_p_mult_fine, CEED_BASIS_NONE, mult_vec));
+    CeedCall(CeedOperatorSetField(*op_restrict, "output", rstr_coarse, basis_c_to_f, CEED_VECTOR_ACTIVE));
 
     // Set name
     char *restriction_name;
 
     CeedCall(CeedCalloc(17 + name_len, &restriction_name));
     sprintf(restriction_name, "restriction%s%s", has_name ? " for " : "", has_name ? op_fine->name : "");
-    CeedCall(CeedOperatorSetName(*op_rstrict, restriction_name));
+    CeedCall(CeedOperatorSetName(*op_restrict, restriction_name));
     CeedCall(CeedFree(&restriction_name));
 
     // Check
-    CeedCall(CeedOperatorCheckReady(*op_rstrict));
+    CeedCall(CeedOperatorCheckReady(*op_restrict));
 
     // Cleanup
-    CeedCall(CeedQFunctionDestroy(&qf_rstrict));
+    CeedCall(CeedQFunctionDestroy(&qf_restrict));
   }
 
   // Prolongation
@@ -968,6 +1014,81 @@ CeedPragmaOptimizeOn
 /// @addtogroup CeedOperatorBackend
 /// @{
 
+/**
+  @brief Select correct basis matrix pointer based on CeedEvalMode
+
+  @param[in]  basis     CeedBasis from which to get the basis matrix
+  @param[in]  eval_mode Current basis evaluation mode
+  @param[in]  identity  Pointer to identity matrix
+  @param[out] basis_ptr Basis pointer to set
+
+  @ref Backend
+**/
+int CeedOperatorGetBasisPointer(CeedBasis basis, CeedEvalMode eval_mode, const CeedScalar *identity, const CeedScalar **basis_ptr) {
+  switch (eval_mode) {
+    case CEED_EVAL_NONE:
+      *basis_ptr = identity;
+      break;
+    case CEED_EVAL_INTERP:
+      CeedCall(CeedBasisGetInterp(basis, basis_ptr));
+      break;
+    case CEED_EVAL_GRAD:
+      CeedCall(CeedBasisGetGrad(basis, basis_ptr));
+      break;
+    case CEED_EVAL_DIV:
+      CeedCall(CeedBasisGetDiv(basis, basis_ptr));
+      break;
+    case CEED_EVAL_CURL:
+      CeedCall(CeedBasisGetCurl(basis, basis_ptr));
+      break;
+    case CEED_EVAL_WEIGHT:
+      break;  // Caught by QF Assembly
+  }
+  assert(*basis_ptr != NULL);
+  return CEED_ERROR_SUCCESS;
+}
+
+/**
+  @brief Create point block restriction for active operator field
+
+  @param[in]  rstr             Original CeedElemRestriction for active field
+  @param[out] point_block_rstr Address of the variable where the newly created CeedElemRestriction will be stored
+
+  @return An error code: 0 - success, otherwise - failure
+
+  @ref Backend
+**/
+int CeedOperatorCreateActivePointBlockRestriction(CeedElemRestriction rstr, CeedElemRestriction *point_block_rstr) {
+  Ceed           ceed;
+  CeedInt        num_elem, num_comp, shift, elem_size, comp_stride, *point_block_offsets;
+  CeedSize       l_size;
+  const CeedInt *offsets;
+
+  CeedCall(CeedElemRestrictionGetCeed(rstr, &ceed));
+  CeedCall(CeedElemRestrictionGetOffsets(rstr, CEED_MEM_HOST, &offsets));
+
+  // Expand offsets
+  CeedCall(CeedElemRestrictionGetNumElements(rstr, &num_elem));
+  CeedCall(CeedElemRestrictionGetNumComponents(rstr, &num_comp));
+  CeedCall(CeedElemRestrictionGetElementSize(rstr, &elem_size));
+  CeedCall(CeedElemRestrictionGetCompStride(rstr, &comp_stride));
+  CeedCall(CeedElemRestrictionGetLVectorSize(rstr, &l_size));
+  shift = num_comp;
+  if (comp_stride != 1) shift *= num_comp;
+  CeedCall(CeedCalloc(num_elem * elem_size, &point_block_offsets));
+  for (CeedInt i = 0; i < num_elem * elem_size; i++) {
+    point_block_offsets[i] = offsets[i] * shift;
+  }
+
+  // Create new restriction
+  CeedCall(CeedElemRestrictionCreate(ceed, num_elem, elem_size, num_comp * num_comp, 1, l_size * num_comp, CEED_MEM_HOST, CEED_OWN_POINTER,
+                                     point_block_offsets, point_block_rstr));
+
+  // Cleanup
+  CeedCall(CeedElemRestrictionRestoreOffsets(rstr, &offsets));
+  return CEED_ERROR_SUCCESS;
+}
+
 /**
   @brief Create object holding CeedQFunction assembly data for CeedOperator
 
@@ -1174,7 +1295,8 @@ CeedEvalMode.
   @ref Backend
 **/
 int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssemblyData *data) {
-  CeedInt             num_active_bases = 0, num_input_fields, *num_eval_modes_in = NULL, *num_eval_modes_out = NULL, offset = 0, num_output_fields;
+  CeedInt             num_active_bases_in = 0, num_active_bases_out = 0, offset = 0;
+  CeedInt             num_input_fields, *num_eval_modes_in = NULL, num_output_fields, *num_eval_modes_out = NULL;
   CeedSize          **eval_mode_offsets_in = NULL, **eval_mode_offsets_out = NULL;
   CeedEvalMode      **eval_modes_in = NULL, **eval_modes_out = NULL;
   CeedQFunctionField *qf_fields;
@@ -1192,10 +1314,10 @@ int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssem
 
   // Build OperatorAssembly data
   CeedCall(CeedOperatorGetQFunction(op, &qf));
-  CeedCall(CeedQFunctionGetFields(qf, &num_input_fields, &qf_fields, NULL, NULL));
-  CeedCall(CeedOperatorGetFields(op, NULL, &op_fields, NULL, NULL));
 
   // Determine active input basis
+  CeedCall(CeedQFunctionGetFields(qf, &num_input_fields, &qf_fields, NULL, NULL));
+  CeedCall(CeedOperatorGetFields(op, NULL, &op_fields, NULL, NULL));
   for (CeedInt i = 0; i < num_input_fields; i++) {
     CeedVector vec;
 
@@ -1209,37 +1331,29 @@ int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssem
       CeedCall(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
       CeedCall(CeedBasisGetNumComponents(basis_in, &num_comp));
       CeedCall(CeedBasisGetNumQuadratureComponents(basis_in, eval_mode, &q_comp));
-      for (CeedInt i = 0; i < num_active_bases; i++) {
-        if ((*data)->active_bases[i] == basis_in) index = i;
+      for (CeedInt i = 0; i < num_active_bases_in; i++) {
+        if ((*data)->active_bases_in[i] == basis_in) index = i;
       }
       if (index == -1) {
         CeedElemRestriction elem_rstr_in;
 
-        index = num_active_bases;
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->active_bases));
-        (*data)->active_bases[num_active_bases] = NULL;
-        CeedCall(CeedBasisReferenceCopy(basis_in, &(*data)->active_bases[num_active_bases]));
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->active_elem_rstrs));
-        (*data)->active_elem_rstrs[num_active_bases] = NULL;
+        index = num_active_bases_in;
+        CeedCall(CeedRealloc(num_active_bases_in + 1, &(*data)->active_bases_in));
+        (*data)->active_bases_in[num_active_bases_in] = NULL;
+        CeedCall(CeedBasisReferenceCopy(basis_in, &(*data)->active_bases_in[num_active_bases_in]));
+        CeedCall(CeedRealloc(num_active_bases_in + 1, &(*data)->active_elem_rstrs_in));
+        (*data)->active_elem_rstrs_in[num_active_bases_in] = NULL;
         CeedCall(CeedOperatorFieldGetElemRestriction(op_fields[i], &elem_rstr_in));
-        CeedCall(CeedElemRestrictionReferenceCopy(elem_rstr_in, &(*data)->active_elem_rstrs[num_active_bases]));
-        CeedCall(CeedRealloc(num_active_bases + 1, &num_eval_modes_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &num_eval_modes_out));
-        num_eval_modes_in[index]  = 0;
-        num_eval_modes_out[index] = 0;
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_modes_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_modes_out));
-        eval_modes_in[index]  = NULL;
-        eval_modes_out[index] = NULL;
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_mode_offsets_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_mode_offsets_out));
-        eval_mode_offsets_in[index]  = NULL;
-        eval_mode_offsets_out[index] = NULL;
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->assembled_bases_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->assembled_bases_out));
-        (*data)->assembled_bases_in[index]  = NULL;
-        (*data)->assembled_bases_out[index] = NULL;
-        num_active_bases++;
+        CeedCall(CeedElemRestrictionReferenceCopy(elem_rstr_in, &(*data)->active_elem_rstrs_in[num_active_bases_in]));
+        CeedCall(CeedRealloc(num_active_bases_in + 1, &num_eval_modes_in));
+        num_eval_modes_in[index] = 0;
+        CeedCall(CeedRealloc(num_active_bases_in + 1, &eval_modes_in));
+        eval_modes_in[index] = NULL;
+        CeedCall(CeedRealloc(num_active_bases_in + 1, &eval_mode_offsets_in));
+        eval_mode_offsets_in[index] = NULL;
+        CeedCall(CeedRealloc(num_active_bases_in + 1, &(*data)->assembled_bases_in));
+        (*data)->assembled_bases_in[index] = NULL;
+        num_active_bases_in++;
       }
       if (eval_mode != CEED_EVAL_WEIGHT) {
         // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF Assembly
@@ -1272,37 +1386,29 @@ int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssem
       CeedCall(CeedQFunctionFieldGetEvalMode(qf_fields[i], &eval_mode));
       CeedCall(CeedBasisGetNumComponents(basis_out, &num_comp));
       CeedCall(CeedBasisGetNumQuadratureComponents(basis_out, eval_mode, &q_comp));
-      for (CeedInt i = 0; i < num_active_bases; i++) {
-        if ((*data)->active_bases[i] == basis_out) index = i;
+      for (CeedInt i = 0; i < num_active_bases_out; i++) {
+        if ((*data)->active_bases_out[i] == basis_out) index = i;
       }
       if (index == -1) {
         CeedElemRestriction elem_rstr_out;
 
-        index = num_active_bases;
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->active_bases));
-        (*data)->active_bases[num_active_bases] = NULL;
-        CeedCall(CeedBasisReferenceCopy(basis_out, &(*data)->active_bases[num_active_bases]));
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->active_elem_rstrs));
-        (*data)->active_elem_rstrs[num_active_bases] = NULL;
+        index = num_active_bases_out;
+        CeedCall(CeedRealloc(num_active_bases_out + 1, &(*data)->active_bases_out));
+        (*data)->active_bases_out[num_active_bases_out] = NULL;
+        CeedCall(CeedBasisReferenceCopy(basis_out, &(*data)->active_bases_out[num_active_bases_out]));
+        CeedCall(CeedRealloc(num_active_bases_out + 1, &(*data)->active_elem_rstrs_out));
+        (*data)->active_elem_rstrs_out[num_active_bases_out] = NULL;
         CeedCall(CeedOperatorFieldGetElemRestriction(op_fields[i], &elem_rstr_out));
-        CeedCall(CeedElemRestrictionReferenceCopy(elem_rstr_out, &(*data)->active_elem_rstrs[num_active_bases]));
-        CeedCall(CeedRealloc(num_active_bases + 1, &num_eval_modes_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &num_eval_modes_out));
-        num_eval_modes_in[index]  = 0;
+        CeedCall(CeedElemRestrictionReferenceCopy(elem_rstr_out, &(*data)->active_elem_rstrs_out[num_active_bases_out]));
+        CeedCall(CeedRealloc(num_active_bases_out + 1, &num_eval_modes_out));
         num_eval_modes_out[index] = 0;
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_modes_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_modes_out));
-        eval_modes_in[index]  = NULL;
+        CeedCall(CeedRealloc(num_active_bases_out + 1, &eval_modes_out));
         eval_modes_out[index] = NULL;
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_mode_offsets_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &eval_mode_offsets_out));
-        eval_mode_offsets_in[index]  = NULL;
+        CeedCall(CeedRealloc(num_active_bases_out + 1, &eval_mode_offsets_out));
         eval_mode_offsets_out[index] = NULL;
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->assembled_bases_in));
-        CeedCall(CeedRealloc(num_active_bases + 1, &(*data)->assembled_bases_out));
-        (*data)->assembled_bases_in[index]  = NULL;
+        CeedCall(CeedRealloc(num_active_bases_out + 1, &(*data)->assembled_bases_out));
         (*data)->assembled_bases_out[index] = NULL;
-        num_active_bases++;
+        num_active_bases_out++;
       }
       if (eval_mode != CEED_EVAL_WEIGHT) {
         // q_comp = 1 if CEED_EVAL_NONE, CEED_EVAL_WEIGHT caught by QF Assembly
@@ -1317,14 +1423,15 @@ int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssem
       }
     }
   }
+  (*data)->num_active_bases_in   = num_active_bases_in;
   (*data)->num_eval_modes_in     = num_eval_modes_in;
   (*data)->eval_modes_in         = eval_modes_in;
   (*data)->eval_mode_offsets_in  = eval_mode_offsets_in;
-  (*data)->num_output_components = offset;
+  (*data)->num_active_bases_out  = num_active_bases_out;
   (*data)->num_eval_modes_out    = num_eval_modes_out;
   (*data)->eval_modes_out        = eval_modes_out;
   (*data)->eval_mode_offsets_out = eval_mode_offsets_out;
-  (*data)->num_active_bases      = num_active_bases;
+  (*data)->num_output_components = offset;
   return CEED_ERROR_SUCCESS;
 }
 
@@ -1334,11 +1441,12 @@ int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssem
   Note: See CeedOperatorAssemblyDataCreate for a full description of the data stored in this object.
 
   @param[in]  data                  CeedOperatorAssemblyData
-  @param[out] num_active_bases      Total number of active bases
+  @param[out] num_active_bases_in   Total number of active bases for input
   @param[out] num_eval_modes_in     Pointer to hold array of numbers of input CeedEvalModes, or NULL.
                                       `eval_modes_in[0]` holds an array of eval modes for the first active basis.
   @param[out] eval_modes_in         Pointer to hold arrays of input CeedEvalModes, or NULL.
   @param[out] eval_mode_offsets_in  Pointer to hold arrays of input offsets at each quadrature point.
+  @param[out] num_active_bases_out  Total number of active bases for output
   @param[out] num_eval_modes_out    Pointer to hold array of numbers of output CeedEvalModes, or NULL
   @param[out] eval_modes_out        Pointer to hold arrays of output CeedEvalModes, or NULL.
   @param[out] eval_mode_offsets_out Pointer to hold arrays of output offsets at each quadrature point
@@ -1347,16 +1455,17 @@ int CeedOperatorAssemblyDataCreate(Ceed ceed, CeedOperator op, CeedOperatorAssem
 
   @return An error code: 0 - success, otherwise - failure
 
-
   @ref Backend
 **/
-int CeedOperatorAssemblyDataGetEvalModes(CeedOperatorAssemblyData data, CeedInt *num_active_bases, CeedInt **num_eval_modes_in,
-                                         const CeedEvalMode ***eval_modes_in, CeedSize ***eval_mode_offsets_in, CeedInt **num_eval_modes_out,
-                                         const CeedEvalMode ***eval_modes_out, CeedSize ***eval_mode_offsets_out, CeedSize *num_output_components) {
-  if (num_active_bases) *num_active_bases = data->num_active_bases;
+int CeedOperatorAssemblyDataGetEvalModes(CeedOperatorAssemblyData data, CeedInt *num_active_bases_in, CeedInt **num_eval_modes_in,
+                                         const CeedEvalMode ***eval_modes_in, CeedSize ***eval_mode_offsets_in, CeedInt *num_active_bases_out,
+                                         CeedInt **num_eval_modes_out, const CeedEvalMode ***eval_modes_out, CeedSize ***eval_mode_offsets_out,
+                                         CeedSize *num_output_components) {
+  if (num_active_bases_in) *num_active_bases_in = data->num_active_bases_in;
   if (num_eval_modes_in) *num_eval_modes_in = data->num_eval_modes_in;
   if (eval_modes_in) *eval_modes_in = (const CeedEvalMode **)data->eval_modes_in;
   if (eval_mode_offsets_in) *eval_mode_offsets_in = data->eval_mode_offsets_in;
+  if (num_active_bases_out) *num_active_bases_out = data->num_active_bases_out;
   if (num_eval_modes_out) *num_eval_modes_out = data->num_eval_modes_out;
   if (eval_modes_out) *eval_modes_out = (const CeedEvalMode **)data->eval_modes_out;
   if (eval_mode_offsets_out) *eval_mode_offsets_out = data->eval_mode_offsets_out;
@@ -1369,29 +1478,33 @@ int CeedOperatorAssemblyDataGetEvalModes(CeedOperatorAssemblyData data, CeedInt
 
   Note: See CeedOperatorAssemblyDataCreate for a full description of the data stored in this object.
 
-  @param[in]  data                CeedOperatorAssemblyData
-  @param[out] num_active_bases    Number of active bases, or NULL
-  @param[out] active_bases        Pointer to hold active CeedBasis, or NULL
-  @param[out] assembled_bases_in  Pointer to hold assembled active input B, or NULL
-  @param[out] assembled_bases_out Pointer to hold assembled active output B, or NULL
+  @param[in]  data                 CeedOperatorAssemblyData
+  @param[out] num_active_bases_in  Number of active input bases, or NULL
+  @param[out] active_bases_in      Pointer to hold active input CeedBasis, or NULL
+  @param[out] assembled_bases_in   Pointer to hold assembled active input B, or NULL
+  @param[out] num_active_bases_out Number of active output bases, or NULL
+  @param[out] active_bases_out     Pointer to hold active output CeedBasis, or NULL
+  @param[out] assembled_bases_out  Pointer to hold assembled active output B, or NULL
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref Backend
 **/
-int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num_active_bases, CeedBasis **active_bases,
-                                     const CeedScalar ***assembled_bases_in, const CeedScalar ***assembled_bases_out) {
+int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num_active_bases_in, CeedBasis **active_bases_in,
+                                     const CeedScalar ***assembled_bases_in, CeedInt *num_active_bases_out, CeedBasis **active_bases_out,
+                                     const CeedScalar ***assembled_bases_out) {
   // Assemble B_in, B_out if needed
   if (assembled_bases_in && !data->assembled_bases_in[0]) {
     CeedInt num_qpts;
 
-    CeedCall(CeedBasisGetNumQuadraturePoints(data->active_bases[0], &num_qpts));
-    for (CeedInt b = 0; b < data->num_active_bases; b++) {
+    if (data->active_bases_in[0] == CEED_BASIS_NONE) CeedCall(CeedElemRestrictionGetElementSize(data->active_elem_rstrs_in[0], &num_qpts));
+    else CeedCall(CeedBasisGetNumQuadraturePoints(data->active_bases_in[0], &num_qpts));
+    for (CeedInt b = 0; b < data->num_active_bases_in; b++) {
       bool        has_eval_none = false;
       CeedInt     num_nodes;
       CeedScalar *B_in = NULL, *identity = NULL;
 
-      CeedCall(CeedBasisGetNumNodes(data->active_bases[b], &num_nodes));
+      CeedCall(CeedElemRestrictionGetElementSize(data->active_elem_rstrs_in[b], &num_nodes));
       CeedCall(CeedCalloc(num_qpts * num_nodes * data->num_eval_modes_in[b], &B_in));
 
       for (CeedInt i = 0; i < data->num_eval_modes_in[b]; i++) {
@@ -1413,8 +1526,8 @@ int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num
             const CeedInt     qq = data->num_eval_modes_in[b] * q;
             const CeedScalar *B  = NULL;
 
-            CeedOperatorGetBasisPointer(data->active_bases[b], data->eval_modes_in[b][e_in], identity, &B);
-            CeedCall(CeedBasisGetNumQuadratureComponents(data->active_bases[b], data->eval_modes_in[b][e_in], &q_comp_in));
+            CeedCall(CeedOperatorGetBasisPointer(data->active_bases_in[b], data->eval_modes_in[b][e_in], identity, &B));
+            CeedCall(CeedBasisGetNumQuadratureComponents(data->active_bases_in[b], data->eval_modes_in[b][e_in], &q_comp_in));
             if (q_comp_in > 1) {
               if (e_in == 0 || data->eval_modes_in[b][e_in] != eval_mode_in_prev) d_in = 0;
               else B = &B[(++d_in) * num_qpts * num_nodes];
@@ -1432,13 +1545,14 @@ int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num
   if (assembled_bases_out && !data->assembled_bases_out[0]) {
     CeedInt num_qpts;
 
-    CeedCall(CeedBasisGetNumQuadraturePoints(data->active_bases[0], &num_qpts));
-    for (CeedInt b = 0; b < data->num_active_bases; b++) {
+    if (data->active_bases_out[0] == CEED_BASIS_NONE) CeedCall(CeedElemRestrictionGetElementSize(data->active_elem_rstrs_out[0], &num_qpts));
+    else CeedCall(CeedBasisGetNumQuadraturePoints(data->active_bases_out[0], &num_qpts));
+    for (CeedInt b = 0; b < data->num_active_bases_out; b++) {
       bool        has_eval_none = false;
       CeedInt     num_nodes;
       CeedScalar *B_out = NULL, *identity = NULL;
 
-      CeedCall(CeedBasisGetNumNodes(data->active_bases[b], &num_nodes));
+      CeedCall(CeedElemRestrictionGetElementSize(data->active_elem_rstrs_out[b], &num_nodes));
       CeedCall(CeedCalloc(num_qpts * num_nodes * data->num_eval_modes_out[b], &B_out));
 
       for (CeedInt i = 0; i < data->num_eval_modes_out[b]; i++) {
@@ -1460,8 +1574,8 @@ int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num
             const CeedInt     qq = data->num_eval_modes_out[b] * q;
             const CeedScalar *B  = NULL;
 
-            CeedOperatorGetBasisPointer(data->active_bases[b], data->eval_modes_out[b][e_out], identity, &B);
-            CeedCall(CeedBasisGetNumQuadratureComponents(data->active_bases[b], data->eval_modes_out[b][e_out], &q_comp_out));
+            CeedCall(CeedOperatorGetBasisPointer(data->active_bases_out[b], data->eval_modes_out[b][e_out], identity, &B));
+            CeedCall(CeedBasisGetNumQuadratureComponents(data->active_bases_out[b], data->eval_modes_out[b][e_out], &q_comp_out));
             if (q_comp_out > 1) {
               if (e_out == 0 || data->eval_modes_out[b][e_out] != eval_mode_out_prev) d_out = 0;
               else B = &B[(++d_out) * num_qpts * num_nodes];
@@ -1477,8 +1591,11 @@ int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num
   }
 
   // Pass out assembled data
-  if (active_bases) *active_bases = data->active_bases;
+  if (num_active_bases_in) *num_active_bases_in = data->num_active_bases_in;
+  if (active_bases_in) *active_bases_in = data->active_bases_in;
   if (assembled_bases_in) *assembled_bases_in = (const CeedScalar **)data->assembled_bases_in;
+  if (num_active_bases_out) *num_active_bases_out = data->num_active_bases_out;
+  if (active_bases_out) *active_bases_out = data->active_bases_out;
   if (assembled_bases_out) *assembled_bases_out = (const CeedScalar **)data->assembled_bases_out;
   return CEED_ERROR_SUCCESS;
 }
@@ -1488,18 +1605,23 @@ int CeedOperatorAssemblyDataGetBases(CeedOperatorAssemblyData data, CeedInt *num
 
   Note: See CeedOperatorAssemblyDataCreate for a full description of the data stored in this object.
 
-  @param[in]  data                  CeedOperatorAssemblyData
-  @param[out] num_active_elem_rstrs Number of active element restrictions, or NULL
-  @param[out] active_elem_rstrs     Pointer to hold active CeedElemRestrictions, or NULL
+  @param[in]  data                      CeedOperatorAssemblyData
+  @param[out] num_active_elem_rstrs_in  Number of active input element restrictions, or NULL
+  @param[out] active_elem_rstrs_in      Pointer to hold active input CeedElemRestrictions, or NULL
+  @param[out] num_active_elem_rstrs_out Number of active output element restrictions, or NULL
+  @param[out] active_elem_rstrs_out     Pointer to hold active output CeedElemRestrictions, or NULL
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref Backend
 **/
-int CeedOperatorAssemblyDataGetElemRestrictions(CeedOperatorAssemblyData data, CeedInt *num_active_elem_rstrs,
-                                                CeedElemRestriction **active_elem_rstrs) {
-  if (num_active_elem_rstrs) *num_active_elem_rstrs = data->num_active_bases;
-  if (active_elem_rstrs) *active_elem_rstrs = data->active_elem_rstrs;
+int CeedOperatorAssemblyDataGetElemRestrictions(CeedOperatorAssemblyData data, CeedInt *num_active_elem_rstrs_in,
+                                                CeedElemRestriction **active_elem_rstrs_in, CeedInt *num_active_elem_rstrs_out,
+                                                CeedElemRestriction **active_elem_rstrs_out) {
+  if (num_active_elem_rstrs_in) *num_active_elem_rstrs_in = data->num_active_bases_in;
+  if (active_elem_rstrs_in) *active_elem_rstrs_in = data->active_elem_rstrs_in;
+  if (num_active_elem_rstrs_out) *num_active_elem_rstrs_out = data->num_active_bases_out;
+  if (active_elem_rstrs_out) *active_elem_rstrs_out = data->active_elem_rstrs_out;
   return CEED_ERROR_SUCCESS;
 }
 
@@ -1518,18 +1640,24 @@ int CeedOperatorAssemblyDataDestroy(CeedOperatorAssemblyData *data) {
     return CEED_ERROR_SUCCESS;
   }
   CeedCall(CeedDestroy(&(*data)->ceed));
-  for (CeedInt b = 0; b < (*data)->num_active_bases; b++) {
-    CeedCall(CeedBasisDestroy(&(*data)->active_bases[b]));
-    CeedCall(CeedElemRestrictionDestroy(&(*data)->active_elem_rstrs[b]));
+  for (CeedInt b = 0; b < (*data)->num_active_bases_in; b++) {
+    CeedCall(CeedBasisDestroy(&(*data)->active_bases_in[b]));
+    CeedCall(CeedElemRestrictionDestroy(&(*data)->active_elem_rstrs_in[b]));
     CeedCall(CeedFree(&(*data)->eval_modes_in[b]));
-    CeedCall(CeedFree(&(*data)->eval_modes_out[b]));
     CeedCall(CeedFree(&(*data)->eval_mode_offsets_in[b]));
-    CeedCall(CeedFree(&(*data)->eval_mode_offsets_out[b]));
     CeedCall(CeedFree(&(*data)->assembled_bases_in[b]));
+  }
+  for (CeedInt b = 0; b < (*data)->num_active_bases_out; b++) {
+    CeedCall(CeedBasisDestroy(&(*data)->active_bases_out[b]));
+    CeedCall(CeedElemRestrictionDestroy(&(*data)->active_elem_rstrs_out[b]));
+    CeedCall(CeedFree(&(*data)->eval_modes_out[b]));
+    CeedCall(CeedFree(&(*data)->eval_mode_offsets_out[b]));
     CeedCall(CeedFree(&(*data)->assembled_bases_out[b]));
   }
-  CeedCall(CeedFree(&(*data)->active_bases));
-  CeedCall(CeedFree(&(*data)->active_elem_rstrs));
+  CeedCall(CeedFree(&(*data)->active_bases_in));
+  CeedCall(CeedFree(&(*data)->active_bases_out));
+  CeedCall(CeedFree(&(*data)->active_elem_rstrs_in));
+  CeedCall(CeedFree(&(*data)->active_elem_rstrs_out));
   CeedCall(CeedFree(&(*data)->num_eval_modes_in));
   CeedCall(CeedFree(&(*data)->num_eval_modes_out));
   CeedCall(CeedFree(&(*data)->eval_modes_in));
@@ -1825,37 +1953,34 @@ int CeedOperatorLinearAssemblePointBlockDiagonalSymbolic(CeedOperator op, CeedSi
     num_sub_operators = 1;
   }
 
-  {  // Verify operator can be assembled correctly
-    CeedInt                  num_active_elem_rstrs, comp_stride;
+  // Verify operator can be assembled correctly
+  {
     CeedOperatorAssemblyData data;
+    CeedInt                  num_active_elem_rstrs, comp_stride;
     CeedElemRestriction     *active_elem_rstrs;
 
     // Get initial values to check against
     CeedCall(CeedOperatorGetOperatorAssemblyData(sub_operators[0], &data));
-    CeedCall(CeedOperatorAssemblyDataGetElemRestrictions(data, &num_active_elem_rstrs, &active_elem_rstrs));
+    CeedCall(CeedOperatorAssemblyDataGetElemRestrictions(data, &num_active_elem_rstrs, &active_elem_rstrs, NULL, NULL));
     CeedCall(CeedElemRestrictionGetCompStride(active_elem_rstrs[0], &comp_stride));
     CeedCall(CeedElemRestrictionGetNumComponents(active_elem_rstrs[0], &num_active_components));
 
+    // Verify that all active element restrictions have same component stride and number of components
     for (CeedInt k = 0; k < num_sub_operators; k++) {
       CeedCall(CeedOperatorGetOperatorAssemblyData(sub_operators[k], &data));
-
-      // Verify that all active element restrictions have same component stride and number of components
-      CeedCall(CeedOperatorAssemblyDataGetElemRestrictions(data, &num_active_elem_rstrs, &active_elem_rstrs));
-      CeedCall(CeedElemRestrictionGetCompStride(active_elem_rstrs[0], &comp_stride));
+      CeedCall(CeedOperatorAssemblyDataGetElemRestrictions(data, &num_active_elem_rstrs, &active_elem_rstrs, NULL, NULL));
       for (CeedInt i = 0; i < num_active_elem_rstrs; i++) {
-        CeedInt comp_stride_sub;
+        CeedInt comp_stride_sub, num_active_components_sub;
+
         CeedCall(CeedElemRestrictionGetCompStride(active_elem_rstrs[i], &comp_stride_sub));
         CeedCheck(comp_stride == comp_stride_sub, ceed, CEED_ERROR_DIMENSION,
                   "Active element restrictions must have the same component stride: %d vs %d", comp_stride, comp_stride_sub);
-
-        CeedInt num_active_components_sub;
         CeedCall(CeedElemRestrictionGetNumComponents(active_elem_rstrs[i], &num_active_components_sub));
         CeedCheck(num_active_components == num_active_components_sub, ceed, CEED_ERROR_INCOMPATIBLE,
                   "All suboperators must have the same number of output components");
       }
     }
   }
-
   *num_entries = input_size * num_active_components;
   CeedCall(CeedCalloc(*num_entries, rows));
   CeedCall(CeedCalloc(*num_entries, cols));
@@ -2059,7 +2184,7 @@ int CeedOperatorLinearAssembleSymbolic(CeedOperator op, CeedSize *num_entries, C
 
   // Default interface implementation
 
-  // count entries and allocate rows, cols arrays
+  // Count entries and allocate rows, cols arrays
   *num_entries = 0;
   if (is_composite) {
     CeedCall(CeedCompositeOperatorGetNumSub(op, &num_suboperators));
@@ -2075,7 +2200,7 @@ int CeedOperatorLinearAssembleSymbolic(CeedOperator op, CeedSize *num_entries, C
   CeedCall(CeedCalloc(*num_entries, rows));
   CeedCall(CeedCalloc(*num_entries, cols));
 
-  // assemble nonzero locations
+  // Assemble nonzero locations
   if (is_composite) {
     CeedCall(CeedCompositeOperatorGetNumSub(op, &num_suboperators));
     CeedCall(CeedCompositeOperatorGetSubList(op, &sub_operators));
@@ -2242,20 +2367,20 @@ grid interpolation
   @param[in]  basis_coarse Coarse grid active vector basis
   @param[out] op_coarse    Coarse grid operator
   @param[out] op_prolong   Coarse to fine operator, or NULL
-  @param[out] op_rstrict  Fine to coarse operator, or NULL
+  @param[out] op_restrict  Fine to coarse operator, or NULL
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref User
 **/
 int CeedOperatorMultigridLevelCreate(CeedOperator op_fine, CeedVector p_mult_fine, CeedElemRestriction rstr_coarse, CeedBasis basis_coarse,
-                                     CeedOperator *op_coarse, CeedOperator *op_prolong, CeedOperator *op_rstrict) {
+                                     CeedOperator *op_coarse, CeedOperator *op_prolong, CeedOperator *op_restrict) {
   CeedBasis basis_c_to_f = NULL;
 
   CeedCall(CeedOperatorCheckReady(op_fine));
 
   // Build prolongation matrix, if required
-  if (op_prolong || op_rstrict) {
+  if (op_prolong || op_restrict) {
     CeedBasis basis_fine;
 
     CeedCall(CeedOperatorGetActiveBasis(op_fine, &basis_fine));
@@ -2263,7 +2388,7 @@ int CeedOperatorMultigridLevelCreate(CeedOperator op_fine, CeedVector p_mult_fin
   }
 
   // Core code
-  CeedCall(CeedSingleOperatorMultigridLevel(op_fine, p_mult_fine, rstr_coarse, basis_coarse, basis_c_to_f, op_coarse, op_prolong, op_rstrict));
+  CeedCall(CeedSingleOperatorMultigridLevel(op_fine, p_mult_fine, rstr_coarse, basis_coarse, basis_c_to_f, op_coarse, op_prolong, op_restrict));
   return CEED_ERROR_SUCCESS;
 }
 
@@ -2279,7 +2404,7 @@ int CeedOperatorMultigridLevelCreate(CeedOperator op_fine, CeedVector p_mult_fin
   @param[in]  interp_c_to_f Matrix for coarse to fine interpolation, or NULL if not creating prolongation/restriction operators
   @param[out] op_coarse     Coarse grid operator
   @param[out] op_prolong    Coarse to fine operator, or NULL
-  @param[out] op_rstrict   Fine to coarse operator, or NULL
+  @param[out] op_restrict   Fine to coarse operator, or NULL
 
   @return An error code: 0 - success, otherwise - failure
 
@@ -2287,7 +2412,7 @@ int CeedOperatorMultigridLevelCreate(CeedOperator op_fine, CeedVector p_mult_fin
 **/
 int CeedOperatorMultigridLevelCreateTensorH1(CeedOperator op_fine, CeedVector p_mult_fine, CeedElemRestriction rstr_coarse, CeedBasis basis_coarse,
                                              const CeedScalar *interp_c_to_f, CeedOperator *op_coarse, CeedOperator *op_prolong,
-                                             CeedOperator *op_rstrict) {
+                                             CeedOperator *op_restrict) {
   Ceed      ceed;
   CeedInt   Q_f, Q_c;
   CeedBasis basis_fine, basis_c_to_f = NULL;
@@ -2302,7 +2427,7 @@ int CeedOperatorMultigridLevelCreateTensorH1(CeedOperator op_fine, CeedVector p_
   CeedCheck(Q_f == Q_c, ceed, CEED_ERROR_DIMENSION, "Bases must have compatible quadrature spaces");
 
   // Create coarse to fine basis, if required
-  if (op_prolong || op_rstrict) {
+  if (op_prolong || op_restrict) {
     CeedInt     dim, num_comp, num_nodes_c, P_1d_f, P_1d_c;
     CeedScalar *q_ref, *q_weight, *grad;
 
@@ -2324,7 +2449,7 @@ int CeedOperatorMultigridLevelCreateTensorH1(CeedOperator op_fine, CeedVector p_
   }
 
   // Core code
-  CeedCall(CeedSingleOperatorMultigridLevel(op_fine, p_mult_fine, rstr_coarse, basis_coarse, basis_c_to_f, op_coarse, op_prolong, op_rstrict));
+  CeedCall(CeedSingleOperatorMultigridLevel(op_fine, p_mult_fine, rstr_coarse, basis_coarse, basis_c_to_f, op_coarse, op_prolong, op_restrict));
   return CEED_ERROR_SUCCESS;
 }
 
@@ -2340,14 +2465,15 @@ int CeedOperatorMultigridLevelCreateTensorH1(CeedOperator op_fine, CeedVector p_
   @param[in]  interp_c_to_f Matrix for coarse to fine interpolation, or NULL if not creating prolongation/restriction operators
   @param[out] op_coarse     Coarse grid operator
   @param[out] op_prolong    Coarse to fine operator, or NULL
-  @param[out] op_rstrict   Fine to coarse operator, or NULL
+  @param[out] op_restrict   Fine to coarse operator, or NULL
 
   @return An error code: 0 - success, otherwise - failure
 
   @ref User
 **/
 int CeedOperatorMultigridLevelCreateH1(CeedOperator op_fine, CeedVector p_mult_fine, CeedElemRestriction rstr_coarse, CeedBasis basis_coarse,
-                                       const CeedScalar *interp_c_to_f, CeedOperator *op_coarse, CeedOperator *op_prolong, CeedOperator *op_rstrict) {
+                                       const CeedScalar *interp_c_to_f, CeedOperator *op_coarse, CeedOperator *op_prolong,
+                                       CeedOperator *op_restrict) {
   Ceed      ceed;
   CeedInt   Q_f, Q_c;
   CeedBasis basis_fine, basis_c_to_f = NULL;
@@ -2362,7 +2488,7 @@ int CeedOperatorMultigridLevelCreateH1(CeedOperator op_fine, CeedVector p_mult_f
   CeedCheck(Q_f == Q_c, ceed, CEED_ERROR_DIMENSION, "Bases must have compatible quadrature spaces");
 
   // Coarse to fine basis
-  if (op_prolong || op_rstrict) {
+  if (op_prolong || op_restrict) {
     CeedInt          dim, num_comp, num_nodes_c, num_nodes_f;
     CeedScalar      *q_ref, *q_weight, *grad;
     CeedElemTopology topo;
@@ -2385,7 +2511,7 @@ int CeedOperatorMultigridLevelCreateH1(CeedOperator op_fine, CeedVector p_mult_f
   }
 
   // Core code
-  CeedCall(CeedSingleOperatorMultigridLevel(op_fine, p_mult_fine, rstr_coarse, basis_coarse, basis_c_to_f, op_coarse, op_prolong, op_rstrict));
+  CeedCall(CeedSingleOperatorMultigridLevel(op_fine, p_mult_fine, rstr_coarse, basis_coarse, basis_c_to_f, op_coarse, op_prolong, op_restrict));
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/interface/ceed-qfunctioncontext.c b/interface/ceed-qfunctioncontext.c
index d0617e5a..8a22f599 100644
--- a/interface/ceed-qfunctioncontext.c
+++ b/interface/ceed-qfunctioncontext.c
@@ -832,7 +832,6 @@ int CeedQFunctionContextDestroy(CeedQFunctionContext *ctx) {
   CeedCall(CeedFree(&(*ctx)->field_labels));
   CeedCall(CeedDestroy(&(*ctx)->ceed));
   CeedCall(CeedFree(ctx));
-
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/interface/ceed-vector.c b/interface/ceed-vector.c
index 84b03c4f..6b2c93cb 100644
--- a/interface/ceed-vector.c
+++ b/interface/ceed-vector.c
@@ -850,7 +850,6 @@ int CeedVectorViewRange(CeedVector vec, CeedSize start, CeedSize stop, CeedInt s
   for (CeedSize i = start; step > 0 ? (i < stop) : (i > stop); i += step) fprintf(stream, fmt, x[i]);
   CeedCall(CeedVectorRestoreArrayRead(vec, &x));
   if (stop != vec->length) fprintf(stream, "  ...\n");
-
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/interface/ceed.c b/interface/ceed.c
index a40104cb..6ca34363 100644
--- a/interface/ceed.c
+++ b/interface/ceed.c
@@ -463,7 +463,7 @@ int CeedSetObjectDelegate(Ceed ceed, Ceed delegate, const char *obj_name) {
 
   @ref Backend
 **/
-int CeedGetOperatorfallback_resource(Ceed ceed, const char **resource) {
+int CeedGetOperatorFallbackResource(Ceed ceed, const char **resource) {
   *resource = (const char *)ceed->op_fallback_resource;
   return CEED_ERROR_SUCCESS;
 }
@@ -491,7 +491,7 @@ int CeedGetOperatorFallbackCeed(Ceed ceed, Ceed *fallback_ceed) {
     Ceed        fallback_ceed;
     const char *fallback_resource;
 
-    CeedCall(CeedGetOperatorfallback_resource(ceed, &fallback_resource));
+    CeedCall(CeedGetOperatorFallbackResource(ceed, &fallback_resource));
     CeedCall(CeedInit(fallback_resource, &fallback_ceed));
     fallback_ceed->op_fallback_parent = ceed;
     fallback_ceed->Error              = ceed->Error;
@@ -881,7 +881,6 @@ int CeedInit(const char *resource, Ceed *ceed) {
 
   // Backend specific setup
   CeedCall(backends[match_index].init(&resource[match_help], *ceed));
-
   return CEED_ERROR_SUCCESS;
 }
 
diff --git a/tests/README.md b/tests/README.md
index e8b4959e..5f591810 100644
--- a/tests/README.md
+++ b/tests/README.md
@@ -27,4 +27,5 @@ The tests are organized by API object, and some tests are further organized, as
     5.4. CeedOperator element inverse tests  
     5.5. CeedOperator multigrid level tests  
     5.6. CeedOperator full assembly tests  
-    5.7. CeedOperator H(div) and H(curl) tests
+    5.7. CeedOperator full assembly tests (extended)  
+    5.8. CeedOperator H(div) and H(curl) tests
diff --git a/tests/t509-operator.c b/tests/t509-operator.c
index 793f8d40..b43fc876 100644
--- a/tests/t509-operator.c
+++ b/tests/t509-operator.c
@@ -14,13 +14,13 @@ int main(int argc, char **argv) {
   CeedOperator        op_identity;
   CeedVector          u, v;
   CeedInt             num_elem = 15, p = 5, q = 8;
-  CeedInt             elem_size = p, num_nodes = num_elem * (p - 1) + 1;
+  CeedInt             num_nodes = num_elem * (p - 1) + 1;
   CeedInt             ind_u[num_elem * p];
 
   CeedInit(argv[1], &ceed);
 
   CeedVectorCreate(ceed, num_nodes, &u);
-  CeedVectorCreate(ceed, elem_size * num_elem, &v);
+  CeedVectorCreate(ceed, q * num_elem, &v);
 
   // Restrictions
   for (CeedInt i = 0; i < num_elem; i++) {
@@ -28,21 +28,20 @@ int main(int argc, char **argv) {
       ind_u[p * i + j] = i * (p - 1) + j;
     }
   }
-  CeedElemRestrictionCreate(ceed, num_elem, elem_size, 1, 1, num_nodes, CEED_MEM_HOST, CEED_USE_POINTER, ind_u, &elem_restriction_u);
+  CeedElemRestrictionCreate(ceed, num_elem, p, 1, 1, num_nodes, CEED_MEM_HOST, CEED_USE_POINTER, ind_u, &elem_restriction_u);
 
-  CeedInt strides_u_i[3] = {1, p, p};
-  CeedElemRestrictionCreateStrided(ceed, num_elem, elem_size, 1, elem_size * num_elem, strides_u_i, &elem_restriction_u_i);
+  CeedInt strides_u_i[3] = {1, q, q};
+  CeedElemRestrictionCreateStrided(ceed, num_elem, q, 1, q * num_elem, strides_u_i, &elem_restriction_u_i);
 
   // Bases
   CeedBasisCreateTensorH1Lagrange(ceed, 1, 1, p, q, CEED_GAUSS, &basis_u);
 
   // QFunction
-  CeedQFunctionCreateIdentity(ceed, 1, CEED_EVAL_NONE, CEED_EVAL_NONE, &qf_identity);
+  CeedQFunctionCreateIdentity(ceed, 1, CEED_EVAL_INTERP, CEED_EVAL_NONE, &qf_identity);
 
   // Operators
   CeedOperatorCreate(ceed, qf_identity, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_identity);
-  // -- Note: number of quadrature points for field set by elem_size of restriction when CEED_BASIS_NONE used
-  CeedOperatorSetField(op_identity, "input", elem_restriction_u, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
+  CeedOperatorSetField(op_identity, "input", elem_restriction_u, basis_u, CEED_VECTOR_ACTIVE);
   CeedOperatorSetField(op_identity, "output", elem_restriction_u_i, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
 
   CeedVectorSetValue(u, 3.0);
@@ -53,8 +52,8 @@ int main(int argc, char **argv) {
     const CeedScalar *v_array;
 
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    for (CeedInt i = 0; i < elem_size * num_elem; i++) {
-      if (fabs(v_array[i] - 3.) > 1e-14) printf("[%" CeedInt_FMT "] Computed Value: %f != True Value: 1.0\n", i, v_array[i]);
+    for (CeedInt i = 0; i < q * num_elem; i++) {
+      if (fabs(v_array[i] - 3.) > 100. * CEED_EPSILON) printf("[%" CeedInt_FMT "] Computed Value: %f != True Value: 3.0\n", i, v_array[i]);
     }
     CeedVectorRestoreArrayRead(v, &v_array);
   }
diff --git a/tests/t523-operator.c b/tests/t523-operator.c
index 178e46b5..b8823795 100644
--- a/tests/t523-operator.c
+++ b/tests/t523-operator.c
@@ -20,7 +20,7 @@
 
 int main(int argc, char **argv) {
   Ceed                ceed;
-  CeedElemRestriction elem_restriction_x_tet, elem_restriction_u_tet, elem_restr_qd_tet, elem_restriction_x_hex, elem_restriction_u_hex,
+  CeedElemRestriction elem_restriction_x_tet, elem_restriction_u_tet, elem_restr_q_data_tet, elem_restriction_x_hex, elem_restriction_u_hex,
       elem_restriction_q_data_hex;
   CeedBasis     basis_x_tet, basis_u_tet, basis_x_hex, basis_u_hex;
   CeedQFunction qf_setup_tet, qf_mass_tet, qf_setup_hex, qf_mass_hex;
@@ -69,7 +69,7 @@ int main(int argc, char **argv) {
   CeedElemRestrictionCreate(ceed, num_elem_tet, p_tet, 1, 1, num_dofs, CEED_MEM_HOST, CEED_USE_POINTER, ind_x_tet, &elem_restriction_u_tet);
 
   CeedInt strides_q_data_tet[3] = {1, q_tet, q_tet};
-  CeedElemRestrictionCreateStrided(ceed, num_elem_tet, q_tet, 1, num_qpts_tet, strides_q_data_tet, &elem_restr_qd_tet);
+  CeedElemRestrictionCreateStrided(ceed, num_elem_tet, q_tet, 1, num_qpts_tet, strides_q_data_tet, &elem_restr_q_data_tet);
 
   // -- Bases
   Build2DSimplex(q_ref, q_weight, interp, grad);
@@ -95,11 +95,11 @@ int main(int argc, char **argv) {
   CeedOperatorSetName(op_setup_tet, "triangle elements");
   CeedOperatorSetField(op_setup_tet, "weight", CEED_ELEMRESTRICTION_NONE, basis_x_tet, CEED_VECTOR_NONE);
   CeedOperatorSetField(op_setup_tet, "dx", elem_restriction_x_tet, basis_x_tet, CEED_VECTOR_ACTIVE);
-  CeedOperatorSetField(op_setup_tet, "rho", elem_restr_qd_tet, CEED_BASIS_NONE, q_data_tet);
+  CeedOperatorSetField(op_setup_tet, "rho", elem_restr_q_data_tet, CEED_BASIS_NONE, q_data_tet);
   // ---- Mass _tet
   CeedOperatorCreate(ceed, qf_mass_tet, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_mass_tet);
   CeedOperatorSetName(op_mass_tet, "triangle elements");
-  CeedOperatorSetField(op_mass_tet, "rho", elem_restr_qd_tet, CEED_BASIS_NONE, q_data_tet);
+  CeedOperatorSetField(op_mass_tet, "rho", elem_restr_q_data_tet, CEED_BASIS_NONE, q_data_tet);
   CeedOperatorSetField(op_mass_tet, "u", elem_restriction_u_tet, basis_u_tet, CEED_VECTOR_ACTIVE);
   CeedOperatorSetField(op_mass_tet, "v", elem_restriction_u_tet, basis_u_tet, CEED_VECTOR_ACTIVE);
 
@@ -173,7 +173,7 @@ int main(int argc, char **argv) {
   CeedVectorDestroy(&q_data_hex);
   CeedElemRestrictionDestroy(&elem_restriction_u_tet);
   CeedElemRestrictionDestroy(&elem_restriction_x_tet);
-  CeedElemRestrictionDestroy(&elem_restr_qd_tet);
+  CeedElemRestrictionDestroy(&elem_restr_q_data_tet);
   CeedElemRestrictionDestroy(&elem_restriction_u_hex);
   CeedElemRestrictionDestroy(&elem_restriction_x_hex);
   CeedElemRestrictionDestroy(&elem_restriction_q_data_hex);
diff --git a/tests/t539-operator.c b/tests/t539-operator.c
index d43f174c..c444a780 100644
--- a/tests/t539-operator.c
+++ b/tests/t539-operator.c
@@ -10,7 +10,7 @@
 
 int main(int argc, char **argv) {
   Ceed                ceed;
-  CeedElemRestriction elem_restriction_x, elem_restriction_u_0, elem_restriction_u_1, elem_restr_qd_mass, elem_restr_qd_diff;
+  CeedElemRestriction elem_restriction_x, elem_restriction_u_0, elem_restriction_u_1, elem_restr_q_data_mass, elem_restr_q_data_diff;
   CeedBasis           basis_x, basis_u_0, basis_u_1;
   CeedQFunction       qf_setup_mass, qf_setup_diff, qf_apply;
   CeedOperator        op_setup_mass, op_setup_diff, op_apply;
@@ -64,11 +64,11 @@ int main(int argc, char **argv) {
                             CEED_USE_POINTER, ind_u_1, &elem_restriction_u_1);
 
   CeedInt strides_q_data_mass[3] = {1, q * q, q * q};
-  CeedElemRestrictionCreateStrided(ceed, num_elem, q * q, 1, num_qpts, strides_q_data_mass, &elem_restr_qd_mass);
+  CeedElemRestrictionCreateStrided(ceed, num_elem, q * q, 1, num_qpts, strides_q_data_mass, &elem_restr_q_data_mass);
 
   CeedInt strides_q_data_diff[3] = {1, q * q, dim * (dim + 1) / 2 * q * q};
   CeedElemRestrictionCreateStrided(ceed, num_elem, q * q, dim * (dim + 1) / 2, dim * (dim + 1) / 2 * num_qpts, strides_q_data_diff,
-                                   &elem_restr_qd_diff);
+                                   &elem_restr_q_data_diff);
 
   // Bases
   CeedBasisCreateTensorH1Lagrange(ceed, dim, dim, p_0, q, CEED_GAUSS, &basis_x);
@@ -82,7 +82,7 @@ int main(int argc, char **argv) {
   CeedOperatorCreate(ceed, qf_setup_mass, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_setup_mass);
   CeedOperatorSetField(op_setup_mass, "dx", elem_restriction_x, basis_x, CEED_VECTOR_ACTIVE);
   CeedOperatorSetField(op_setup_mass, "weights", CEED_ELEMRESTRICTION_NONE, basis_x, CEED_VECTOR_NONE);
-  CeedOperatorSetField(op_setup_mass, "qdata", elem_restr_qd_mass, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
+  CeedOperatorSetField(op_setup_mass, "qdata", elem_restr_q_data_mass, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
 
   // QFunction - setup diffusion
   CeedQFunctionCreateInteriorByName(ceed, "Poisson2DBuild", &qf_setup_diff);
@@ -91,7 +91,7 @@ int main(int argc, char **argv) {
   CeedOperatorCreate(ceed, qf_setup_diff, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_setup_diff);
   CeedOperatorSetField(op_setup_diff, "dx", elem_restriction_x, basis_x, CEED_VECTOR_ACTIVE);
   CeedOperatorSetField(op_setup_diff, "weights", CEED_ELEMRESTRICTION_NONE, basis_x, CEED_VECTOR_NONE);
-  CeedOperatorSetField(op_setup_diff, "qdata", elem_restr_qd_diff, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
+  CeedOperatorSetField(op_setup_diff, "qdata", elem_restr_q_data_diff, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
 
   // Apply Setup Operators
   CeedOperatorApply(op_setup_mass, x, q_data_mass, CEED_REQUEST_IMMEDIATE);
@@ -111,8 +111,8 @@ int main(int argc, char **argv) {
   // Operator - apply
   CeedOperatorCreate(ceed, qf_apply, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_apply);
   CeedOperatorSetField(op_apply, "du_0", elem_restriction_u_0, basis_u_0, CEED_VECTOR_ACTIVE);
-  CeedOperatorSetField(op_apply, "mass qdata", elem_restr_qd_mass, CEED_BASIS_NONE, q_data_mass);
-  CeedOperatorSetField(op_apply, "diff qdata", elem_restr_qd_diff, CEED_BASIS_NONE, q_data_diff);
+  CeedOperatorSetField(op_apply, "mass qdata", elem_restr_q_data_mass, CEED_BASIS_NONE, q_data_mass);
+  CeedOperatorSetField(op_apply, "diff qdata", elem_restr_q_data_diff, CEED_BASIS_NONE, q_data_diff);
   CeedOperatorSetField(op_apply, "u_0", elem_restriction_u_0, basis_u_0, CEED_VECTOR_ACTIVE);
   CeedOperatorSetField(op_apply, "u_1", elem_restriction_u_1, basis_u_1, CEED_VECTOR_ACTIVE);
   CeedOperatorSetField(op_apply, "v_0", elem_restriction_u_0, basis_u_0, CEED_VECTOR_ACTIVE);
@@ -170,8 +170,8 @@ int main(int argc, char **argv) {
   CeedElemRestrictionDestroy(&elem_restriction_x);
   CeedElemRestrictionDestroy(&elem_restriction_u_0);
   CeedElemRestrictionDestroy(&elem_restriction_u_1);
-  CeedElemRestrictionDestroy(&elem_restr_qd_mass);
-  CeedElemRestrictionDestroy(&elem_restr_qd_diff);
+  CeedElemRestrictionDestroy(&elem_restr_q_data_mass);
+  CeedElemRestrictionDestroy(&elem_restr_q_data_diff);
   CeedBasisDestroy(&basis_x);
   CeedBasisDestroy(&basis_u_0);
   CeedBasisDestroy(&basis_u_1);
diff --git a/tests/t560-operator.c b/tests/t560-operator.c
index 240c435f..3298b5fd 100644
--- a/tests/t560-operator.c
+++ b/tests/t560-operator.c
@@ -112,31 +112,31 @@ int main(int argc, char **argv) {
 
   // Manually assemble operator
   CeedVectorSetValue(u, 0.0);
-  for (CeedInt i = 0; i < num_dofs; i++) {
+  for (CeedInt j = 0; j < num_dofs; j++) {
     CeedScalar       *u_array;
     const CeedScalar *v_array;
 
     // Set input
     CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
-    u_array[i] = 1.0;
-    if (i) u_array[i - 1] = 0.0;
+    u_array[j] = 1.0;
+    if (j) u_array[j - 1] = 0.0;
     CeedVectorRestoreArray(u, &u_array);
 
-    // Compute entries for column i
+    // Compute entries for column j
     CeedOperatorApply(op_mass, u, v, CEED_REQUEST_IMMEDIATE);
 
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    for (CeedInt k = 0; k < num_dofs; k++) assembled_true[i * num_dofs + k] = v_array[k];
+    for (CeedInt i = 0; i < num_dofs; i++) assembled_true[i * num_dofs + j] = v_array[i];
     CeedVectorRestoreArrayRead(v, &v_array);
   }
 
   // Check output
   for (CeedInt i = 0; i < num_dofs; i++) {
     for (CeedInt j = 0; j < num_dofs; j++) {
-      if (fabs(assembled_values[j * num_dofs + i] - assembled_true[j * num_dofs + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values[i * num_dofs + j] - assembled_true[i * num_dofs + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[j * num_dofs + i],
-               assembled_true[j * num_dofs + i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_dofs + j],
+               assembled_true[i * num_dofs + j]);
         // LCOV_EXCL_STOP
       }
     }
diff --git a/tests/t561-operator.c b/tests/t561-operator.c
index e2aa1ee6..6f1dd0bd 100644
--- a/tests/t561-operator.c
+++ b/tests/t561-operator.c
@@ -113,31 +113,31 @@ int main(int argc, char **argv) {
 
   // Manually assemble operator
   CeedVectorSetValue(u, 0.0);
-  for (CeedInt i = 0; i < num_dofs; i++) {
+  for (CeedInt j = 0; j < num_dofs; j++) {
     CeedScalar       *u_array;
     const CeedScalar *v_array;
 
     // Set input
     CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
-    u_array[i] = 1.0;
-    if (i) u_array[i - 1] = 0.0;
+    u_array[j] = 1.0;
+    if (j) u_array[j - 1] = 0.0;
     CeedVectorRestoreArray(u, &u_array);
 
-    // Compute entries for column i
+    // Compute entries for column j
     CeedOperatorApply(op_diff, u, v, CEED_REQUEST_IMMEDIATE);
 
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    for (CeedInt k = 0; k < num_dofs; k++) assembled_true[i * num_dofs + k] = v_array[k];
+    for (CeedInt i = 0; i < num_dofs; i++) assembled_true[i * num_dofs + j] = v_array[i];
     CeedVectorRestoreArrayRead(v, &v_array);
   }
 
   // Check output
   for (CeedInt i = 0; i < num_dofs; i++) {
     for (CeedInt j = 0; j < num_dofs; j++) {
-      if (fabs(assembled_values[j * num_dofs + i] - assembled_true[j * num_dofs + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values[i * num_dofs + j] - assembled_true[i * num_dofs + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[j * num_dofs + i],
-               assembled_true[j * num_dofs + i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_dofs + j],
+               assembled_true[i * num_dofs + j]);
         // LCOV_EXCL_STOP
       }
     }
diff --git a/tests/t562-operator.c b/tests/t562-operator.c
index d02a7823..d74c8efb 100644
--- a/tests/t562-operator.c
+++ b/tests/t562-operator.c
@@ -137,31 +137,31 @@ int main(int argc, char **argv) {
 
   // Manually assemble operator
   CeedVectorSetValue(u, 0.0);
-  for (CeedInt i = 0; i < num_dofs; i++) {
-    // Set input
+  for (CeedInt j = 0; j < num_dofs; j++) {
     CeedScalar       *u_array;
     const CeedScalar *v_array;
 
+    // Set input
     CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
-    u_array[i] = 1.0;
-    if (i) u_array[i - 1] = 0.0;
+    u_array[j] = 1.0;
+    if (j) u_array[j - 1] = 0.0;
     CeedVectorRestoreArray(u, &u_array);
 
-    // Compute entries for column i
+    // Compute entries for column j
     CeedOperatorApply(op_apply, u, v, CEED_REQUEST_IMMEDIATE);
 
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    for (CeedInt k = 0; k < num_dofs; k++) assembled_true[i * num_dofs + k] = v_array[k];
+    for (CeedInt i = 0; i < num_dofs; i++) assembled_true[i * num_dofs + j] = v_array[i];
     CeedVectorRestoreArrayRead(v, &v_array);
   }
 
   // Check output
   for (CeedInt i = 0; i < num_dofs; i++) {
     for (CeedInt j = 0; j < num_dofs; j++) {
-      if (fabs(assembled_values[j * num_dofs + i] - assembled_true[j * num_dofs + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values[i * num_dofs + j] - assembled_true[i * num_dofs + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[j * num_dofs + i],
-               assembled_true[j * num_dofs + i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_dofs + j],
+               assembled_true[i * num_dofs + j]);
         // LCOV_EXCL_STOP
       }
     }
diff --git a/tests/t563-operator.c b/tests/t563-operator.c
index 27d32ba8..93eaf0f4 100644
--- a/tests/t563-operator.c
+++ b/tests/t563-operator.c
@@ -149,31 +149,31 @@ int main(int argc, char **argv) {
 
   // Manually assemble operator
   CeedVectorSetValue(u, 0.0);
-  for (CeedInt i = 0; i < num_dofs; i++) {
+  for (CeedInt j = 0; j < num_dofs; j++) {
     CeedScalar       *u_array;
     const CeedScalar *v_array;
 
     // Set input
     CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
-    u_array[i] = 1.0;
-    if (i) u_array[i - 1] = 0.0;
+    u_array[j] = 1.0;
+    if (j) u_array[j - 1] = 0.0;
     CeedVectorRestoreArray(u, &u_array);
 
-    // Compute entries for column i
+    // Compute entries for column j
     CeedOperatorApply(op_apply, u, v, CEED_REQUEST_IMMEDIATE);
 
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    for (CeedInt k = 0; k < num_dofs; k++) assembled_true[i * num_dofs + k] = v_array[k];
+    for (CeedInt i = 0; i < num_dofs; i++) assembled_true[i * num_dofs + j] = v_array[i];
     CeedVectorRestoreArrayRead(v, &v_array);
   }
 
   // Check output
   for (CeedInt i = 0; i < num_dofs; i++) {
     for (CeedInt j = 0; j < num_dofs; j++) {
-      if (fabs(assembled_values[j * num_dofs + i] - assembled_true[j * num_dofs + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values[i * num_dofs + j] - assembled_true[i * num_dofs + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[j * num_dofs + i],
-               assembled_true[j * num_dofs + i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_dofs + j],
+               assembled_true[i * num_dofs + j]);
         // LCOV_EXCL_STOP
       }
     }
diff --git a/tests/t564-operator.c b/tests/t564-operator.c
index 10562794..d9be2dbe 100644
--- a/tests/t564-operator.c
+++ b/tests/t564-operator.c
@@ -1,6 +1,6 @@
 /// @file
-/// Test assembly of mass matrix operator (multi-component) (see t537)
-/// \test Test assembly of mass matrix operator (multi-component)
+/// Test full assembly of mass matrix operator (multi-component) (see t537)
+/// \test Test full assembly of mass matrix operator (multi-component)
 #include <ceed.h>
 #include <math.h>
 #include <stdio.h>
@@ -125,21 +125,21 @@ int main(int argc, char **argv) {
     old_index = ind;
     CeedVectorRestoreArray(u, &u_array);
 
-    // Compute effect of DoF j
+    // Compute effect of DoF ind
     CeedOperatorApply(op_mass, u, v, CEED_REQUEST_IMMEDIATE);
 
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    for (CeedInt k = 0; k < num_dofs * num_comp; k++) assembled_true[j * num_dofs * num_comp + k] = v_array[k];
+    for (CeedInt i = 0; i < num_dofs * num_comp; i++) assembled_true[i * num_dofs * num_comp + j] = v_array[i];
     CeedVectorRestoreArrayRead(v, &v_array);
   }
 
   // Check output
   for (CeedInt i = 0; i < num_comp * num_dofs; i++) {
     for (CeedInt j = 0; j < num_comp * num_dofs; j++) {
-      if (fabs(assembled_values[j * num_dofs * num_comp + i] - assembled_true[j * num_dofs * num_comp + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values[i * num_dofs * num_comp + j] - assembled_true[i * num_dofs * num_comp + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[j * num_dofs * num_comp + i],
-               assembled_true[j * num_dofs * num_comp + i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_dofs * num_comp + j],
+               assembled_true[i * num_dofs * num_comp + j]);
         // LCOV_EXCL_STOP
       }
     }
diff --git a/tests/t565-operator.c b/tests/t565-operator.c
index 6f63bcc8..b5a54245 100644
--- a/tests/t565-operator.c
+++ b/tests/t565-operator.c
@@ -132,33 +132,33 @@ int main(int argc, char **argv) {
     CeedVectorRestoreArrayRead(assembled, &assembled_array);
   }
 
-  // Manually assemble diagonal
+  // Manually assemble operator
   CeedVectorSetValue(u, 0.0);
-  for (CeedInt i = 0; i < num_dofs; i++) {
+  for (CeedInt j = 0; j < num_dofs; j++) {
     CeedScalar       *u_array;
     const CeedScalar *v_array;
 
     // Set input
     CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
-    u_array[i] = 1.0;
-    if (i) u_array[i - 1] = 0.0;
+    u_array[j] = 1.0;
+    if (j) u_array[j - 1] = 0.0;
     CeedVectorRestoreArray(u, &u_array);
 
-    // Compute entries for column i
+    // Compute entries for column j
     CeedOperatorApply(op_apply, u, v, CEED_REQUEST_IMMEDIATE);
 
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    for (CeedInt k = 0; k < num_dofs; k++) assembled_true[i * num_dofs + k] = v_array[k];
+    for (CeedInt i = 0; i < num_dofs; i++) assembled_true[i * num_dofs + j] = v_array[i];
     CeedVectorRestoreArrayRead(v, &v_array);
   }
 
   // Check output
   for (CeedInt i = 0; i < num_dofs; i++) {
     for (CeedInt j = 0; j < num_dofs; j++) {
-      if (fabs(assembled_values[j * num_dofs + i] - assembled_true[j * num_dofs + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values[i * num_dofs + j] - assembled_true[i * num_dofs + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[j * num_dofs + i],
-               assembled_true[j * num_dofs + i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_dofs + j],
+               assembled_true[i * num_dofs + j]);
         // LCOV_EXCL_STOP
       }
     }
diff --git a/tests/t566-operator.c b/tests/t566-operator.c
index fd043cfa..5ee5908b 100644
--- a/tests/t566-operator.c
+++ b/tests/t566-operator.c
@@ -1,6 +1,6 @@
 /// @file
-/// Test assembly of non-symmetric mass matrix operator (multi-component) (see t537)
-/// \test Test assembly of non-symmetric mass matrix operator (multi-component)
+/// Test full assembly of non-symmetric mass matrix operator (multi-component) (see t537)
+/// \test Test full assembly of non-symmetric mass matrix operator (multi-component)
 #include "t566-operator.h"
 
 #include <ceed.h>
@@ -127,11 +127,11 @@ int main(int argc, char **argv) {
       old_index = ind;
       CeedVectorRestoreArray(u, &u_array);
 
-      // Compute effect of DoF j
+      // Compute effect of DoF ind
       CeedOperatorApply(op_mass, u, v, CEED_REQUEST_IMMEDIATE);
 
       CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-      for (CeedInt k = 0; k < num_dofs * num_comp; k++) assembled_true[k * num_dofs * num_comp + ind] = v_array[k];
+      for (CeedInt i = 0; i < num_dofs * num_comp; i++) assembled_true[i * num_dofs * num_comp + ind] = v_array[i];
       CeedVectorRestoreArrayRead(v, &v_array);
     }
   }
diff --git a/tests/t567-operator.c b/tests/t567-operator.c
index 93264df8..021bbdf7 100644
--- a/tests/t567-operator.c
+++ b/tests/t567-operator.c
@@ -1,6 +1,6 @@
 /// @file
-/// Test assembly of non-symmetric Poisson operator (multi-component)
-/// \test Test assembly of non-symmetric Poisson operator (multi-component)
+/// Test full assembly of non-symmetric Poisson operator (multi-component)
+/// \test Test full assembly of non-symmetric Poisson operator (multi-component)
 #include "t567-operator.h"
 
 #include <ceed.h>
@@ -127,11 +127,11 @@ int main(int argc, char **argv) {
       old_index = ind;
       CeedVectorRestoreArray(u, &u_array);
 
-      // Compute effect of DoF j
+      // Compute effect of DoF ind
       CeedOperatorApply(op_diff, u, v, CEED_REQUEST_IMMEDIATE);
 
       CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-      for (CeedInt k = 0; k < num_dofs * num_comp; k++) assembled_true[k * num_dofs * num_comp + ind] = v_array[k];
+      for (CeedInt i = 0; i < num_dofs * num_comp; i++) assembled_true[i * num_dofs * num_comp + ind] = v_array[i];
       CeedVectorRestoreArrayRead(v, &v_array);
     }
   }
diff --git a/tests/t568-operator.c b/tests/t568-operator.c
index 703a0ef5..5f9057c2 100644
--- a/tests/t568-operator.c
+++ b/tests/t568-operator.c
@@ -1,6 +1,6 @@
 /// @file
-/// Test assembly of Poisson operator with extra input field (non-square D)
-/// \test Test assembly of Poisson operator with extra input field (non-square D)
+/// Test full assembly of Poisson operator with extra input field (non-square D)
+/// \test Test full assembly of Poisson operator with extra input field (non-square D)
 #include "t568-operator.h"
 
 #include <ceed.h>
@@ -130,11 +130,11 @@ int main(int argc, char **argv) {
       old_index = ind;
       CeedVectorRestoreArray(u, &u_array);
 
-      // Compute effect of DoF j
+      // Compute effect of DoF ind
       CeedOperatorApply(op_diff, u, v, CEED_REQUEST_IMMEDIATE);
 
       CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-      for (CeedInt k = 0; k < num_dofs * num_comp; k++) assembled_true[k * num_dofs * num_comp + ind] = v_array[k];
+      for (CeedInt i = 0; i < num_dofs * num_comp; i++) assembled_true[i * num_dofs * num_comp + ind] = v_array[i];
       CeedVectorRestoreArrayRead(v, &v_array);
     }
   }
diff --git a/tests/t569-operator.c b/tests/t569-operator.c
new file mode 100644
index 00000000..c19ca872
--- /dev/null
+++ b/tests/t569-operator.c
@@ -0,0 +1,159 @@
+/// @file
+/// Test full assembly of a non-square mass matrix operator (see t553)
+/// \test Test full assembly of a non-square mass matrix operator
+#include <ceed.h>
+#include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+int main(int argc, char **argv) {
+  Ceed                ceed;
+  CeedElemRestriction elem_restriction_x, elem_restriction_q_data, elem_restriction_u_coarse, elem_restriction_u_fine;
+  CeedBasis           basis_x, basis_u_coarse, basis_u_fine;
+  CeedQFunction       qf_setup, qf_mass;
+  CeedOperator        op_setup, op_mass;
+  CeedVector          q_data, x, u, v;
+  CeedInt             num_elem = 15, p_coarse = 3, p_fine = 5, q = 8;
+  CeedInt             num_dofs_x = num_elem + 1, num_dofs_u_coarse = num_elem * (p_coarse - 1) + 1, num_dofs_u_fine = num_elem * (p_fine - 1) + 1;
+  CeedInt             ind_u_coarse[num_elem * p_coarse], ind_u_fine[num_elem * p_fine], ind_x[num_elem * 2];
+  CeedScalar          assembled_values[num_dofs_u_coarse * num_dofs_u_fine];
+  CeedScalar          assembled_true[num_dofs_u_coarse * num_dofs_u_fine];
+
+  CeedInit(argv[1], &ceed);
+
+  CeedVectorCreate(ceed, num_dofs_x, &x);
+  {
+    CeedScalar x_array[num_dofs_x];
+
+    for (CeedInt i = 0; i < num_dofs_x; i++) x_array[i] = (CeedScalar)i / (num_dofs_x - 1);
+    CeedVectorSetArray(x, CEED_MEM_HOST, CEED_COPY_VALUES, x_array);
+  }
+  CeedVectorCreate(ceed, num_dofs_u_coarse, &u);
+  CeedVectorCreate(ceed, num_dofs_u_fine, &v);
+  CeedVectorCreate(ceed, num_elem * q, &q_data);
+
+  // Restrictions
+  for (CeedInt i = 0; i < num_elem; i++) {
+    ind_x[2 * i + 0] = i;
+    ind_x[2 * i + 1] = i + 1;
+  }
+  CeedElemRestrictionCreate(ceed, num_elem, 2, 1, 1, num_dofs_x, CEED_MEM_HOST, CEED_USE_POINTER, ind_x, &elem_restriction_x);
+
+  for (CeedInt i = 0; i < num_elem; i++) {
+    for (CeedInt j = 0; j < p_coarse; j++) {
+      ind_u_coarse[p_coarse * i + j] = i * (p_coarse - 1) + j;
+    }
+  }
+  CeedElemRestrictionCreate(ceed, num_elem, p_coarse, 1, 1, num_dofs_u_coarse, CEED_MEM_HOST, CEED_USE_POINTER, ind_u_coarse,
+                            &elem_restriction_u_coarse);
+
+  for (CeedInt i = 0; i < num_elem; i++) {
+    for (CeedInt j = 0; j < p_fine; j++) {
+      ind_u_fine[p_fine * i + j] = i * (p_fine - 1) + j;
+    }
+  }
+  CeedElemRestrictionCreate(ceed, num_elem, p_fine, 1, 1, num_dofs_u_fine, CEED_MEM_HOST, CEED_USE_POINTER, ind_u_fine, &elem_restriction_u_fine);
+
+  CeedInt strides_q_data[3] = {1, q, q};
+  CeedElemRestrictionCreateStrided(ceed, num_elem, q, 1, q * num_elem, strides_q_data, &elem_restriction_q_data);
+
+  // Bases
+  CeedBasisCreateTensorH1Lagrange(ceed, 1, 1, 2, q, CEED_GAUSS, &basis_x);
+  CeedBasisCreateTensorH1Lagrange(ceed, 1, 1, p_coarse, q, CEED_GAUSS, &basis_u_coarse);
+  CeedBasisCreateTensorH1Lagrange(ceed, 1, 1, p_fine, q, CEED_GAUSS, &basis_u_fine);
+
+  // QFunctions
+  CeedQFunctionCreateInteriorByName(ceed, "Mass1DBuild", &qf_setup);
+  CeedQFunctionCreateInteriorByName(ceed, "MassApply", &qf_mass);
+
+  // Operators
+  CeedOperatorCreate(ceed, qf_setup, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_setup);
+
+  CeedOperatorSetField(op_setup, "weights", CEED_ELEMRESTRICTION_NONE, basis_x, CEED_VECTOR_NONE);
+  CeedOperatorSetField(op_setup, "dx", elem_restriction_x, basis_x, CEED_VECTOR_ACTIVE);
+  CeedOperatorSetField(op_setup, "qdata", elem_restriction_q_data, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
+
+  CeedOperatorCreate(ceed, qf_mass, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_mass);
+  CeedOperatorSetField(op_mass, "qdata", elem_restriction_q_data, CEED_BASIS_NONE, q_data);
+  CeedOperatorSetField(op_mass, "u", elem_restriction_u_coarse, basis_u_coarse, CEED_VECTOR_ACTIVE);
+  CeedOperatorSetField(op_mass, "v", elem_restriction_u_fine, basis_u_fine, CEED_VECTOR_ACTIVE);
+
+  CeedOperatorApply(op_setup, x, q_data, CEED_REQUEST_IMMEDIATE);
+
+  // Fully assemble operator
+  CeedSize   num_entries;
+  CeedInt   *rows;
+  CeedInt   *cols;
+  CeedVector assembled;
+
+  for (CeedInt k = 0; k < num_dofs_u_coarse * num_dofs_u_fine; ++k) {
+    assembled_values[k] = 0.0;
+    assembled_true[k]   = 0.0;
+  }
+  CeedOperatorLinearAssembleSymbolic(op_mass, &num_entries, &rows, &cols);
+  CeedVectorCreate(ceed, num_entries, &assembled);
+  CeedOperatorLinearAssemble(op_mass, assembled);
+  {
+    const CeedScalar *assembled_array;
+
+    CeedVectorGetArrayRead(assembled, CEED_MEM_HOST, &assembled_array);
+    for (CeedInt k = 0; k < num_entries; ++k) {
+      assembled_values[rows[k] * num_dofs_u_coarse + cols[k]] += assembled_array[k];
+    }
+    CeedVectorRestoreArrayRead(assembled, &assembled_array);
+  }
+
+  // Manually assemble operator
+  CeedVectorSetValue(u, 0.0);
+  for (CeedInt j = 0; j < num_dofs_u_coarse; j++) {
+    CeedScalar       *u_array;
+    const CeedScalar *v_array;
+
+    // Set input
+    CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
+    u_array[j] = 1.0;
+    if (j) u_array[j - 1] = 0.0;
+    CeedVectorRestoreArray(u, &u_array);
+
+    // Compute entries for column j
+    CeedOperatorApply(op_mass, u, v, CEED_REQUEST_IMMEDIATE);
+
+    CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
+    for (CeedInt i = 0; i < num_dofs_u_fine; i++) assembled_true[i * num_dofs_u_coarse + j] = v_array[i];
+    CeedVectorRestoreArrayRead(v, &v_array);
+  }
+
+  // Check output
+  for (CeedInt i = 0; i < num_dofs_u_fine; i++) {
+    for (CeedInt j = 0; j < num_dofs_u_coarse; j++) {
+      if (fabs(assembled_values[i * num_dofs_u_coarse + j] - assembled_true[i * num_dofs_u_coarse + j]) > 100. * CEED_EPSILON) {
+        // LCOV_EXCL_START
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_dofs_u_coarse + j],
+               assembled_true[i * num_dofs_u_coarse + j]);
+        // LCOV_EXCL_STOP
+      }
+    }
+  }
+
+  // Cleanup
+  free(rows);
+  free(cols);
+  CeedVectorDestroy(&x);
+  CeedVectorDestroy(&q_data);
+  CeedVectorDestroy(&u);
+  CeedVectorDestroy(&v);
+  CeedVectorDestroy(&assembled);
+  CeedElemRestrictionDestroy(&elem_restriction_u_coarse);
+  CeedElemRestrictionDestroy(&elem_restriction_u_fine);
+  CeedElemRestrictionDestroy(&elem_restriction_x);
+  CeedElemRestrictionDestroy(&elem_restriction_q_data);
+  CeedBasisDestroy(&basis_u_coarse);
+  CeedBasisDestroy(&basis_u_fine);
+  CeedBasisDestroy(&basis_x);
+  CeedQFunctionDestroy(&qf_setup);
+  CeedQFunctionDestroy(&qf_mass);
+  CeedOperatorDestroy(&op_setup);
+  CeedOperatorDestroy(&op_mass);
+  CeedDestroy(&ceed);
+  return 0;
+}
diff --git a/tests/t570-operator.c b/tests/t570-operator.c
index 3da2208b..c627c0f5 100644
--- a/tests/t570-operator.c
+++ b/tests/t570-operator.c
@@ -1,147 +1,117 @@
 /// @file
-/// Test assembly of H(div) mass matrix operator diagonal
-/// \test Test assembly of H(div) mass matrix operator diagonal
-#include "t570-operator.h"
-
+/// Test full assembly of an identity operator (see t509)
+/// \test Test full assembly of an identity operator
 #include <ceed.h>
 #include <math.h>
 #include <stdio.h>
 #include <stdlib.h>
 
-#include "t330-basis.h"
-
 int main(int argc, char **argv) {
   Ceed                ceed;
-  CeedElemRestriction elem_restriction_x, elem_restriction_u;
-  CeedBasis           basis_x, basis_u;
-  CeedQFunction       qf_mass;
-  CeedOperator        op_mass;
-  CeedVector          x, assembled, u, v;
-  CeedInt             dim = 2, p = 8, q = 3, px = 2;
-  CeedInt             n_x = 1, n_y = 1;  // Currently only implemented for single element
-  CeedInt             row, column, offset;
-  CeedInt             num_elem = n_x * n_y, num_faces = (n_x + 1) * n_y + (n_y + 1) * n_x;
-  CeedInt             num_dofs_x = (n_x + 1) * (n_y + 1), num_dofs_u = num_faces * 2, num_qpts = q * q;
-  CeedInt             ind_x[num_elem * px * px], ind_u[num_elem * p];
-  bool                orient_u[num_elem * p];
-  CeedScalar          assembled_true[num_dofs_u];
-  CeedScalar          q_ref[dim * num_qpts], q_weight[num_qpts];
-  CeedScalar          interp[dim * p * num_qpts], div[p * num_qpts];
+  CeedElemRestriction elem_restriction_u, elem_restriction_u_i;
+  CeedBasis           basis_u;
+  CeedQFunction       qf_identity;
+  CeedOperator        op_identity;
+  CeedVector          u, v;
+  CeedInt             num_elem = 15, p = 5, q = 8;
+  CeedInt             num_nodes = num_elem * (p - 1) + 1;
+  CeedInt             ind_u[num_elem * p];
+  CeedScalar          assembled_values[num_nodes * q * num_elem];
+  CeedScalar          assembled_true[num_nodes * q * num_elem];
 
   CeedInit(argv[1], &ceed);
 
-  // Vectors
-  CeedVectorCreate(ceed, dim * num_dofs_x, &x);
-  {
-    CeedScalar x_array[dim * num_dofs_x];
-
-    for (CeedInt i = 0; i < n_x + 1; i++) {
-      for (CeedInt j = 0; j < n_y + 1; j++) {
-        x_array[i + j * (n_x + 1) + 0 * num_dofs_x] = i / (CeedScalar)n_x;
-        x_array[i + j * (n_x + 1) + 1 * num_dofs_x] = j / (CeedScalar)n_y;
-      }
-    }
-    CeedVectorSetArray(x, CEED_MEM_HOST, CEED_COPY_VALUES, x_array);
-  }
-  CeedVectorCreate(ceed, num_dofs_u, &u);
-  CeedVectorCreate(ceed, num_dofs_u, &v);
+  CeedVectorCreate(ceed, num_nodes, &u);
+  CeedVectorCreate(ceed, q * num_elem, &v);
 
   // Restrictions
   for (CeedInt i = 0; i < num_elem; i++) {
-    column = i % n_x;
-    row    = i / n_x;
-    offset = column * (px - 1) + row * (n_x + 1) * (px - 1);
-
-    for (CeedInt j = 0; j < px; j++) {
-      for (CeedInt k = 0; k < px; k++) {
-        ind_x[px * (px * i + k) + j] = offset + k * (n_x + 1) + j;
-      }
+    for (CeedInt j = 0; j < p; j++) {
+      ind_u[p * i + j] = i * (p - 1) + j;
     }
   }
-  bool    orient_u_local[8] = {false, false, false, false, true, true, true, true};
-  CeedInt ind_u_local[8]    = {0, 1, 6, 7, 2, 3, 4, 5};
-  for (CeedInt j = 0; j < n_y; j++) {
-    for (CeedInt i = 0; i < n_x; i++) {
-      for (CeedInt k = 0; k < p; k++) {
-        ind_u[k]    = ind_u_local[k];
-        orient_u[k] = orient_u_local[k];
-      }
-    }
-  }
-  CeedElemRestrictionCreate(ceed, num_elem, px * px, dim, num_dofs_x, dim * num_dofs_x, CEED_MEM_HOST, CEED_USE_POINTER, ind_x, &elem_restriction_x);
-  CeedElemRestrictionCreateOriented(ceed, num_elem, p, 1, 1, num_dofs_u, CEED_MEM_HOST, CEED_COPY_VALUES, ind_u, orient_u, &elem_restriction_u);
+  CeedElemRestrictionCreate(ceed, num_elem, p, 1, 1, num_nodes, CEED_MEM_HOST, CEED_USE_POINTER, ind_u, &elem_restriction_u);
 
-  // Bases
-  CeedBasisCreateTensorH1Lagrange(ceed, dim, dim, px, q, CEED_GAUSS, &basis_x);
+  CeedInt strides_u_i[3] = {1, q, q};
+  CeedElemRestrictionCreateStrided(ceed, num_elem, q, 1, q * num_elem, strides_u_i, &elem_restriction_u_i);
 
-  BuildHdivQuadrilateral(q, q_ref, q_weight, interp, div, CEED_GAUSS);
-  CeedBasisCreateHdiv(ceed, CEED_TOPOLOGY_QUAD, 1, p, num_qpts, interp, div, q_ref, q_weight, &basis_u);
+  // Bases
+  CeedBasisCreateTensorH1Lagrange(ceed, 1, 1, p, q, CEED_GAUSS, &basis_u);
 
-  // QFunctions
-  CeedQFunctionCreateInterior(ceed, 1, mass, mass_loc, &qf_mass);
-  CeedQFunctionAddInput(qf_mass, "weight", 1, CEED_EVAL_WEIGHT);
-  CeedQFunctionAddInput(qf_mass, "dx", dim * dim, CEED_EVAL_GRAD);
-  CeedQFunctionAddInput(qf_mass, "u", dim, CEED_EVAL_INTERP);
-  CeedQFunctionAddOutput(qf_mass, "v", dim, CEED_EVAL_INTERP);
+  // QFunction
+  CeedQFunctionCreateIdentity(ceed, 1, CEED_EVAL_INTERP, CEED_EVAL_NONE, &qf_identity);
 
   // Operators
-  CeedOperatorCreate(ceed, qf_mass, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_mass);
-  CeedOperatorSetField(op_mass, "weight", CEED_ELEMRESTRICTION_NONE, basis_x, CEED_VECTOR_NONE);
-  CeedOperatorSetField(op_mass, "dx", elem_restriction_x, basis_x, x);
-  CeedOperatorSetField(op_mass, "u", elem_restriction_u, basis_u, CEED_VECTOR_ACTIVE);
-  CeedOperatorSetField(op_mass, "v", elem_restriction_u, basis_u, CEED_VECTOR_ACTIVE);
+  CeedOperatorCreate(ceed, qf_identity, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_identity);
+  CeedOperatorSetField(op_identity, "input", elem_restriction_u, basis_u, CEED_VECTOR_ACTIVE);
+  CeedOperatorSetField(op_identity, "output", elem_restriction_u_i, CEED_BASIS_NONE, CEED_VECTOR_ACTIVE);
+
+  // Fully assemble operator
+  CeedSize   num_entries;
+  CeedInt   *rows;
+  CeedInt   *cols;
+  CeedVector assembled;
+
+  for (CeedInt k = 0; k < num_nodes * q * num_elem; ++k) {
+    assembled_values[k] = 0.0;
+    assembled_true[k]   = 0.0;
+  }
+  CeedOperatorLinearAssembleSymbolic(op_identity, &num_entries, &rows, &cols);
+  CeedVectorCreate(ceed, num_entries, &assembled);
+  CeedOperatorLinearAssemble(op_identity, assembled);
+  {
+    const CeedScalar *assembled_array;
 
-  // Assemble diagonal
-  CeedVectorCreate(ceed, num_dofs_u, &assembled);
-  CeedOperatorLinearAssembleDiagonal(op_mass, assembled, CEED_REQUEST_IMMEDIATE);
+    CeedVectorGetArrayRead(assembled, CEED_MEM_HOST, &assembled_array);
+    for (CeedInt k = 0; k < num_entries; ++k) {
+      assembled_values[rows[k] * num_nodes + cols[k]] += assembled_array[k];
+    }
+    CeedVectorRestoreArrayRead(assembled, &assembled_array);
+  }
 
-  // Manually assemble diagonal
+  // Manually assemble operator
   CeedVectorSetValue(u, 0.0);
-  for (int i = 0; i < num_dofs_u; i++) {
+  for (CeedInt j = 0; j < num_nodes; j++) {
     CeedScalar       *u_array;
     const CeedScalar *v_array;
 
     // Set input
     CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
-    u_array[i] = 1.0;
-    if (i) u_array[i - 1] = 0.0;
+    u_array[j] = 1.0;
+    if (j) u_array[j - 1] = 0.0;
     CeedVectorRestoreArray(u, &u_array);
 
-    // Compute diag entry for DoF i
-    CeedOperatorApply(op_mass, u, v, CEED_REQUEST_IMMEDIATE);
+    // Compute entries for column j
+    CeedOperatorApply(op_identity, u, v, CEED_REQUEST_IMMEDIATE);
 
-    // Retrieve entry
     CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
-    assembled_true[i] = v_array[i];
+    for (CeedInt i = 0; i < q * num_elem; i++) assembled_true[i * num_nodes + j] = v_array[i];
     CeedVectorRestoreArrayRead(v, &v_array);
   }
 
   // Check output
-  {
-    const CeedScalar *assembled_array;
-
-    CeedVectorGetArrayRead(assembled, CEED_MEM_HOST, &assembled_array);
-    for (int i = 0; i < num_dofs_u; i++) {
-      if (fabs(assembled_array[i] - assembled_true[i]) > 100. * CEED_EPSILON) {
+  for (CeedInt i = 0; i < q * num_elem; i++) {
+    for (CeedInt j = 0; j < num_nodes; j++) {
+      if (fabs(assembled_values[i * num_nodes + j] - assembled_true[i * num_nodes + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT "] Error in assembly: %f != %f\n", i, assembled_array[i], assembled_true[i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in assembly: %f != %f\n", i, j, assembled_values[i * num_nodes + j],
+               assembled_true[i * num_nodes + j]);
         // LCOV_EXCL_STOP
       }
     }
-    CeedVectorRestoreArrayRead(assembled, &assembled_array);
   }
 
   // Cleanup
-  CeedVectorDestroy(&x);
-  CeedVectorDestroy(&assembled);
+  free(rows);
+  free(cols);
   CeedVectorDestroy(&u);
   CeedVectorDestroy(&v);
+  CeedVectorDestroy(&assembled);
   CeedElemRestrictionDestroy(&elem_restriction_u);
-  CeedElemRestrictionDestroy(&elem_restriction_x);
+  CeedElemRestrictionDestroy(&elem_restriction_u_i);
   CeedBasisDestroy(&basis_u);
-  CeedBasisDestroy(&basis_x);
-  CeedQFunctionDestroy(&qf_mass);
-  CeedOperatorDestroy(&op_mass);
+  CeedQFunctionDestroy(&qf_identity);
+  CeedOperatorDestroy(&op_identity);
   CeedDestroy(&ceed);
   return 0;
 }
diff --git a/tests/t580-operator.c b/tests/t580-operator.c
new file mode 100644
index 00000000..dcd4b837
--- /dev/null
+++ b/tests/t580-operator.c
@@ -0,0 +1,147 @@
+/// @file
+/// Test assembly of H(div) mass matrix operator diagonal
+/// \test Test assembly of H(div) mass matrix operator diagonal
+#include "t580-operator.h"
+
+#include <ceed.h>
+#include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+#include "t330-basis.h"
+
+int main(int argc, char **argv) {
+  Ceed                ceed;
+  CeedElemRestriction elem_restriction_x, elem_restriction_u;
+  CeedBasis           basis_x, basis_u;
+  CeedQFunction       qf_mass;
+  CeedOperator        op_mass;
+  CeedVector          x, assembled, u, v;
+  CeedInt             dim = 2, p = 8, q = 3, px = 2;
+  CeedInt             n_x = 1, n_y = 1;  // Currently only implemented for single element
+  CeedInt             row, column, offset;
+  CeedInt             num_elem = n_x * n_y, num_faces = (n_x + 1) * n_y + (n_y + 1) * n_x;
+  CeedInt             num_dofs_x = (n_x + 1) * (n_y + 1), num_dofs_u = num_faces * 2, num_qpts = q * q;
+  CeedInt             ind_x[num_elem * px * px], ind_u[num_elem * p];
+  bool                orient_u[num_elem * p];
+  CeedScalar          assembled_true[num_dofs_u];
+  CeedScalar          q_ref[dim * num_qpts], q_weight[num_qpts];
+  CeedScalar          interp[dim * p * num_qpts], div[p * num_qpts];
+
+  CeedInit(argv[1], &ceed);
+
+  // Vectors
+  CeedVectorCreate(ceed, dim * num_dofs_x, &x);
+  {
+    CeedScalar x_array[dim * num_dofs_x];
+
+    for (CeedInt i = 0; i < n_x + 1; i++) {
+      for (CeedInt j = 0; j < n_y + 1; j++) {
+        x_array[i + j * (n_x + 1) + 0 * num_dofs_x] = i / (CeedScalar)n_x;
+        x_array[i + j * (n_x + 1) + 1 * num_dofs_x] = j / (CeedScalar)n_y;
+      }
+    }
+    CeedVectorSetArray(x, CEED_MEM_HOST, CEED_COPY_VALUES, x_array);
+  }
+  CeedVectorCreate(ceed, num_dofs_u, &u);
+  CeedVectorCreate(ceed, num_dofs_u, &v);
+
+  // Restrictions
+  for (CeedInt i = 0; i < num_elem; i++) {
+    column = i % n_x;
+    row    = i / n_x;
+    offset = column * (px - 1) + row * (n_x + 1) * (px - 1);
+
+    for (CeedInt j = 0; j < px; j++) {
+      for (CeedInt k = 0; k < px; k++) {
+        ind_x[px * (px * i + k) + j] = offset + k * (n_x + 1) + j;
+      }
+    }
+  }
+  bool    orient_u_local[8] = {false, false, false, false, true, true, true, true};
+  CeedInt ind_u_local[8]    = {0, 1, 6, 7, 2, 3, 4, 5};
+  for (CeedInt j = 0; j < n_y; j++) {
+    for (CeedInt i = 0; i < n_x; i++) {
+      for (CeedInt k = 0; k < p; k++) {
+        ind_u[k]    = ind_u_local[k];
+        orient_u[k] = orient_u_local[k];
+      }
+    }
+  }
+  CeedElemRestrictionCreate(ceed, num_elem, px * px, dim, num_dofs_x, dim * num_dofs_x, CEED_MEM_HOST, CEED_USE_POINTER, ind_x, &elem_restriction_x);
+  CeedElemRestrictionCreateOriented(ceed, num_elem, p, 1, 1, num_dofs_u, CEED_MEM_HOST, CEED_COPY_VALUES, ind_u, orient_u, &elem_restriction_u);
+
+  // Bases
+  CeedBasisCreateTensorH1Lagrange(ceed, dim, dim, px, q, CEED_GAUSS, &basis_x);
+
+  BuildHdivQuadrilateral(q, q_ref, q_weight, interp, div, CEED_GAUSS);
+  CeedBasisCreateHdiv(ceed, CEED_TOPOLOGY_QUAD, 1, p, num_qpts, interp, div, q_ref, q_weight, &basis_u);
+
+  // QFunctions
+  CeedQFunctionCreateInterior(ceed, 1, mass, mass_loc, &qf_mass);
+  CeedQFunctionAddInput(qf_mass, "weight", 1, CEED_EVAL_WEIGHT);
+  CeedQFunctionAddInput(qf_mass, "dx", dim * dim, CEED_EVAL_GRAD);
+  CeedQFunctionAddInput(qf_mass, "u", dim, CEED_EVAL_INTERP);
+  CeedQFunctionAddOutput(qf_mass, "v", dim, CEED_EVAL_INTERP);
+
+  // Operators
+  CeedOperatorCreate(ceed, qf_mass, CEED_QFUNCTION_NONE, CEED_QFUNCTION_NONE, &op_mass);
+  CeedOperatorSetField(op_mass, "weight", CEED_ELEMRESTRICTION_NONE, basis_x, CEED_VECTOR_NONE);
+  CeedOperatorSetField(op_mass, "dx", elem_restriction_x, basis_x, x);
+  CeedOperatorSetField(op_mass, "u", elem_restriction_u, basis_u, CEED_VECTOR_ACTIVE);
+  CeedOperatorSetField(op_mass, "v", elem_restriction_u, basis_u, CEED_VECTOR_ACTIVE);
+
+  // Assemble diagonal
+  CeedVectorCreate(ceed, num_dofs_u, &assembled);
+  CeedOperatorLinearAssembleDiagonal(op_mass, assembled, CEED_REQUEST_IMMEDIATE);
+
+  // Manually assemble diagonal
+  CeedVectorSetValue(u, 0.0);
+  for (int i = 0; i < num_dofs_u; i++) {
+    CeedScalar       *u_array;
+    const CeedScalar *v_array;
+
+    // Set input
+    CeedVectorGetArray(u, CEED_MEM_HOST, &u_array);
+    u_array[i] = 1.0;
+    if (i) u_array[i - 1] = 0.0;
+    CeedVectorRestoreArray(u, &u_array);
+
+    // Compute diag entry for DoF i
+    CeedOperatorApply(op_mass, u, v, CEED_REQUEST_IMMEDIATE);
+
+    // Retrieve entry
+    CeedVectorGetArrayRead(v, CEED_MEM_HOST, &v_array);
+    assembled_true[i] = v_array[i];
+    CeedVectorRestoreArrayRead(v, &v_array);
+  }
+
+  // Check output
+  {
+    const CeedScalar *assembled_array;
+
+    CeedVectorGetArrayRead(assembled, CEED_MEM_HOST, &assembled_array);
+    for (int i = 0; i < num_dofs_u; i++) {
+      if (fabs(assembled_array[i] - assembled_true[i]) > 100. * CEED_EPSILON) {
+        // LCOV_EXCL_START
+        printf("[%" CeedInt_FMT "] Error in assembly: %f != %f\n", i, assembled_array[i], assembled_true[i]);
+        // LCOV_EXCL_STOP
+      }
+    }
+    CeedVectorRestoreArrayRead(assembled, &assembled_array);
+  }
+
+  // Cleanup
+  CeedVectorDestroy(&x);
+  CeedVectorDestroy(&assembled);
+  CeedVectorDestroy(&u);
+  CeedVectorDestroy(&v);
+  CeedElemRestrictionDestroy(&elem_restriction_u);
+  CeedElemRestrictionDestroy(&elem_restriction_x);
+  CeedBasisDestroy(&basis_u);
+  CeedBasisDestroy(&basis_x);
+  CeedQFunctionDestroy(&qf_mass);
+  CeedOperatorDestroy(&op_mass);
+  CeedDestroy(&ceed);
+  return 0;
+}
diff --git a/tests/t570-operator.h b/tests/t580-operator.h
similarity index 100%
rename from tests/t570-operator.h
rename to tests/t580-operator.h
diff --git a/tests/t571-operator.c b/tests/t581-operator.c
similarity index 100%
rename from tests/t571-operator.c
rename to tests/t581-operator.c
diff --git a/tests/t572-operator.c b/tests/t582-operator.c
similarity index 100%
rename from tests/t572-operator.c
rename to tests/t582-operator.c
diff --git a/tests/t573-operator.c b/tests/t583-operator.c
similarity index 95%
rename from tests/t573-operator.c
rename to tests/t583-operator.c
index 08a11991..4d2902c0 100644
--- a/tests/t573-operator.c
+++ b/tests/t583-operator.c
@@ -160,16 +160,16 @@ int main(int argc, char **argv) {
   // Check output
   for (CeedInt i = 0; i < num_dofs; i++) {
     for (CeedInt j = 0; j < num_dofs; j++) {
-      if (fabs(assembled_values_oriented[j * num_dofs + i] - assembled_values[j * num_dofs + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values_oriented[i * num_dofs + j] - assembled_values[i * num_dofs + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
-        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in oriented assembly: %f != %f\n", i, j, assembled_values_oriented[j * num_dofs + i],
-               assembled_values[j * num_dofs + i]);
+        printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in oriented assembly: %f != %f\n", i, j, assembled_values_oriented[i * num_dofs + j],
+               assembled_values[i * num_dofs + j]);
         // LCOV_EXCL_STOP
       }
-      if (fabs(assembled_values_curl_oriented[j * num_dofs + i] - assembled_values[j * num_dofs + i]) > 100. * CEED_EPSILON) {
+      if (fabs(assembled_values_curl_oriented[i * num_dofs + j] - assembled_values[i * num_dofs + j]) > 100. * CEED_EPSILON) {
         // LCOV_EXCL_START
         printf("[%" CeedInt_FMT ", %" CeedInt_FMT "] Error in curl-oriented assembly: %f != %f\n", i, j,
-               assembled_values_curl_oriented[j * num_dofs + i], assembled_values[j * num_dofs + i]);
+               assembled_values_curl_oriented[i * num_dofs + j], assembled_values[i * num_dofs + j]);
         // LCOV_EXCL_STOP
       }
     }
